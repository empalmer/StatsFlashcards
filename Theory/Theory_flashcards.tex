\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\newenvironment{note}{\paragraph{NOTE:}}{}
\newenvironment{field}{\paragraph{field:}}{}
%\newenvironment{tags}{\paragraph{tags:}}{}
\newcommand*{\tags}[1]{\paragraph{tags: }#1}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%start_tag From Stat Cheatsheet
\tags{FromStatCheatsheet}

\begin{note}
  \begin{field}
    \tiny 1
  \end{field}
  \begin{field}
    CDF of Geometric ($p$)
  \end{field}
  \begin{field}
    $1 - (1-p)^x$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 2
  \end{field}
  \begin{field}
    CDF of Exponential($\beta$)
  \end{field}
  \begin{field}
    $1 - e^{-\frac{x}{\beta}}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 3
  \end{field}
  \begin{field}
    \begin{itemize}
  \item $P(\varnothing) = $
  \item $B = \Omega \cap B = (A \cup A^c) \cap B
    = (A \cap B) \cup (A^c \cap B)$
  \item $P(A^c) = $
  \item $P(B) = $
  \item $P(\Omega) =  \qquad P(\varnothing) = $
  \item $\left(\bigcup_n A_n\right) =
    \quad
    \left(\bigcap_n A_n\right) =
    \qquad$
    \textsc{DeMorgan}
\end{itemize}
  \end{field}
  \begin{field}
    \begin{itemize}
  \item $P(\varnothing) = 0$
  \item $B = \Omega \cap B = (A \cup A^c) \cap B
    = (A \cap B) \cup (A^c \cap B)$
  \item $P(A^c) = 1 - P(A)$
  \item $P(B) = P(A \cap B) + P(A^c \cap B)$
  \item $P(\Omega) = 1 \qquad P(\varnothing) = 0$
  \item $\left(\bigcup_n A_n\right) = \bigcap_n A_n
    \quad
    \left(\bigcap_n A_n\right) = \bigcup_n A_n
    \qquad$
    \textsc{DeMorgan}
\end{itemize}
  \end{field}
\end{note}




\begin{note}
  \begin{field}
    \tiny 4
  \end{field}
  \begin{field}
    Probability Set intersection
    \begin{itemize}
      \item $P(\bigcup_n A_n)
        = 1 - P(\bigcap_n A_n^c)$
      \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)
        \implies P(A \cup B) \leq P(A) + P(B)$
      \item $P(A \cup B)
        = $
      \item $P(A \cap B^c) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Probability Set intersection
    \begin{itemize}
      \item $P(\bigcup_n A_n)
        = 1 - P(\bigcap_n \mathsf{A_n})$
      \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)\\[1ex]
        \implies P(A \cup B) \le P(A) + P(B)$
      \item $P(A \cup B)
        = P(A \cap B^c) + P(A^c \cap B) + P(A \cap B)$
      \item $P(A \cap B^c) = P(A) - P(A \cap B)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 5
  \end{field}
  \begin{field}
    $P(A \cap B) =  \text{ when }A \text{ and } B \text{independent}$
  \end{field}
  \begin{field}
    $P(A \cap B) = P(A)P(B) \text{ when }A \text{ and } B \text{independent}$
  \end{field}
  \end{note}

\begin{note}
  \begin{field}
    \tiny 6
  \end{field}
  \begin{field}
    $$P(A|B) = $$
  \end{field}
  \begin{field}
    $$P(A|B) = \frac{P(A\cap B)}{P(B)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 7
  \end{field}
  \begin{field}
    Law of total probability
  \end{field}
  \begin{field}
    Law of total probability
    $$P(B) = \sum_{i=1}^n P(B|A_i)P(A_i) \quad \Omega = \cup_{i=1}^n A_i$$

    $$P(B) = P(A \cup B) + P(A^c \cup B) $$
   \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 8
  \end{field}
  \begin{field}
    Bayes Theorem
  \end{field}
  \begin{field}
    Bayes Theorem
    $$P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^n P(B|A_j)P(A_j)} \quad \Omega = \cup_{i=1}^n A_i$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 9
  \end{field}
  \begin{field}
    CDF Laws
  \end{field}
  \begin{field}
    CDF Laws
    \begin{enumerate}
      \item Nondecreasing: $x_1 < x_2 \implies F(x_1) \leq F(x_2)$
      \item Limits: $\lim_{x \to -\infty}=0$ and $\lim_{x\to \infty} = 1$
      \item Right-Continuous $\lim_{y \to x^+}F(y) = F(x)$
    \end{enumerate}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 10
  \end{field}
  \begin{field}
    $$f_{y|x}(y|x) = $$
  \end{field}
  \begin{field}
    $$f_{y|x}(y|x) = \frac{f(x,y)}{f_x(x)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 11
  \end{field}
  \begin{field}
    $X,Y \text{independent}$
    \begin{itemize}
      \item $P(X \leq x, Y \leq y) = $
      \item $f_{x,y}(x,y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    $X,Y \text{independent}$
    \begin{itemize}
      \item $P(X \leq x, Y \leq y) = P(X \leq x)P(Y \leq y)$
      \item $f_{x,y}(x,y) = f_x(x)f_y(y)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 12
  \end{field}
  \begin{field}
     Transformations $Z = \phi(X)$

    \begin{itemize}
      \item Discrete: $f_Z(z) = $
      \item Continuous: $F_Z(z)=$
      \item Cont, $\phi$ strictly monotone:
      $f_z(z)$
    \end{itemize}
  \end{field}
  \begin{field}
    Transformations $Z = \phi(X)$
  \begin{itemize}
    \item Discrete: $$f_Z(z) = P(\phi(X) = z) = P(X \in \phi^{-1}(z)) = \sum_{x \in \phi^{-1}(z)}f_x(x) $$
    \item Continuous (Method of CDF): $$F_Z(z)= P(\phi(X)\leq z) = \int_{x:\phi(x)\leq z} f(x) dx$$
    \item Cont, $\phi$ strictly monotone: (Method of PDF)
    $f_z(z) = f_x(\phi^{-1}(z))|\frac{d}{dz} \phi^{-1}(z)|$
  \end{itemize}
\end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 13
  \end{field}
  \begin{field}
     Rule of the Lazy Statistician: $ E[g(x)] = $
  \end{field}
  \begin{field}
    Rule of the Lazy Statistician:  $E[g(x)] = \int g(x)f_x(x)dx$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 14
  \end{field}
  \begin{field}
    Expectation rules
    \begin{itemize}
      \item $E(c) = $
      \item $E(cX) = $
      \item $E(X + Y) = $
      \item $E(\phi(X)) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Expectation rules
    \begin{itemize}
      \item $E(c) = c$
      \item $E(cX) = cE(X)$
      \item $E(X + Y) = E(X) + E(Y)$
      \item $E(\phi(X)) \neq \phi(E(X))$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 15
  \end{field}
  \begin{field}
    Conditional expectation
      \begin{itemize}
        \item $E(Y|X = x) = $
        \item $E(X) = $
        \item $E(Y+Z|X) = $
        \item $E(Y|X) = c \implies $
      \end{itemize}
  \end{field}
  \begin{field}
    Conditional expectation
      \begin{itemize}
        \item $E(Y|X = x) = \int y f(y|x) dy$
        \item $E(X) = E(E(X|Y))$
        \item $E(Y+Z|X) = E(Y|X) + E(Z|X)$
        \item $E(Y|X) = c \implies Cov(X,Y) = 0$
      \end{itemize}

  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 16
  \end{field}
  \begin{field}
    Variance
    \begin{itemize}
      \item $V(X) = \sigma_x^2 = $
      \item $V(X+Y) =$
      \item $V\bigg[\sum_{i=1}^n X_i\bigg] = $
    \end{itemize}
  \end{field}
  \begin{field}
    Variance
    \begin{itemize}
      \item $V(X) = \sigma_x^2 = E[(X - E(X))^2] = E(X^2) - E(X)^2$
      \item $V(X+Y) = V(X) + V(Y) + Cov(X,Y)$
      \item $V\bigg[\sum_{i=1}^n X_i\bigg] = \sum_{i=1}^n V(X_i) + \sum_{i\neq j} Cov(X_i,X_j)$
    \end{itemize}
  \end{field}
\end{note}








\begin{note}
  \begin{field}
    \tiny 17
  \end{field}
  \begin{field}
    Covariance
    \begin{itemize}
      \item $Cov(X,Y) = $
      \item $Cov(X,c) = $
      \item $Cov(Y,X) = $
      \item $Cov(aX,bY) =$
      \item $Cov(X + a, Y + b) =$
      \item $Cov\bigg(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\bigg) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Covariance
    \begin{itemize}
      \item $Cov(X,Y) = E[(X - E(X)(Y - E(Y)))] = E(XY) - E(X)E(Y)$
      \item $Cov(X,c) = 0$
      \item $Cov(Y,X) = Cov(X,Y)$
      \item $Cov(aX,bY) = abCov(X,Y)$
      \item $Cov(X + a, Y + b) = Cov(X,Y)$
      \item $Cov\bigg(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\bigg) = \sum_{i=1}^n \sum_{j=1}^m Cov(X_i,Y_j)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 18
  \end{field}
  \begin{field}
    Correlation: $\rho(X,Y)$
  \end{field}
  \begin{field}
    Correlation: $\rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{V(X)V(Y)}}$
  \end{field}
\end{note}



\begin{note}
  \begin{field}
    \tiny 19
  \end{field}
  \begin{field}
    Conditional Variance
    \begin{itemize}
      \item $V(Y|X) = $
      \item $V(Y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Conditional Variance
    \begin{itemize}
      \item $V(Y|X) = E\big[(Y - E(Y|X))^2|X\big] = E(Y^2|X) - E(Y|X)^2$
      \item $V(Y) = E(V(Y|X)) + V(E(Y|X))$
    \end{itemize}
  \end{field}
\end{note}

%%end_tag
%%start_tag Undergrad_Textbook

\tags{UndergradTextbook}
%Chapter 2

\begin{note}
  \begin{field}
    \tiny 20
  \end{field}
  \begin{field}
    Law of total probability $k=2$ (using conditional probability)
  \end{field}
  \begin{field}
    $P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$

  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 21
  \end{field}
  \begin{field}
    Bayes formula in terms of law of total probability,
  \end{field}
  \begin{field}
    $P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^c)P(B^c)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 22
  \end{field}
  \begin{field}
    $P(A \text{ and }B)$
  \end{field}
  \begin{field}
    $P(A \text{ and }B) = P(A|B)P(B) = P(B|A)P(A)$
  \end{field}
\end{note}


% Chapter 3

\begin{note}
  \begin{field}
    \tiny 23
  \end{field}
  \begin{field}
    Events $A$ and $B$ are independent if
  \end{field}
  \begin{field}
    $P(A|B) = P(A)$ equivalently $P(A \text{ and } B)  = P(A)P(B)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 24
  \end{field}
  \begin{field}
    Poisson setting
  \end{field}
  \begin{field}
    The Poisson setting arises in the context of discrete counts of events that occur over space or time with the small probability and where successive events are independent

    Eg: 2 on average calls a minute, $X$ is number of calls a minute, $X \sim Pois $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 25
  \end{field}
  \begin{field}
    Poisson approximation of binomial distribution
  \end{field}
  \begin{field}
    Suppose $X \sim Binom(n,p)$, $Y \sim Pois (\lambda)$. If $n\to \infty$, and $p \to 0$, in such a way that $np \to \lambda > 0$, then for all $k$, $P(X = k) \to P(Y = k)$. The Poisson distribution with parameter $\lambda = np$ serves as a good approiximation for the binomial distribution when $n$ is large and $p$ is small.
  \end{field}
\end{note}

% Chapter 4


\begin{note}
  \begin{field}
    \tiny 26
  \end{field}
  \begin{field}
    $E(f(X,Y))$ when $X,Y$ are discrete
  \end{field}
  \begin{field}
    $E(f(X,Y)) = \sum_x \sum_y f(x,y)P(X=x,Y=y)$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 27
  \end{field}
  \begin{field}
    If $X,Y$ are independent, then $f(X),g(Y)$
  \end{field}
  \begin{field}
    are also independent
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 28
  \end{field}
  \begin{field}
    If $X,Y$ independent, $E(XY) = , E(f(X)g(Y)) = $
  \end{field}
  \begin{field}
    If $X,Y$ independent, $E(XY) = E(X)E(Y), E(f(X)g(Y)) = E(f(X))E(g(Y))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 29
  \end{field}
  \begin{field}
    Sum of independent discrete random variables $X,Y$: $P(X+Y = k)$
  \end{field}
  \begin{field}
    $P(X+Y = k) = \sum_i P(X=i)P(Y=k-i)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 30
  \end{field}
  \begin{field}
    $V(X) = 0$
  \end{field}
  \begin{field}
    If and only if $X $ is a constant
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 31
  \end{field}
  \begin{field}
    $E(I_A) = , V(I_A)$ Where $I_A$ is an indicator function
  \end{field}
  \begin{field}
    $E(I_A) = P(A), V(I_A) = P(A)P(A^c)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 32
  \end{field}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = $$
  \end{field}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = \frac{P(X=x,Y=y)}{P(X=x)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 33
  \end{field}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = $
  \end{field}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = \sum_y y P(Y = y | X = x)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 34
  \end{field}
  \begin{field}
    Problem solving strategy for expected value of counting
  \end{field}
  \begin{field}
    Use indicator functions for each trial , where $X = \sum I$ and use linearity of expectation
  \end{field}
\end{note}

 % Chapter 5

\begin{note}
  \begin{field}
    \tiny 35
  \end{field}
  \begin{field}
    $P(X > s + t|X > t)$ for geometric, exponential
  \end{field}
  \begin{field}
    $P(X > s + t|X > t) = P(X > s)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 36
  \end{field}
  \begin{field}
    Distribution for: A bag of $N$ balls which conatins $r$ red balls and $N-r$ blue balls, $X$ is number of red balls in a sample of size $n$ taken without replacement.
  \end{field}
  \begin{field}
    Hypergeometric.
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 37
  \end{field}
  \begin{field}
    Distribution for modeling arrival time
  \end{field}
  \begin{field}
    Exponential
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 38
  \end{field}
  \begin{field}
    $E(g(X,Y)) = $ (continuous )
  \end{field}
  \begin{field}
    $E(g(X,Y)) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty g(x,y)f(x,y)dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 39
  \end{field}
  \begin{field}
    $Cov(X,Y) = $ (integration )
  \end{field}
  \begin{field}
    $Cov(X,Y) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty (x - E(X))(y - E(Y))dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 40
  \end{field}
  \begin{field}
    Problem solving strategies for functions of random variables
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Methods of cdf: $Y = g(X)$, find cdf $P(Y \leq y) = P(g(X) \leq y) = P(X \leq g^{-1}(y))$
      \item For finding $P(X < Y)$, set up integrals that cover
      \item For finding probabilities of independent uniform random variables, use geometric (area) properties
    \end{itemize}
  \end{field}
\end{note}

% Chapter 7

\begin{note}
  \begin{field}
    \tiny 41
  \end{field}
  \begin{field}
    Quantile
  \end{field}
  \begin{field}
    If $X$ is a continuous random variable, then the $p$th quantile is is the number $q$ that satisfies $P(X \leq q) = p/100 $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 42
  \end{field}
  \begin{field}
    Poisson process
  \end{field}
  \begin{field}
    Times between arrivals are modeled as iid exponential random variables with parameter $\lambda = 1/\beta$. Let $N_t$ be the number of arrivals up to time $t$. Then $N_t \sim Pois(\lambda t)$
% go back and do properties of poisson process
  \end{field}
\end{note}

% Chapter 8

\begin{note}
  \begin{field}
    \tiny 43
  \end{field}
  \begin{field}
    Conditional density function $f_{Y|X}(y|x) = $
  \end{field}
  \begin{field}
    $f_{Y|X}(y|x) = \frac{f(x,y)}{f_x(x)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 44
  \end{field}
  \begin{field}
    Continuous bayes formula
  \end{field}
  \begin{field}
    $f_{X|Y}(x|y)  = \frac{f_{Y|X}(y|x)f_x(x)}{\int_{t = -\infty}^\infty f_{Y|X}(y|t)f_x(t)dt}$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 45
  \end{field}
  \begin{field}
    Conditional expectation for continuous random variables
    $E(Y|X = x)$
  \end{field}
  \begin{field}
    $E(Y|X = x) = \int_y y f_{Y|X}(y|x)dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 46
  \end{field}
  \begin{field}
    Law of total expectation
  \end{field}
  \begin{field}
    $E(Y) = E(E(Y|X))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 47
  \end{field}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = $
      \item $E(g(Y)|X=x) = $
      \item If $X,Y$ independent, $E(Y|X) = $
      \item If $Y = g(X)$, then $E(Y|X) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = a E(Y|X) + bE(Z|X)$
      \item $E(g(Y)|X=x) = \int_y g(y) f_{Y|X}(y|x)dy$
      \item If $X,Y$ independent, $E(Y|X) = E(Y)$
      \item If $Y = g(X)$, then $E(Y|X) = Y$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 48
  \end{field}
  \begin{field}
    Law of total probability, continuous
  \end{field}
  \begin{field}
    $P(A) = \int_{-\infty}^\infty P(A|X=x)f_x(x)dx$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
  \tiny 49
\end{field}
  \begin{field}
    Conditional variance $V(Y|X = x)$
  \end{field}
  \begin{field}
    $$V(Y|X = x) = \sum_y (y - E(Y|X=x))^2P(Y=y|X=x)$$ discrete
    $$V(Y|X = x) = \int_y (y - E(Y|X=x))^2 f_{Y|X}(y|x)dy$$ continuous
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 50
  \end{field}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = $
      \item $V(aY + b | X = x) = $
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = E(Y^2 | X=x) - (E(Y|X=x))^2$
      \item $V(aY + b | X = x) = a^2 V(Y|X=x)$
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = V(Y|X=x) + V(Z|X=x)$
    \end{itemize}
  \end{field}
\end{note}

% Chapter 9

\begin{note}
  \begin{field}
    \tiny 51
  \end{field}
  \begin{field}
    $P(X \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(X \geq \epsilon) \leq E(X) / \epsilon$ (Markov's Inequality )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 52
  \end{field}
  \begin{field}
    $P(|X-\mu| \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(|X-\mu| \geq \epsilon) \leq \sigma^2/\epsilon^2$ (Chebyshev's inequality, if mean and variance finite )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 53
  \end{field}
  \begin{field}
    $P(\lim_{n\to \infty} S_n/n = \mu) = $
  \end{field}
  \begin{field}
  $P(\lim_{n\to \infty} S_n/n = \mu) = 1$ (Strong law of large numbers )
\end{field}
\end{note}

%%end_tag
%%start_tag Distribution Relationships
\tags{distributionrelationships dist}

\begin{note}
  \begin{field}
    \tiny 54
  \end{field}
  \begin{field}
    $X \sim Gamma(a,b)$
    $P(X \leq X) = $
  \end{field}
  \begin{field}
    $X \sim Gamma(a,b)$
    $P(X \leq X) = P(Y \geq a)$
    Where $Y \sim Pois (x/b)$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 55
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(0,1)$$
    $$ \sum X_i \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(0,1)$$
    $$ \sum X_i \sim N(0,n)$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 56
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(\mu_i,\sigma_i^2)$$
    $$ \sum X_i \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(\mu_i,\sigma_i^2)$$
    $$ \sum X_i \sim N(\sum \mu_i,\sum \sigma^2_i)$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 57
  \end{field}
  \begin{field}
    $$X  \sim N(\mu,\sigma^2)$$
    $$ aX + b \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ aX + Y \sim N(a\mu + b, a^2\sigma^2)$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 58
  \end{field}
  \begin{field}
    $X \sim Binom(1,p) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Bern(p)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 59
  \end{field}
  \begin{field}
    $X \sim NegBinom(1,p) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Geom(p)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 60
  \end{field}
  \begin{field}
    $X \sim Gamma(1,\theta) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Exp(\theta)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 61
  \end{field}
  \begin{field}
    $X \sim Exp(\theta) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Gamma(1,\theta)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 62
  \end{field}
  \begin{field}
    $X \sim Gamma(v/2,1/2) \overset{?}{\sim} $
  \end{field}
  \begin{field}
    $X \sim \chi^2(v)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 63
  \end{field}
  \begin{field}
    $$X \sim \chi^2(v) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$X \sim Gamma(v/2,1/2)$$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 64 \end{field}
  \begin{field}
    $$ X \sim \chi^2(2) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim exp(2)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 65 \end{field}
  \begin{field}
    $$ X \sim Weibull(1,\beta) \overset{?}{\sim} $$
  \end{field}
  \begin{field}
    $$X \sim Exp(\beta)$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 66 \end{field}
  \begin{field}
    $X_1, X_2 \sim \chi^2(v_i)$ independent
    $ \frac{X_1/v_1}{X_2/v_2}$
  \end{field}
  \begin{field}
    $$ \frac{(X_1/v_1)}{(X_2/v_2)} \sim F(v_1,v_2)$$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 67 \end{field}
  \begin{field}
    $$ X \sim beta(1,1) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim Unif(0,1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 68 \end{field}
  \begin{field}
    $$ X \sim Unif(0,1) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim beta(1,1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 69 \end{field}
  \begin{field}
    Special case of t
    $$ X \sim t(1) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim Caucy(0,1)$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 70 \end{field}
  \begin{field}
    Scaled Gamma
    $$X \sim Gamma(\alpha,\beta), Y = aX \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ Y \sim Gamma(\alpha,a\beta)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 71 \end{field}
  \begin{field}
    Scaled Exponential
    $$ X \sim Exp(\lambda), Y = aX \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ Y \sim Exp(a\lambda)$$
  \end{field}
\end{note}

%check
\begin{note} \begin{field} \tiny 72 \end{field}
  \begin{field}
    Sum of Exponential, equal rate
    $X_i \sim Exp(\lambda), Y = \sum X_i$
  \end{field}
  \begin{field}
    $$Y \sim Gamma(n,\lambda)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 73 \end{field}
  \begin{field}
    $$ X \sim Exp(\lambda), Y  = e^{-x}$$
  \end{field}
  \begin{field}
    $$Y \sim Beta(\lambda,1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 74 \end{field}
  \begin{field}
    Min of Exponential
    $$X_1, \ldots , X_n Exp(\lambda_i), Y = \min(X_i) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $Y \sim exp(\sum \lambda_i)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 75 \end{field}
  \begin{field}
    Min of Uniform
    $$ X_i \sim Unif(0,1), Y = \lim n \min(X_i) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$Y \sim Exp(1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 76 \end{field}
  \begin{field}
    $$ X \sim Beta(\alpha,\beta), Y = (1-X)$$
  \end{field}
  \begin{field}
    $$ Y \sim Beta(\beta,\alpha)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 77 \end{field}
  \begin{field}
    $X \sim F_X(X), Y = F^{-1}_X(X)$
  \end{field}
  \begin{field}
    $Y \sim Unif(0,1)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 78 \end{field}
  \begin{field}
    $X \sim N(\mu,\sigma^2), Y = e^X$
  \end{field}
  \begin{field}
    $Y \sim lognormal(\mu,\sigma^2)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 79 \end{field}
  \begin{field}
    $X \sim exp(\beta), Y = X^{1/z}$
  \end{field}
  \begin{field}
    $Y \sim Weibull(z,\beta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 80 \end{field}
  \begin{field}
    Square of Normal
    $X \sim N(0,1), Y = X^2$
  \end{field}
  \begin{field}
    $Y \sim \chi^2(1)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 81 \end{field}
  \begin{field}
    Square of t
    $X \sim t(v), Y = X^2$
  \end{field}
  \begin{field}
    $Y \sim F(1,v)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 82 \end{field}
  \begin{field}
    Sum of Poisson
    $X_i \sim Poisson(\mu_i) Y = \sum X_i$
  \end{field}
  \begin{field}
    $Y \sim Poisson(\sum \mu_i)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 83 \end{field}
  \begin{field}
    Sum of Gamma
    $X_i \sim Gamma(\alpha_i, \beta), Y = \sum X_i$
  \end{field}
  \begin{field}
    $Y \sim Gamma(\sum \alpha_i, \beta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 84 \end{field}
  \begin{field}
    Sum of independent Chi-squared
    $ X_i \sim \chi^2(v_i) Y = \sum X_i$
  \end{field}
  \begin{field}
    $Y \sim \chi^2(\sum v_i)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 85 \end{field}
  \begin{field}
    $X,Y$ independent
    $X,Y \sim N(0,1), X/Y$
  \end{field}
  \begin{field}
    $X/Y \sim Cauchy(0,1)$
  \end{field}
\end{note}





\begin{note} \begin{field} \tiny 86 \end{field}
  \begin{field}
    $X_1,X_2 \sim gamma(\alpha_i,1)$ independent, $\frac{X_1}{X_1 + X_2} $
  \end{field}
  \begin{field}
    $$ \frac{X_1}{X_1 + X_2} \sim beta(\alpha_1,\alpha_2)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 87 \end{field}
  \begin{field}
    $X_1,X_2 \sim gamma(\alpha_i,\beta_i)$ independent, $\frac{\beta_2X_1}{\beta_2X_1 + \beta_1X_2} $
  \end{field}
  \begin{field}
    $$ \frac{\beta_2X_1}{\beta_2X_1 + \beta_1X_2}\sim beta(\alpha_1,\alpha_2)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 88 \end{field}
  \begin{field}
    $X,Y $ independent $exp(\mu)$ $X-Y$
  \end{field}
  \begin{field}
    $X-Y \sim $ double exponential$(0,\mu)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 89 \end{field}
  \begin{field}

    $X \sim Gamma(\alpha,\beta)$ $Y = 1/X$
  \end{field}
  \begin{field}
      Inverted Gamma
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 90 \end{field}
  \begin{field}
    Bernoulii$(p), E(X) = , V(X) = $
  \end{field}
  \begin{field}
     Bernoulii$(p), E(X) = p, V(X) = p(1-p)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 91 \end{field}
  \begin{field}
    Discrete Uniform $N, E(X) = , V(X) = $
  \end{field}
  \begin{field}
    Discrete Uniform $N, E(X) = \frac{N+1}{2}, V(X) = \frac{(N+1)(N-1)}{12}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 92 \end{field}
  \begin{field}
    Cauchy($\theta,\sigma), E(X) = ,V(X)=$
  \end{field}
  \begin{field}
    Cauchy($\theta,\sigma), E(X) = na,V(X)=na$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 93 \end{field}
  \begin{field}
    Double Exponential$(\mu,\sigma), E(X) = , V(X) = $
  \end{field}
  \begin{field}
    Double Exponential$(\mu,\sigma), E(X) = \mu, V(X) = 2\sigma^2$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 94 \end{field}
  \begin{field}
    F($v_1,v_2), E(X) = , V(X) = $
  \end{field}
  \begin{field}
F($v_1,v_2), E(X) = \frac{v_1}{v_2-2}, V(X) = 2\big(\frac{v_2}{v_2-2}\big)^2\frac{(v_1+v_2-2)}{v_1(v)2-4}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 95 \end{field}
  \begin{field}
    Mean and Variance for Distributions not on bible (but in CB)
      \begin{itemize}

        \item Double Exponential$(\mu,\sigma), E(X) = , V(X) = $
        \item F($v_1,v_2), E(X) = , V(X) = $
        \item Logistic$(\mu,\beta), E(X) = , V(X) = $
        \item Lognormal$(\mu,\sigma^2), E(X)=, V(X) = $
        \item Pareto$(\alpha,\beta), E(X) =, V(X) =$
        \item t$(v), E(X) = , V(X) = $
        \item Weibull$(\gamma,\beta), E(X) =, V(X) = $
      \end{itemize}
  \end{field}
  \begin{field}
    Mean and Variance. for Distributions not on bible (but in CB)
      \begin{itemize}

        \item Logistic$(\mu,\beta), E(X) = \mu, V(X) = \frac{\phi^2\beta^2}{3}$
        \item Lognormal$(\mu,\sigma^2), E(X)=e^{\mu + (\sigma^2/2)}) V(X) = e^{2(\mu+\sigma^2)} - e^{2\mu + \sigma^2}$
        \item Pareto$(\alpha,\beta), E(X) = \frac{\beta\alpha}{\beta-1}, V(X) = \frac{\beta\alpha^2}{(\beta-1)^2(\beta-2)}$
        \item t$(v), E(X) = 0, V(X) = \frac{v}{v-2}$
        \item Weibull$(\gamma,\beta), E(X) = \beta^{1/\gamma}\Gamma(1 + 1/\gamma), V(X) = \beta^{2/\gamma}(\Gamma(1 + 2/\gamma) - \Gamma^2(1 + 1/\gamma))$
      \end{itemize}
  \end{field}
\end{note}

%%end_tag


%%start_tag Calculus

\tags{Calculus calc}

\begin{note} \begin{field} \tiny 96 \end{field}
  \begin{field}
    $\int_0^\infty e^{-x^2/2} = $
  \end{field}
  \begin{field}
    $\int_0^\infty e^{-x^2/2} = \sqrt{\pi/2}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 97 \end{field}
  \begin{field}
    $\int_0^\infty x^{a-1}e^{-x/b} = $
  \end{field}
  \begin{field}
    $\int_0^\infty x^{a-1}e^{-x/b} = \Gamma(a)b^a$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 98 \end{field}
  \begin{field}
    $\int_0^1 x^{a - 1}(1 - x)^{b-1} = $
  \end{field}
  \begin{field}
    $\int_0^1 x^{a - 1}(1 - x)^{b-1} = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 99 \end{field}
  \begin{field}
    $\log(x) = y , x = $
  \end{field}
  \begin{field}
    $\log(x) = y , x = e^y$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100 \end{field}
  \begin{field}
    $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = $
  \end{field}
    \begin{field}
      $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = e^a$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 101 \end{field}
  \begin{field}
    $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = e^a$
  \end{field}
  \begin{field}
    $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 102 \end{field}
  \begin{field}
    $\frac{d}{dx}f(g(x)) = $
  \end{field}
  \begin{field}
    $\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)$ (Chain rule)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 103 \end{field}
  \begin{field}
    $\frac{d}{dx} \int_{a}^x f(t)dt = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} \int_{a}^x f(t)dt = f(x)$ (fundamental theorem of calculus )
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 104 \end{field}
  \begin{field}
    $\int_a^b u dv = $

    ex: $\int x e^{-x}$
  \end{field}
  \begin{field}
    $\int_a^b u dv = uv|_a^b - \int_a^b v du $

    ex: $u = x, dv = e^{-x}, du = dx, v = -e^{-x}$
    \begin{align*}
      \int x e^{-x} &= -x e^{-x} + \int e^{-x} \\
      &= -x e^{-x}  -e^{-x} + c
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 105 \end{field}
  \begin{field}
    $\sum_{k=0}^\infty \frac{x^k}{k!} = $
  \end{field}
  \begin{field}
    $\sum_{k=0}^\infty \frac{x^k}{k!} = e^x$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 106 \end{field}
  \begin{field}
    $e^x = $
  \end{field}
  \begin{field}
    $e^x = \sum_{k=0}^\infty \frac{x^k}{k!}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 107 \end{field}
  \begin{field}
    $\sum_{k=0}^\infty x^k = $
  \end{field}
  \begin{field}
    $\sum_{k=0}^\infty x^k = \frac{1}{1-x}$ for $|x| < 1$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 108 \end{field}
  \begin{field}
    $\sum_{k=0}^n x^k =$
  \end{field}
  \begin{field}
    $\sum_{k=0}^n x^k = \frac{1 - x^{n+1}}{1-x}$ for $x \neq 1$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 109 \end{field}
  \begin{field}
    $\lim_{x \to -\infty} e^{-x} = $
  \end{field}
  \begin{field}
    $\lim_{x \to -\infty} e^{-x} = \infty$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 110 \end{field}
  \begin{field}
    $\lim_{x \to \infty} e^{-x} = $
  \end{field}
  \begin{field}
    $\lim_{x \to -\infty} e^{-x} = 0$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 111 \end{field}
  \begin{field}
    $$ (fg)' = $$
  \end{field}
  \begin{field}
    $$ (fg)' = f'g + g'f$$ (product rule )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 112 \end{field}
  \begin{field}
    $\frac{d}{dx} x^n = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} x^n = nx^{n-1}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 113 \end{field}
  \begin{field}
    $\frac{d}{dx} a^x = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} a^x = a^x ln(a)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 114 \end{field}
  \begin{field}
    $\frac{d}{dx} ln(x) = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} ln(x) = \frac{1}{x}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 115 \end{field}
  \begin{field}
    $\frac{d}{dx} (f(x))^n = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} (f(x))^n = n(f(x))^{n-1}f'(x)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 116 \end{field}
  \begin{field}
    $ \frac{d}{dx} ln(f(x)) = $
  \end{field}
  \begin{field}
    $ \frac{d}{dx} ln(f(x)) = \frac{f'(x)}{f(x)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 117 \end{field}
  \begin{field}
    $\frac{d}{dx} e^{f(x)} = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} e^{f(x)} = f'(x)e^{f(x)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 118 \end{field}
  \begin{field}
    $\int x^n = $
  \end{field}
  \begin{field}
    $\int x^n = \frac{1}{n+1}x^{n+1}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 119 \end{field}
  \begin{field}
    $\int \frac{1}{x} = $
  \end{field}
  \begin{field}
    $\int \frac{1}{x} = ln(|x|)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 120 \end{field}
  \begin{field}
    $\int \frac{1}{ax + b} = $
  \end{field}
  \begin{field}
    $\int \frac{1}{ax + b} = \frac{1}{a}ln(|ax + b|)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 121 \end{field}
  \begin{field}
    $\int e^{cx} = $
  \end{field}
  \begin{field}
    $\int e^{cx} = \frac{1}{c}e^{cx}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 122 \end{field}
  \begin{field}
    $\int x e^{-cx^2} = $
  \end{field}
  \begin{field}
    $\int x e^{-cx^2} = -\frac{1}{2c}e^{-cx^2}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 123 \end{field}
  \begin{field}
    U substitution:

    example; $\int_1^2 5x^2\cos(x^3)$
  \end{field}
  \begin{field}
    $\int_a^bf(g(x))g'(x) = \int_{g(a)}^g(b) f(u)du$

    Where $u = g(x), du = g' dx$

    Ex: $u = x^3, du = 3x^2, x^2 du = 1/3 du$
    $\int_1^2 5x^2\cos(x^3) = \int_{1}^8 5/3 \cos(u)du$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 124 \end{field}
  \begin{field}
    $\Gamma(a) = $
  \end{field}
  \begin{field}
    $\int_0^\infty t^{a-1}e^{-t}dt $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 125 \end{field}
  \begin{field}
    $\int_0^\infty t^{a-1}e^{-t}dt $
  \end{field}
  \begin{field}
    $= \Gamma(a) $
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 126 \end{field}
  \begin{field}
    $\Gamma(a + 1) = $
  \end{field}
  \begin{field}
    $\Gamma(a + 1) = a\Gamma(a)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 127 \end{field}
  \begin{field}
    $\Gamma(n) = $
  \end{field}
  \begin{field}
    $\Gamma(n) = (n-1)!$ (for $n$ an integer)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 128 \end{field}
  \begin{field}
    $\Gamma(1/2) = $
  \end{field}
  \begin{field}
    $\Gamma(1/2) = \sqrt{\pi}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 129 \end{field}
  \begin{field}
    $\Gamma(1) = $
  \end{field}
  \begin{field}
    $\Gamma(1) = 1$
  \end{field}
\end{note}


%%end_tag

%%start_tag Theory1
\tags{Theory1}
%%start_tag Casella Ch1
\begin{note} \begin{field} \tiny 130 \end{field}
  \begin{field}
    \begin{tabular}{|c| c c |}
    \hline \\
    & replace & no replacement \\
    number of trials &  &  \\
    Draw till nth success & & \\
    \hline
    \end{tabular}
  \end{field}
  \begin{field}
    \begin{tabular}{|c| c c |}
    \hline \\
    & replace & no replacement \\
    number of trials & Binom & Hypergeometric \\
    Draw till nth success & Nbinom & Negative hypergeometric \\
    \hline
    \end{tabular}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 131 \end{field}
  \begin{field}
    Plug uniform into inverse CDF
  \end{field}
  \begin{field}
    Get cdf
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 132 \end{field}
  \begin{field}
    Sample Space
  \end{field}
  \begin{field}
    The set, $S$, of all possible outcomes of a particular experiment is called the \textit{sample space} for the experiment.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 133 \end{field}
  \begin{field}
    Event
  \end{field}
  \begin{field}
    An \textit{event} is any collection of possible outcomes of an experiment, that is, any subset of $S$ (including $S$ itself).
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 134 \end{field}
  \begin{field}
    Union
  \end{field}
  \begin{field}
    $ A \cup B = \{x:x \in A \text{ or } x \in B\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 135 \end{field}
  \begin{field}
    Intersection
  \end{field}
  \begin{field}
    $A \cap B = \{x:x \in A \text{ and } x \in B\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 136 \end{field}
  \begin{field}
    Complementation
  \end{field}
  \begin{field}
    $A^c = \{x:x\notin A\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 137 \end{field}
  \begin{field}
    Commutativity
    $$ A \cup B = $$
    $$ A \cap B = $$
  \end{field}
  \begin{field}
    Commutativity
    $$ A \cup B = B \cup A$$
    $$ A \cap B = B \cap A$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 138 \end{field}
  \begin{field}
    Associativity
    $$ A \cup (B \cup C) = $$
    $$ A \cap (B \cap C) = $$
  \end{field}
    \begin{field}
      Associativity
        $$ A \cup (B \cup C) = (A \cup B) \cup C$$
        $$ A \cap (B \cap C) = (A \cap B) \cap C$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 139 \end{field}
  \begin{field}
    Distributive Laws
    $$A \cap ( B \cup C) = $$
    $$ A \cup ( B \cap C) = $$
  \end{field}
    \begin{field}
      Distributive Laws
        $$A \cap ( B \cup C) = (A \cap B) \cup (A \cap C)$$
        $$ A \cup ( B \cap C) = (A \cup B) \cap ( A \cup C)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 140 \end{field}
  \begin{field}
    DeMorgan's Laws
    $$(A \cup B)^c = $$
    $$(A \cap B)^c = $$
  \end{field}
    \begin{field}
      DeMorgan's Laws
        $$(A \cup B)^c = A^c \cap B^c$$
        $$(A \cap B)^c = A^c \cup B^c$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 141 \end{field}
  \begin{field}
    Disjoint
  \end{field}
    \begin{field}
        Disjoint: Two events $A$ and $B$ are disjoint ( or mutually exclusive) if $ A \cap B = \emptyset $
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 142 \end{field}
  \begin{field}
    $$P(A_1 \cap A_2 \cap \cdots \cap A_n)  = $$
  \end{field}
  \begin{field}
    $$P(A_1)P(A_2|A_1)P(A_3|A_1A_2) \ldots P(A_n|A_1\cdots A_{n-1}) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 143 \end{field}
  \begin{field}
    $$ P(A,B,C) = $$
  \end{field}
  \begin{field}
    $$ P(A,B,C) = P(A)P(B|A)P(C|A,B)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 144 \end{field}
  \begin{field}
    $$P(A \cup B \cup C) =  $$
  \end{field}
  \begin{field}
    $$P(A \cup B \cup C) =  P(A) + P(B) + P(C) - P(A \cap B) - P(B\cap C) - P(A \cap C) + P(A \cap B \cap C)$$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 145 \end{field}
  \begin{field}
    Pairwise disjoint
  \end{field}
    \begin{field}
        Two Events $A_1, A_2$ are pairwise disjoint ( or mutually exclusive) if $A_i \cap A_j = \emptyset $ for all $i \neq j$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 146 \end{field}
    \begin{field}
      Partition
    \end{field}
    \begin{field}
        If $A_1, A_2, \ldots$ are pairwise disjoint and $\cup_{i=1}^\infty A_i = S$, then the collection $A_1, A_2, \ldots$ forms a partition of $S$.
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 147 \end{field}
    \begin{field}
        Sigma Algebra
    \end{field}
    \begin{field}
        A collection of subsets of $S$ is called a sigma algebra (or Borel field), denoted by $\mathcal{B}$, if it satisfies the following three properties:
            \begin{enumerate}
              \item $\emptyset \in \mathcal{B}$ (the empty set is an element of $\mathcal{B}$)
              \item If $A \in \mathcal{B}$, then $A^c \in \mathcal{B}$ ($\mathcal{B}$ is closed under complementation)
              \item If $A_1, A_2, \ldots \in \mathcal{B}$, then $\cup_{i-1}^\infty A_i \in \mathcal{B} \mathcal{B}$ is closed under countable unions)
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 148 \end{field}
    \begin{field}
        Probability Function / Kolmogorov Axioms
    \end{field}
    \begin{field}
        Given a sample space $S$ and an associated sigma algebra $\mathcal{B}$, a probability function is a function $P$ with domain $\mathcal{B}$ that satisfies:
            \begin{enumerate}
              \item $P(A) \geq 0$ for all $A \in \mathcal{B}$
              \item $P(S) = 1$
              \item If $A_1, A_2, \ldots \mathcal{B}$ are pairwise disjoint, then $P(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)$ (Axiom of Countable Additivity)
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 149 \end{field}
    \begin{field}
      If $A\in \mathcal{B}$ and $B \in \mathcal{B}$ are disjoint, then
          $$P(A\cup B) = P(A) + P(B) $$
        Axiom of Finite Additivity
    \end{field}
    \begin{field}
        If $A\in \mathcal{B}$ and $B \in \mathcal{B}$ are disjoint, then
            $$P(A\cup B) = P(A) + P(B) $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 150 \end{field}
    \begin{field}
        Properties of probability functions
        \begin{enumerate}
              \item $P(\emptyset) = $
              \item $P(A) $
              \item $P(A^c) = $
            \end{enumerate}
    \end{field}
    \begin{field}
      Properties of probability functions
        \begin{enumerate}
              \item $P(\emptyset) = 0$
              \item $P(A) \leq 1$
              \item $P(A^c) = 1 - P(A)$
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 151 \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(B\cap A^c) = $$
    \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(B\cap A^c) = P(B) - P(A \cap B)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 152 \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(A\cup B) = $$
    \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(A\cup B) = P(A) + P(B) - P(A \cap B)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 153 \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then if $A \subset B$ then
    \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then if $A \subset B$ then $P(A) \leq P(B)$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 154 \end{field}
    \begin{field}
        Bonferroni's Inequality
        $$P(A\cap B)  $$
    \end{field}
    \begin{field}
      Bonferroni's Inequality:
        $$P(A\cap B) \geq P(A) + P(B) -1 $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 155 \end{field}
    \begin{field}
        If $P$ is a probability function, then for any partition $C_1, C_2, \ldots P(A) = $
    \end{field}
    \begin{field}
        If $P$ is a probability function, then for any partition $C_1, C_2, \ldots P(A) = \sum_{i=1}^\infty P(A \cap C_i)$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 156 \end{field}
    \begin{field}
        Boole's Inequality
        $$P(\cup_{i=1}^\infty A_i)$$
    \end{field}
    \begin{field}
        If $P$ is a probability function,
            $$P(\cup_{i=1}^\infty A_i) \leq \sum_{i=1}^\infty P(A_i) \text{ for any sets } A_1, A_2, \ldots $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 157 \end{field}
    \begin{field}
        Fundamental Theorem of Counting
    \end{field}
    \begin{field}
        Ifa job consists of $k$ separate tasks, the $i$th of which can be done in $n_i$ ways, $i = 1, \ldots, k$, then the entire job can be done in $n_1 \times n_2 \times \cdots, \times n_k$ ways.
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 158 \end{field}
    \begin{field}
        Ordered without replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
        $$\frac{n!}{(n-r)!}$$

        eg lottery with $n=44$ choices for $r=6$ values, cant use same number twice, order matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 159 \end{field}
    \begin{field}
        Unordered without replacement: number of arrangements of size $r$ from $n$ objects
      \end{field}
    \begin{field}
        $$\binom{n}{r} = \frac{n!}{r!(n-r!)}$$

        eg lottery with $n=44$ choices for $r=6$ values, cant use same number twice, order does not matter (Use ordered without replacement and divide by redundant orderings )
    \end{field}
\end{note}


\begin{note} \begin{field} \tiny 160 \end{field}
    \begin{field}
        Ordered with replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
      Ordered with replacement: number of arrangements of size $r$ from $n$ objects
        $$n^r$$
        eg lottery with $n=44$ choices for $r=6$ values, can use same number twice, order matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 161 \end{field}
    \begin{field}
        Unordered with replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
      Unordered with replacement: number of arrangements of size $r$ from $n$ objects
        $$\binom{n+r-1}{r} = \frac{(n+r-1)!}{r!(n-1)!}$$

        eg lottery with $n=44$ choices for $r=6$ values, can use same number twice, order does not matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 162 \end{field}
  \begin{field}
    Number of arrangements of size $r$ from $n$ objects

    \begin{tabular}{|c| c c|}
      \hline \\
       & Without Replacement & With replacement\\
       \hline \\
       Ordered & & \\
       Unordered & & \\
       \hline
    \end{tabular}
  \end{field}
  \begin{field}
    Number of arrangements of size $r$ from $n$ objects

    \begin{tabular}{|c| c c|}
      \hline \\
       & Without Replacement & With replacement\\
       \hline \\
       Ordered & $\frac{n!}{(n-r)!}$ & $n^r$ \\
       Unordered & $\binom{n}{r}$& $\binom{n+r-1}{r}$ \\
       \hline
    \end{tabular}
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 163 \end{field}
    \begin{field}
        Binomial Coefficient $\binom{n}{r}$
    \end{field}
    \begin{field}
      Binomial Coefficient
        $$\binom{n}{r} = \frac{n!}{r!(n-r)!}$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 164 \end{field}
  \begin{field}
    $$P(A|B)=$$
  \end{field}
  \begin{field}
    $$ P(A|B) = \frac{P(A \cap B)}{P(B)} $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 165 \end{field}
  \begin{field}
    Statistically independent
    $P(A \cap B) = $
  \end{field}
  \begin{field}
    Statistically independent
    $P(A \cap B) = P(A)P(B)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 166 \end{field}
  \begin{field}
    If $A$ and $B$ are independent events, what else is independent?
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $A $ and $B^c$
      \item $A^c$ and $B$
      \item $A^c$ and $B^c$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 167 \end{field}
  \begin{field}
    Mutually independent
  \end{field}
  \begin{field}
    A collection of events $A_1, \ldots , A_n$ are mutually independent for any subcollection $A_{i1}, \ldots , A_{ik}$, we have
    $$ P((\cap_{j=1}^k A_{ij})) = \prod_{j=1}^k P(A_{ij}) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 168 \end{field}
  \begin{field}
    Random variable
  \end{field}
  \begin{field}
    A random variable is a function from a sample space $S$ into the real numbers
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 169 \end{field}
  \begin{field}
    Definition of a pdf
  \end{field}
  \begin{field}
    A function $f_X(x)$ is a pdf (or pmf) of a random variable $X$ if and only if
    \begin{enumerate}
      \item $f_x(x) \geq 0$ for all $x$
      \item $\sum_x f_x(x) = 1$ or $\int_{-\infty}^\infty f_x(x) dx = 1$
    \end{enumerate}
  \end{field}
\end{note}


%%end_tag
%%start_tag Casella Ch2
\begin{note} \begin{field} \tiny 170 \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{enumerate}
      \item If $g$ is an increasing function on $X$, $F_Y(y) = $ for $y \in Y$
      \item If $g$ is a decreasing function on $X$ and $X$ is a continuous random variable, $F_y(y) = $ for $y \in Y$
    \end{enumerate}
  \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{enumerate}
      \item If $g$ is an increasing function on $X$, $F_Y(y) = F_X(g^{-1}(y))$ for $y \in Y$
      \item If $g$ is a decreasing function on $X$ and $X$ is a continuous random variable, $F_y(y) = 1 - F_X(g^{-1}(y))$ for $y \in Y$
    \end{enumerate}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 171 \end{field}
  \begin{field}
    Method of pdf
  \end{field}
  \begin{field}
    Conditions:
    \begin{enumerate}
      \item $g$ is a monotone function
      \item $f_X(x)$ is continuous on $X$
      \item $g^{-1}(y)$ has a continuous derivative
    \end{enumerate}
    Let $X$ have pdf $f_x(x)$ and let $Y = g(Y)$

    $$f_Y(y) = f_x(g^{-1}(y))\big|\frac{d}{dy}g^{-1}(y)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 172 \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{itemize}
      \item If $g$ is an increasing function, $F_Y(y) = $
      \item If $g$ is a decreasing function, and $X$ is a continuous random variable, $F_Y(y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{itemize}
      \item If $g$ is an increasing function, $F_Y(y) = F_x(g^{-1}(y))$
      \item If $g$ is a decreasing function, and $X$ is a continuous random variable, $F_Y(y) = 1 - F_X(g^{-1}(y))$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 173 \end{field}
  \begin{field}
    eg: $X \sim Unif(0,1)$, $Y = -log(X)$
    $F_Y(y) = $
  \end{field}
  \begin{field}
    $F_Y(y) = 1 - F_x(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 173  \end{field}
  \begin{field}
    $X$ is a continuous random variable. For $y > 0$,
    $F_Y(y) = $
  \end{field}
  \begin{field}
    \begin{align*}
      F_Y(y) &= P(Y \leq y)\\
      &= P(X^2 \leq y)\\
      &= P(- \sqrt{y} \leq X \leq \sqrt{y})\\
      &= P(X \leq \sqrt{y}) - P(X \leq -\sqrt{y})\\
      &= F_X(\sqrt{y}) - F_X( - \sqrt{y})
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 174 \end{field}
  \begin{field}
    Pdf of $F_X(g(X))$, where $Y = g(X)$
  \end{field}
  \begin{field}
    Chain rule:
    $f_Y(y) = g'(y)f(g(y))$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 175 \end{field}
  \begin{field}
    Method of pdf if $g$ is not monotone all entire domain
  \end{field}
  \begin{field}
    $f_Y = \sum f_x(g_i^{-1}(y))|\frac{d}{dy}g_i^{-1}(y)|$ $y \in Y$, 0 otherwise

    eg: $Y = X^2$,
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 176 \end{field}
  \begin{field}
    $P(Y  \leq y)$ when $Y = F_X(x)$
  \end{field}
  \begin{field}
    \begin{align*}
      P(Y \leq y) &= P(X \leq F_x^{-1}(y))\\
      &= F_X(F_X^{-1}(y))\\
      &= y
    \end{align*}
    $Y$ is uniformly distributed
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 177 \end{field}
  \begin{field}
    $M_x(t) = $ (discrete )
  \end{field}
  \begin{field}
    $M_x(t) = E(e^{tX}) = \sum_x e^{tX}P(X)$ (discrete )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 178 \end{field}
  \begin{field}
    $M_x(t) = $ (continuous)
  \end{field}
  \begin{field}
    $M_x(t) = E(e^{tX}) = \int_{-\infty}^\infty e^{tX}f_x(x)dx$ (continuous)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 179 \end{field}
  \begin{field}
    $E(X^n) = $
  \end{field}
  \begin{field}
    $E(X^n) = M_x^n(0) = \frac{d^n}{dt^n}M_x(t)|_{t=0}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 180 \end{field}
  \begin{field}
    $M(aX + b)(t) = $
  \end{field}
  \begin{field}
    $M(aX + b)(t) = e^{bt}M_x(at)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 181 \end{field}
  \begin{field}
    If $E(X^n)$ exists then...
  \end{field}
  \begin{field}
    If $E(X^n)$ exists then $E(X^m)$ exists for $m \leq n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 182 \end{field}
  \begin{field}
    If $X_i$ are independent and $Y = a_1X_1 + \cdots + a_nX_n + b$, then $M_Y(t) = $
  \end{field}
  \begin{field}
    If $X_i$ are independent and $Y = a_1X_1 + \cdots + a_nX_n$, then $M_Y(t) = e^{bt}\prod_{i=1}^n M_{X_i}(a_i t) $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 183 \end{field}
  \begin{field}
    Example of using MGF for finding expected value:
    MGF gamma: $(\frac{1}{1 - \beta t})^\alpha$: $E(X) = $
  \end{field}
  \begin{field}
    $E(X) = \frac{\alpha\beta}{(1 - \beta t)^{\alpha+1}}|_{t=0} = \alpha \beta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 184 \end{field}
  \begin{field}
    Using MGF to relate distributions: MGF exp = $( 1 - \beta t)^{-1}$
  \end{field}
  \begin{field}
    $Y = \sum X_i$ is gamma as MGF gamma is $(1 - \beta t)^{-\alpha}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 185 \end{field}
  \begin{field}
    First step in transforming a RV
  \end{field}
  \begin{field}
    Determine support
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 186 \end{field}
  \begin{field}
    $nth$ Moment of X
  \end{field}
  \begin{field}
    $E(X^n)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 187 \end{field}
  \begin{field}
    $n$th central moment of $X$
  \end{field}
  \begin{field}
    $E(X - \mu)^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 188 \end{field}
  \begin{field}
    $(a + b)^n = $
  \end{field}
  \begin{field}
    $(a + b)^n = \sum_{x = 0}^n \binom{n}{x}a^xb^{n-x}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 189 \end{field}
  \begin{field}
    $\sum_{x = 0}^n \binom{n}{x}a^xb^{n-x} = $
  \end{field}
  \begin{field}
    $(a + b)^n$
  \end{field}
\end{note}

%%end_tag
%%start_tag Casella Ch3
\begin{note} \begin{field} \tiny 190 \end{field}
  \begin{field}
    $N$ balls $r$ red $N - r$ green. Select $n$ balls. Probability that $y$ are red?
  \end{field}
  \begin{field}
    Hypergeometric distribution($N,r,n$)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 191 \end{field}
  \begin{field}
    Hypergeometric distribution description $(N,r,n)$
  \end{field}
  \begin{field}
    $N$ is total balls, $r$ is number red balls, $n$ is number balls selected.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 192 \end{field}
  \begin{field}
    Negative binomial description
  \end{field}
  \begin{field}
    Number of Bernoulli trials required to get a fixed number of successes. $r$ being the $r$th success
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 193 \end{field}
  \begin{field}
    Geometric description
  \end{field}
  \begin{field}
    Modeling waiting time. $X$ is the trial at which the first success occurs.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 194 \end{field}
  \begin{field}
    Location-scale family for $f(x)$
  \end{field}
  \begin{field}
    $1/\sigma f((x - \mu)/\sigma)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 195 \end{field}
  \begin{field}
    Given $X$ give the mean and variance for the location-scale random $Y= 1/\sigma f((y - \mu)/\sigma)$ variable
  \end{field}
  \begin{field}
    $E(Y) = \sigma E(X) + \mu$, $V(Y) = \sigma^2 V(X)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 196 \end{field}
  \begin{field}
    $X \sim Pois(\lambda)$ $P(X = x+1) = $
  \end{field}
  \begin{field}
    $X \sim Pois(\lambda)$ $P(X = x+1) = \frac{\lambda}{x + 1}P(X = x)$
  \end{field}
\end{note}

%%end_tag
%%start_tag Casella Ch4

\begin{note} \begin{field} \tiny 197 \end{field}
  \begin{field}
    $f(y|x) = $
  \end{field}
  \begin{field}
    $f(y|x) = \frac{f(x,y)}{f_x(x)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 198 \end{field}
  \begin{field}
    $E(g(Y)|x) = $
  \end{field}
  \begin{field}
    $E(g(Y)|x) = \int_{-\infty}^\infty g(y)f(y|x) dy $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 199 \end{field}
  \begin{field}
    Example of calculating donditional pdfs
$f(x,y) = e^{-y}, 0 < x < y < \infty$. $f(y|x) = $

  \end{field}
  \begin{field}
    \begin{align*}
      f_x(x) = \int_{-\infty}^\infty f(x,y) dy \\
      &= e^{-x}\\
      \\
      f(y|x) &= \frac{f(x,y)}{f_x(x)}\\
      &= \frac{e^{-y}}{e^{-x}} \text{ if } y > x\\
      &= \frac{0}{e^{-x}} \text{ if } y \leq x
    \end{align*}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 200 \end{field}
  \begin{field}
    Let ($X,Y$) be given as $f(x,y)$. Then $X$ and $Y$ are independent if
  \end{field}
  \begin{field}
    Let ($X,Y$) be given as $f(x,y)$. Then $X$ and $Y$ are independent if there exist functions $g(x), h(y)$ such that $f(x,y) = g(x)h(y)$ (factorization - don't need to compute marginals )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 201 \end{field}
  \begin{field}
    Let $X,Y$ be independent. Then $E(g(X)h(Y)) = $
  \end{field}
  \begin{field}
    $E(g(X)h(Y)) = (E(g(X)))(E(h(Y)))$

    example: $E(X^2Y) = E(X^2)E(Y)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 202 \end{field}
  \begin{field}
    $X,Y$ independent\\ $Z = X + Y$  \\ $M_Z(t) = $
  \end{field}
  \begin{field}
    $M_Z(t) = M_x(t)M_Y(t)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 203 \end{field}
  \begin{field}
    Method of pdf bivariate
  \end{field}
  \begin{field}
    $f_{u,v}(u,v) = f_{x,y}(h_1(u,v),h_2(u,v))|J|$\\
    Where $|J| = \begin{vmatrix}
      \frac{\partial  x}{\partial  u} & \frac{\partial  x }{\partial v }\\ \frac{\partial  y }{\partial u } & \frac{\partial  y }{\partial v }
    \end{vmatrix} = \frac{\partial  x }{\partial u } \frac{\partial  y}{\partial v } - \frac{\partial  y }{\partial u } \frac{\partial  x }{\partial v }$

    and $u = g_1(x,y), v = g_2(x,y)$ and $x = h_1(x,y), y = h_2(x,y)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 204 \end{field}
  \begin{field}
    $X,Y$ independent, $g(X)$ a function only of $X$ and $h(Y)$ a function only of $Y$. Then
  \end{field}
  \begin{field}
    $g(X)$ and $g(Y)$ are independent.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 205 \end{field}
  \begin{field}
    Correlation
  \end{field}
  \begin{field}
    $\rho_{XY} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 206 \end{field}
  \begin{field}
    $m$ independent trials, each trial resulting in one of $n$ outcomes, with probabilities $p_1, \ldots, p_n$. $X_i$ is the count of the number of times the $i$th outcome occured in the $m$ trials.
  \end{field}
  \begin{field}
    Multinomial distribution
    $f(x_1, \ldots, x_n) = \frac{m!}{x_1! \cdots x_n!}p_1^{x_i} \cdots p_n^{x_n}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 207 \end{field}
  \begin{field}
    $|E(XY)| \leq $ (Cauchy-Schwartz)
  \end{field}
  \begin{field}
    $|E(XY)| \leq E(|XY|) \leq (E(|X|^2))^{1/2}(E(|Y|^2))^{1/2}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 208 \end{field}
  \begin{field}
    $E(g(X)) \geq $ where $g$ is a convex function
  \end{field}
  \begin{field}
    $E(g(X)) \geq g(E(X))$ where $g$ is a convex function (Jensen's inequlity)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 209 \end{field}
  \begin{field}
    Ranking of types of means
  \end{field}
  \begin{field}
    $\mu_{\text{harmonic}} \leq \mu_{\text{geometric}} \leq \mu_{\text{arithmetic}}$ By Jensens inequality (using logs )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 210 \end{field}
  \begin{field}
    Linear transformations of multivariate normal $X \sim N(\vec{\mu},\Sigma) $\\
    $A \vec{X} + \vec{b}$
  \end{field}
  \begin{field}
    $A \vec{X} + \vec{b} \sim N(A \vec{\mu} + \vec{v}, A \Sigma A^t)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 211 \end{field}
  \begin{field}
    $X \sim N(\vec{\mu},\Sigma) $\\

    $\vec{X}_a | \vec{X}_b \sim $
  \end{field}
  \begin{field}
    $\vec{X}_a | \vec{X}_b \sim N\big(\vec{\mu_a} + \Sigma_{ab} \Sigma^{-1}_{bb}(\vec{x}_b - \vec{\mu}_b), \Sigma_{ba} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}$\\

ex: $(X_1,X_2,X_3), \vec{\mu} = (1,2,3)^t, \Sigma = \begin{pmatrix}
  3 & 1 & 0 \\ 1 & 3 & 1 \\ 0 & 1 & 3
\end{pmatrix}$
$X1,X_3 | X_2 = 1$\\
$a = \{1,3\}, b = \{2\}$\\
$\mu_a = (1,3)^t, \mu_b = 1$\\
$\Sigma_{aa} = \begin{pmatrix}
  3 & 0 \\ 0 & 3
\end{pmatrix}, \Sigma_{ab} = (1,1)^t$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 212 \end{field}
  \begin{field}
    $(X,Y)$ multinomial \\
    $aX + bY \sim $
  \end{field}
  \begin{field}
    $aX + bY \sim N(a \mu_x + b\mu_y, a^2 \sigma_x^2 + b^2 \sigma_y^2 + 2 ab \rho \sigma_x \sigma_y)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 213 \end{field}
  \begin{field}
    $(X,Y)$ multinomial \\
    $Y|X\sim $
  \end{field}
  \begin{field}
    $Y|X\sim  N(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x - \mu_x), \sigma_Y^2(1 - \rho^2))$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 214 \end{field}
  \begin{field}
    CDF for Max order statistic
  \end{field}
  \begin{field}
    $(F(x))^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 215 \end{field}
  \begin{field}
    PDF for Max order statistic
  \end{field}
  \begin{field}
    $n(F(x))^{n-1}f(x)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 216 \end{field}
  \begin{field}
    CDF for Min order statistic
  \end{field}
  \begin{field}
    $1 - (1 - F(x))^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 217 \end{field}
  \begin{field}
    PDF for Min order statistic
  \end{field}
  \begin{field}
    $n(1 - F(x))^{n-1}f(x)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 218 \end{field}
  \begin{field}
    CDF for $k$th order statistic
  \end{field}
  \begin{field}
    $F_{(k)}(x) = \sum_{j = k}^n \binom{n}{j}(F(x))^j(1 -F(x))^{n-j}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 219 \end{field}
  \begin{field}
    PDF for $k$th order statistic
  \end{field}
  \begin{field}
    $f_{(k)}(x) = k \binom{n}{k}f(x)F(x)^{k-1}(1 - F(x))^{n-k}$
  \end{field}
\end{note}

%%end_tag

%%end_tag
%%start_tag Theory 2
\tags{TheoryTwo t2}

\begin{note} \begin{field} \tiny 220 \end{field}
  \begin{field}
    Definition of Convergence
  \end{field}
  \begin{field}
    A sequence $\{a_n\}_{n > 1}$ of real numbers is said to \textbf{converge} to a point $a \in \mathbb{R}$ if for any $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that for all $m > N $ we have $|a_m - a| < \epsilon $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 221 \end{field}
  \begin{field}
    Example of convergence: $a_n = \frac{1}{n}$
  \end{field}
  \begin{field}
    For any $\epsilon > 0$, choose $N$ such that $\frac{1}{N} < \epsilon$. Then for any $m > N$ we have that

    $$a_n = \frac{1}{n} < \frac{1}{N} < \epsilon$$

    and therefore $|a_m - 0| = \frac{1}{n} < \epsilon$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 222 \end{field}
  \begin{field}
    Given two convergent sequences $\{a_n\}$ and $\{b_m\}$ such that $a_m \to a$ and $b_m \to b$\\
    $\lim_{n \to \infty} a_nb_n = $
  \end{field}
  \begin{field}
    Given two convergent sequences $\{a_n\}$ and $\{b_m\}$ such that $a_m \to a$ and $b_m \to b$\\
    $\lim_{n \to \infty} a_nb_n = (\lim_{n \to \infty}a_n)(\lim_{n \to \infty}b_n) = ab$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 223 \end{field}
  \begin{field}
    Definition: Convergence in probability
  \end{field}
  \begin{field}
    A sequence of random variables $\{X_n\}_{n \geq 1}$ \textbf{converges in probability } to a random variable $X$, if for every $\epsilon > 0$, $$\lim_{n \to \infty}P(|X_n - X| \geq \epsilon) = 0 $$
    We write $X_n \overset{p}{\to} X$

    Equivalently, $x_m \overset{p}{\to} x$ if $\lim_{n \to \infty}P(|x_n - x| < \epsilon) = 1$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 224 \end{field}
  \begin{field}
    Convergence in probability example:
    Let $\{x_n\}$ be a sequence of random variables such that $x_n \sim N(0,1/m^2)$\\
    Show that $x_n \overset{p}{\to} 0$:
  \end{field}
  \begin{field}
    Let $\epsilon > 0$. We obtain $P(|x_n - 0|) = P(x_n > \epsilon) + P(X_n < -\epsilon )$. ie we are looking at the tail probabilities.

    Now,
    \begin{align*}
      P(X_n < -\epsilon) + P(x_n > \epsilon) &=
      P(nx_n < n\epsilon) + P(nx_n > n\epsilon)\\
      &= \Phi(n\epsilon) + 1 - \Phi(n \epsilon)\\
      &= 2\Phi(-n\epsilon) \underset{n\to \infty}{\to}  0
    \end{align*}

    Therefore $x_n \overset{p}{\to} 0$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 225 \end{field}
  \begin{field}
    Example convergence in probability
    Let $W \sim N(0,1)$ and $U \sim Unif(0,1)$, and define the sequence $\{x_n\}_{n \geq 1}$ as $x_n = W $with prob $1 - 1/n, U$ with prob $ 1/n$

    Show that $x_n \overset{p}{\to} W $

  \end{field}
  \begin{field}
    Let $\epsilon > 0$ Then.

    \begin{align*}
      P(|X_n - W| > \epsilon) &= P(|X_n - W| > \epsilon | X_n = W)P(X_n = W) \\
      &+ P(|X_n - W| > \epsilon | X_n = U)P(X_n = U)\\
      &= 0 \cdot (1 - 1/n) + p_n (1/n)
    \end{align*}

    Where $p_n  $ is a probability, and therefore $0 \leq p_n \leq 1 $

    It follows that $p_n \frac{1}{n} \underset{n \to \infty}{\to} 0$, and therefore $P(|X_n - W| > \epsilon ) \underset{n \to \infty}{\to} 0$, for all $\epsilon > 0$, so that $X_n \overset{p}{\to} W$.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 226 \end{field}
  \begin{field}
    Does $X_n \overset{p}{\to} c$ imply $E(X_n) \to c $?
  \end{field}
  \begin{field}
    Let $X_n = 0$ with probability $1 - 1/n$, $n^2 $ with probability $1/n$
    Then $P(|X_n - 0| > \epsilon) \leq P(X_n = n^2) = 1/n \underset{n \to \infty}{\to} 0$
    On the other hand,
    $E(X_n) = 0 \cdot P(X_n =0) + n^2P(X_n = n^2) = 0 + n^2 \frac{1}{n} = n \underset{n \to \infty}{\to} \infty$.
    Therefore $X_n \overset{p}{\to} c$ does not imply $E(X_n) \to c $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 227 \end{field}
  \begin{field}
    Does $E(X_n) \to c $ imply $X_n \overset{p}{\to}c$?
  \end{field}
  \begin{field}
    Let $X_n = 0, $ with prob $1 - 1/n$, $n$ with prob $1/n$.
    Then $E(X_n) = 0 \cdot P(X_n = 0) + n P(X_n = n) = 0  + n 1/n = 1$ for all n.
    But $P(|X_n - 0| > \epsilon) \leq P(X_n = n) = \frac{1}{n } \underset{n \to \infty}{\to} 0$
    It follows, $X_n \overset{p}{\to} 0 $, and therefore we have $E(X_n) \to c $does not imply $X_n \overset{p}{\to}c$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 228 \end{field}
  \begin{field}
     Suppose $\{X_n\}_{n \geq 1}$ and $\{Y_n\}_{n \geq 1}$ be two sequences of random variables such that $X_n \overset{p}{\to} x_0$ and $Y_n \overset{p}{\to} y_0$ as $n \to \infty$, where $x_o, y_0 \in \mathbb{R}$

     What properties do we have?
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $X_n \pm Y_m \overset{p}{\to} x_0 \pm y_0$ as $n$ increases to $\infty$
      \item $X_nY_n \overset{p}{\to} x_0y_0$ as $n$ increases to $\infty$
      \item $X_n/Y_n \overset{p}{\to} x_0/y_0$ as $n$ increases to infinity, provided that $P(Y_n = 0) = 0 $ fro all $n$ and $y_0 \neq 0$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 229 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $x_n \overset{p}{\to} x_0 \in \mathbb{R}$, as $n \to \infty$, and let $g: \mathbb{R} \to \mathbb{R}$ be a continuous function . Then $$g(X_n) \overset{p}{\to}  \text{ as } n \to \infty$$
  \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $x_n \overset{p}{\to} x_0 \in \mathbb{R}$, as $n \to \infty$, and let $g: \mathbb{R} \to \mathbb{R}$ be a continuous function . Then $$g(X_n) \overset{p}{\to} g(x_0) \text{ as } n \to \infty$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 230 \end{field}
  \begin{field}
    Proof of: Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $x_n \overset{p}{\to} x_0 \in \mathbb{R}$, as $n \to \infty$, and let $g: \mathbb{R} \to \mathbb{R}$ be a continuous function . Then $$g(X_n) \overset{p}{\to} g(x_0) \text{ as } n \to \infty$$
  \end{field}
  \begin{field}
    Since $g$ is continuous at $X = x_0$, we have that for any $\epsilon > 0$, there exits $\delta > 0$ such that $|g(x) - g(x_0)| > \epsilon $ implies $|x - x_0| > \delta $

    We obtain

    $$0 \leq P(|g(X_n) - g(x_0)| > \epsilon ) \leq P(|X_n - x_0| > \delta) \underset{n \to \infty}{\to} 0$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 231 \end{field}
  \begin{field}
    Weak Law of Large numbers
  \end{field}
  \begin{field}
    Let $X_1, X_2, X_3 \ldots$ Be a sequence of iid random variables with $E(X_1) = \mu$ (finite) and $V(X_1) = \sigma^2 < \infty$, and define $\bar{X_n} = \frac{1}{n} \sum_{i = 1}^n X_i$ (the sample mean).

    Then $$ \bar{X_n }\overset{p}{\to}\mu \text{ as } n \to \infty$$

  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 232 \end{field}
  \begin{field}
    Proof of Weak Law of Large Numbers
  \end{field}
  \begin{field}
    \begin{align*}
      P(|\bar{X_n} - \mu| > \epsilon ) &= P((\bar{X_n} - \mu)^2 > \epsilon^2)\\
      &\leq \frac{E((\bar{X_n} - \mu)^2)}{\epsilon^2} \text{ by Chebyshev's Inequality}\\
      &= \frac{V(\bar{X_n})}{\epsilon^2} \text{ by def of variance}\\
      &= \frac{\sigma^2}{n \epsilon^2} \underset{n \to \infty}{\to} 0
    \end{align*}

    Therefore $\bar{X_n} \overset{p}{\to} \mu$
  \end{field}
\end{note}


%%start_tag Wk2

\begin{note} \begin{field} \tiny 233 \end{field}
  \begin{field}
    Consistency
  \end{field}
  \begin{field}
    If our estimate converges in probability to the value of the parameter of interest as the sample size $n$ increases
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 234 \end{field}
  \begin{field}
    Consistency of $S^2$
  \end{field}
  \begin{field}
    Suppose $X_1, X_2, \ldots$ is a sequence of iid random varialbes with $E(X_1) = \mu$ finite and $V(X_1) = \sigma^2 < \infty$

    and define $$ S_n^2 = \frac{1}{n-1}\sum_{i = 1}^n(X_i - \bar{X_n})^2 \quad \text{The sample variance}$$

    Can we show that $S_n^2$ is a consistent estimate of $\sigma^2$? In other words, can we show taht $S_n^2 \overset{p}{\to} \sigma^2 \text{ as } n \to \infty$

    Using Chebychev's inequality, we obtain

    \begin{align*}
      P(|S_n^2 - \sigma^2| > \epsilon ) &\leq \frac{E[(S_n^2 - \sigma^2)^2]}{\epsilon^2}\\
      &= \frac{V(S_n^2)}{\epsilon^2}
    \end{align*}

    There fore, a sufficient condition that $S_n^2$ converges in probablility to $\sigma^2$ is that the variance of $S_n^2$  $V(S_n^2) \to 0$, as $n \to \infty$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 235 \end{field}
  \begin{field}
    $V(S_n^2) \to 0$ as long as
  \end{field}
  \begin{field}
    $V(S_n^2) \to 0$ as long as the fourth central moment $\mu_4 = E[(X_1 - \mu)^4]$ is finite.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 236 \end{field}
  \begin{field}
    Khinchin's WLLN
  \end{field}
  \begin{field}
    Let $X_1, X_2, \ldots$ be a sequence of iid random variables with $E(X_1) = \mu$ (finite). Then, $\bar{X_n} \overset{p}{\to} \mu$ as $n \to \infty$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 237 \end{field}
    \begin{field}
      Let $X_1, X_2 \ldots$ be a sequence of random variables, such that for some $r > 0$ and $c \in \mathbb{R}$, $E[|X_n - c|^r] \underset{n \to \infty}{\to} 0$. Then $X_n \overset{p}{\to} $, as $n \to \infty$
    \end{field}
    \begin{field}
      (A general result to establish convergence in probability )

      Let $X_1, X_2 \ldots$ be a sequence of random variables, such that for some $r > 0$ and $c \in \mathbb{R}$, $E[|X_n - c|^r] \underset{n \to \infty}{\to} 0$. Then $X_n \overset{p}{\to} c$, as $n \to \infty$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 238 \end{field}
  \begin{field}
    Consistent estimator for $X_1, X_2, \ldots X_n \sim $ iid Univorm($0,\theta$), $\theta > 0$. ( and sketch of proof )
  \end{field}
  \begin{field}
    $X_{(n)} = \max(X_1, \ldots X_n)$ (the largest order statistic)

    Proof

    First recall that the pdf of $X_{(n)}$ is given by $$f(x) = nx^{n-1} \theta^{-n}, 0 < x < \theta, 0 \text{otherwise} $$

    We obtain

    \begin{align*}
      E(X_{(n)}) &= \int_{0}^\theta x f(x) dx \\
      &= n \theta^{-n} \int_0^\theta x^n dx \\
      &= \frac{n}{n-1}\theta\\
      E(X_{(n)}^2) &= \int_0^\theta x^2f(x)dx \\
      &= n\theta^{-n} \int_0^\theta x^{n+1}dx \\
      &= \frac{n}{n + 2} \theta^2
    \end{align*}

    We have

    \begin{align*}
      E[(X_{(n)} - \theta)^2] &= E(X_{(n)}^2) - 2\theta E(X_{(n)}) + \theta^2 \\
      &= \frac{n}{n + 2}\theta^2 - 2\theta \frac{n}{n + 1} \theta + \theta^2\\
      &\cdots\\
      &= \frac{2\theta^2}{(n + 1)(n + 2)} \underset{n \to \infty}{\to} 0
    \end{align*}

    Hence, taking $c = 0$ and $r = 2$, from the previous theorem, we obtain $X_{(n)} \overset{p}{\to} \theta$ as $n \to \infty$

  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 239 \end{field}
  \begin{field}
    Definition Almost Sure Convergence
  \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables is said to converge \textbf{Almost Surely} to a random variable $X$ if for every $\epsilon > 0$,
    $$ P(\lim_{n \to \infty}|X_n - X| > \epsilon) = 0$$

    We write $X_n \overset{a.s}{\to} X$ as $n \to \infty$

  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 240 \end{field}
  \begin{field}
    Strong Law of Large Numbers
  \end{field}
  \begin{field}
    Let $X_1, X_2, \ldots$ be an iid sequence of random variables, with $E(X_1) = \mu$ (finite) and $V(X_1) = \sigma^2 < \infty$. Then,
    $$\bar{X_n} \overset{a.s}{\to} \mu \quad \text{as } \mu \to \infty$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 241 \end{field}
  \begin{field}
    Does convergence in probability imply convergence almost surely?
  \end{field}
  \begin{field}
    No.
    Let $\Omega = [0.1]$, with uniform probability distribution.  Define the sequence $\{X_n\}_{n \geq 1}$ as:
    \begin{align*}
      X_1(\omega) &= \omega + \mathbb{I}_{[0,1]}(\omega)\\
      X_2(\omega) &= \omega + \mathbb{I}_{0,1/2}(\omega)\\
      X_3(\omega) &= \omega + \mathbb{I}_{1/2,1}(\omega)\\
      X_4(\omega) &= \omega + \mathbb{I}_{0,1/3}(\omega)\\
      X_5(\omega) &= \omega + \mathbb{I}_{1/3,2/3}(\omega)\\
      &\vdots
    \end{align*}
    $X_5(\omega) = \omega + 1$

    Let $X(\omega) = \omega$, then it is easy to show that $X_n \overset{p}{\to} X$ because $P(|X_n - X| \geq \epsilon) = P([a_n,b_n])$, where $l_n = \text{length}([a_n, b_n]) \underset{n \to \infty}{\to} 0$.

    However $X_n$ does not converge to $X$ almost surely, because for every $\omega \in [0,1]$, alternates between $\omega$ and $\omega + 1$, infinetly often as $n \to \infty$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 242 \end{field}
  \begin{field}
    Convergence in Distribution
  \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables converges in distribution to a random variable $X$ if,
    $$ \lim_{n \to \infty} F_{X_n}(x) = F_X(x)$$

    at all points $x$ where $F_X(x)$ is continuous

    We write $X_n \overset{d}{\to} X$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 243 \end{field}
  \begin{field}
    Example of convergence in distribution

    Let $X_n \sim N(0, \frac{n+1}{n})$, and $X \sim N(0,1)$. We want to show that $X_n \overset{d}{\to} X$.
  \end{field}
  \begin{field}
    \begin{align*}
      P(X_n \leq X) &= P(\sqrt{\frac{n}{n + 1}}X_n \leq \sqrt{\frac{n}{n+1}}x)\\
      &= \Phi(\sqrt{\frac{n}{n + 1}}x) \underset{n \to \infty}{\to} \Phi(x)
    \end{align*}

    And we obtain that $F_{X_n} \to \Phi(x) = F_X(x), \forall x$, and therefore $X_n \overset{d}{\to} X$
  \end{field}
\end{note}

%%end_tag
%%start_tag Wk3

\begin{note} \begin{field} \tiny 244 \end{field}
  \begin{field}
    Does Convergence in probability imply convergence in distribtuion?
  \end{field}
  \begin{field}
    Yes
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 245 \end{field}
  \begin{field}
    Does Convergence in distribution imply convergence in probability?
  \end{field}
  \begin{field}
    No - unless converges in distribution to a constant
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 246 \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables converges in probability to a constant $c \in \mathbb{R}$ if and only if
  \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables converges in probability to a constant $c \in \mathbb{R}$ if and only if the sequence converges in distribution to $c$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 247 \end{field}
  \begin{field}
    If $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} Y$ we have that
    \begin{enumerate}
      \item $X_n \pm Y_n $
      \item $X_nY_n $
    \end{enumerate}
  \end{field}
  \begin{field}
    In general it is not true that if $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} Y$ we have that
    \begin{enumerate}
      \item $X_n \pm Y_n \overset{d}{\to} X + Y$
      \item $X_nY_n \overset{d}{\to} XY$
    \end{enumerate}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 248 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $X_n \overset{d}{\to}X$, for some random variable $X$ (possibly a constant). Then for any continuous funciton $g:\mathbb{R} \to \mathbb{R}$, we have $g(X_n) \overset{d}{\to} $
  \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $X_n \overset{d}{\to}X$, for some random variable $X$ (possibly a constant). Then for any continuous funciton $g:\mathbb{R} \to \mathbb{R}$, we have $g(X_n) \overset{d}{\to} g(X)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 249 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ and $\{Y_n\}_{n \geq 1}$ be two sequences of random variables such that $X_n \overset{d}{\to}X$ for some random variable $X$ (possibly a constant) and $Y_n \overset{p}{\to} c \in \mathbb{R}$

    Then, as $n \to \infty$,

    \begin{enumerate}
      \item $X_n \pm Y_n \overset{d}{\to} $
      \item $X_nY_n \overset{d}{\to} $
      \item $X_n/Y_n \overset{d}{\to} \quad \text{ provided } P(Y_n = 0) = 0 \forall n \text{ and } c \neq 0$
    \end{enumerate}
  \end{field}
  \begin{field}
    Slutsky's Theorem
    Let $\{X_n\}_{n \geq 1}$ and $\{Y_n\}_{n \geq 1}$ be two sequences of random variables such that $X_n \overset{d}{\to}X$ for some random variable $X$ (possibly a constant) and $Y_n \overset{p}{\to} c \in \mathbb{R}$

    Then, as $n \to \infty$,

    \begin{enumerate}
      \item $X_n \pm Y_n \overset{d}{\to} X \pm c$
      \item $X_nY_n \overset{d}{\to} cX$
      \item $X_n/Y_n \overset{d}{\to} X/c \quad \text{ provided } P(Y_n = 0) = 0 \forall n \text{ and } c \neq 0$
    \end{enumerate}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 250 \end{field}
  \begin{field}
    Central Limit Theorem
  \end{field}
  \begin{field}
    Let $X_1, X_2, \ldots$ be an iid sequence of random variables, with $E(X_1) = \mu$(finite) and $V(X_1) = \mu^2 < \infty$

    Then, for $\bar{X_n} = \frac{1}{n }\sum_{i = 1}^\infty X_i$ (the sample mean), we have that
    $$ \frac{\sqrt{n}(\bar{X_n} - \mu)}{\sigma} \overset{d}{\to} N(0,1) \quad \text{as } n \to
    \infty$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 251 \end{field}
  \begin{field}
    Equivalent results of CLT
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $\frac{(\bar{X_n} - \mu)}{\frac{\sigma}{\sqrt{n}}} \overset{d}{\to}N(0,1)$
      \item $\sqrt{n}(\bar{X_n} - \mu) \overset{d}{\to} N(0,\sigma^2)$
      \item $\frac{\sum_{i=1}^n X_i - n\mu}{\sqrt{n}\sigma} \overset{d}{\to}N(0,1)$
      \item $\bar{X_n} \overset{d}{\to} N(\mu, \sigma^2/n)$
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 252 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that the mgf $M_{X_n}(t)$ of $X_n$ exists in a neighborhood of 0, for all , and suppose that

    $$ \lim_{n \to \infty} M_{X_n}(t) = M_X(t) \quad \text{for all } t \text{ in a neighborhood of }0$$
    where $M_X(t)$ is the mgf for some random variable $X$. Then,
  \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that the mgf $M_{X_n}(t)$ of $X_n$ exists in a neighborhood of 0, for all , and suppose that

    $$ \lim_{n \to \infty} M_{X_n}(t) = M_X(t) \quad \text{for all } t \text{ in a neighborhood of }0$$
    where $M_X(t)$ is the mgf for some random variable $X$. Then, there exists a unique cdf $F_x(x)$ whose moments are determined by $M_y(t)$ and for all $x$, where $F_x(x)$ is continuous we have $\lim_{n \to \infty} F_{X_n}(x) = F_x(x)$
  \end{field}
\end{note}
%%end_tag
%%start_tag Wk4

\begin{note} \begin{field} \tiny 253 \end{field}
  \begin{field}
    $\frac{\sqrt{n}(\bar{X} - \mu)}{S_n} \overset{d}{\to} $
  \end{field}
  \begin{field}
    Using the CLT, and slutsky's theorem, we have

    $$\frac{\sqrt{n}(\bar{X} - \mu)}{S_n} = \frac{\sigma}{S_n}\cdot \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \overset{d}{\to} N(0,1) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 254 \end{field}
  \begin{field}
    $ g(X) \approx $ \\
$E(g(X)) \approx $, $V(g(X))\approx $
  \end{field}
  \begin{field}
    $$ g(X) \approx g(\mu) + g'(X)(X-\mu)$$ Using a first order taylor approximation
    $E(g(X)) \approx g(\mu)$, $V(g(X))\approx [g'(\mu)]^2V(X)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 255 \end{field}
  \begin{field}
    Delta Method
  \end{field}
  \begin{field}
    Let $\{Y_n\}_{n \geq 1}$ be a sequence of random variables such that $\sqrt{n}(Y_n - \theta) \overset{d}{\to} N(0,\sigma^2)$ as $n \to \infty$. Suppose that for a given function $g$ and a specific value of $\theta$, $g'(\theta)$ exists and is not equal to zero. Then
    $$\sqrt{n}(g(Y_n) - g(\theta)) \overset{d}{\to} N(0,\sigma^2[g'(\theta)]^2)$$

    as $n\to \infty$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 256 \end{field}
  \begin{field}
    Second Order delta method
  \end{field}
  \begin{field}
    Let $\{Y_n\}_{n \geq 1}$ be a sequence of random variables such that $\sqrt{n}(Y_n - \theta ) \overset{d}{\to} N(0,\sigma^2)$ as $n\to \infty$.
    And that for a given function $g$ as specific value of $\theta$, we hvae $g'(\theta)=0$, but $g''(\theta)$ Exists and is not equal to 0. Then

    $$\sqrt{n}(g(Y_n) - g(\theta)) \overset{d}{\to} \sigma^2 \frac{g''(\theta)}{2}\chi^2_1 \quad \text{as } n \to \infty$$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 257 \end{field}
  \begin{field}
    $\chi_n^2 \dot{\sim} $ for sufficiently large $n$
  \end{field}
  \begin{field}
    $\chi_n^2 \dot{\sim} N(n,2n)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 258 \end{field}
  \begin{field}
    Definition Statistic
  \end{field}
  \begin{field}
    Let $X_1, \ldots, X_n$ be a random sample from a given population. Then, any \underline{observable} real-valued (or vector-valued) function $T(\mathbf{X}) = T(X_1, \ldots, X_n)$ of the random variables $X_1, \ldots, X_n$ is called a \textbf{Statistic}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 259 \end{field}
  \begin{field}
    Sampling Distribution
  \end{field}
  \begin{field}
    The probability distribution of the statitic $T(\mathbf{X})$ is called the \textbf{Sampling Distribution} of $T(\mathbf{X})$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 260 \end{field}
  \begin{field}
    Sufficient Statistic
  \end{field}
  \begin{field}
    A statistic $T(\mathbf{X})$ is a \textbf{Sufficient Statistic} for $\theta$, if the conditional distribution of the sample $\mathbf{X}$ given the value of $T(\mathbf{X})$ does not depend on $\theta$
  \end{field}
\end{note}
%%end_tag
%%start_tag Wk5

\begin{note} \begin{field} \tiny 261 \end{field}
  \begin{field}
    Determine if $T(\mathbf{X}) = \sum X_i$ where $X_i\sim Bern(p)$ is sufficient for $p$ using definition of sufficiency
  \end{field}
  \begin{field}
    \begin{align*}
      P(\mathbf{X} = \mathbf{x}\big| T = t) &= \frac{P(\cap_{i = 1}^n X_i = x_i)}{P(T = t)}\\
      &= \prod_{i = 1}^n \frac{P(X_i = x_i)}{P(T = t)} \quad \text{by independence}\\
      &= \frac{p^{\sum_{i = 1}^n x_i}(1 - p)^{n - \sum_{i = 1}^n x_i}}{\binom{n}{t}p^t(1 -p)^{n-t}} \quad \text{Because } T \sim \text{Binom}(n,p)\\
      &= \frac{p^t(1-p)^{n-t}}{\binom{n}{t}p^t(1-p)^{n-t}} \quad \text{because }t = \sum_{i = 1}^n x_i\\
      &= \frac{1}{\binom{n}{t}} \quad \text{which is free of  }p
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 262 \end{field}
  \begin{field}
    How to show sufficiency (not using factorization)
  \end{field}
  \begin{field}
    Let $p(\mathbf{X}|\theta)$ be the joint PDF or PMF of $\mathbf{X}$ and $q(t|\theta)$ the PDF or PMF of the statistic $T(\mathbf{X})$. Then $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if for every $\mathbf{X}$ in the sample space, the ratio

    $$ \frac{p(\mathbf{x}|\theta)}{q(T(\mathbf{x})|\theta)}$$

    is constant as a function of $\theta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 263 \end{field}
  \begin{field}
    Suppose that $X_1, \ldots X_n$ are iid $N(\mu,\sigma^2)$ where $\sigma^2$ is known. If the statistic $T(\mathbf{X}) = \bar{X_n}$ sufficient for $\mu$?
  \end{field}
  \begin{field}
    \begin{align*}
       \frac{f(\mathbf{x}|\mu)}{q(T(\mathbf{X})|\mu)} &=
       \frac{(2\pi\sigma^2)^{n/2}e^{-\frac{1}{2\sigma^2}[\sum_{i = 1}^n(x_i - \bar{x})^2 + n(\bar{x} - \mu)^2]}}{(2\pi\sigma/n)^{-1/2}e^{-\frac{1}{2\sigma^2}(\bar{x}-\mu)^2}}\\
       &= n^{-1/2}(2\pi\sigma^2)^{-(n-1)/2}e^{-\frac{1}{2\sigma^2}\sum_{i = 1}^n(x_i - \bar{x})^2}
    \end{align*}

    Which does not depend on $\mu$, and therefore $\bar{X_n}$ is suffient for $\mu$ as long as $\sigma^2$ is known
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 264 \end{field}
  \begin{field}
    The joint pdf of the sample $\mathbf{X} =(X_1, X_2, \ldots X_n)$ isSuppose that $X_1, \ldots X_n$ are iid $N(\mu,\sigma^2)$ where $\sigma^2$ is known.
  \end{field}
  \begin{field}
    \begin{align*}
      f(\mathbf{x}|\mu) &= \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-1}{2\sigma^2}(x_i - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2} e^{-1/2\sigma^2 \sum_{i = 1}^n (x_i - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2} e^{-1/2\sigma^2 \sum_{i = 1}^n (x_i - \bar{x} + \bar{x} - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2} e^{-1/2\sigma^2 \sum_{i = 1}^n (x_i - \bar{x})^2 + 2(\bar{x} - \mu)\sum_{i = 1}^n (x_i - \bar{x}) + n(\bar{x} - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2}e^{-1/2\sigma^2 (\sum_{i = 1}^n (x_i - \bar{x})^2 + n(\bar{x} - \mu)^2)}
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 265 \end{field}
  \begin{field}
    Show a statistic $T(\mathbf{X})$ is sufficient
  \end{field}
  \begin{field}
    Neyman factorization theorem
    Let $f(\mathbf{x}|\theta)$ denote the joint pdf or pmf of the sample $\mathbf{X}$, A statistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if and only if there exists functions $g(t|\theta)$ and $h(\mathbf{x})$ such that for all sample points $\mathbf{x}$ and all values of $\theta$ we can write
    $$ f(\mathbf{x}|\theta) = g(T(\mathbf{}x)|\theta)h(\mathbf{x})$$

    Note, in the theorem
    \begin{itemize}
      \item The function $g(T(\mathbf{X})|\theta)$ depends on $\mathbf{x} = (x_1, \ldots x_n)$ only through the statistic $T(\mathbf{X})$.
      \item The function $h(\mathbf{X})$ does not depend on $\theta$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 266 \end{field}
  \begin{field}
    Exponential Family
  \end{field}
  \begin{field}
    $$f(\mathbf{X}|\bf{\theta}) = h(\mathbf{x})c(\mathbf{\theta})e^{\sum _{i = 1}^n w_i(\bf(\theta))t_i(x)}$$
  \end{field}
\end{note}



%%end_tag
%%start_tag Wk6

\begin{note} \begin{field} \tiny 267 \end{field}
  \begin{field}
    Sufficiency in the exponential family
  \end{field}
  \begin{field}
    Let $X_1, \ldots , X_n$ be iid observations from a PDF or PMF, $f(x|\boldsymbol\theta)$ that belongs to an exponential family of the form

    $$ f(x|\boldsymbol\theta) = h(x)c(\boldsymbol\theta)e^{\sum _{i = 1}^k w_i(\boldsymbol\theta)t_i(x)}$$

    Where $\boldsymbol\theta = (\theta_1, \ldots, \theta_d)$, $d\leq k$. Then
    $$ T(\mathbf{X}) = \big(\sum _{j = 1}^k t_i(x_j), \cdots , \sum _{j = 1}^k t_k(x_j)\big)$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 268 \end{field}
  \begin{field}
    Minimal Sufficient Statistic
  \end{field}
  \begin{field}
    A sufficient statistic $T(\mathbf{X})$ is called a \textbf{Minimal Sufficient Statistic} if for any other sufficient statistic $T'(\mathbf{X})$, $T(\mathbf{X})$ is a function of $T'(\mathbf{X})$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 269 \end{field}
  \begin{field}
    Determining if a statistic is minimal sufficient
  \end{field}
  \begin{field}
    Let $f(x|\theta)$ be the PDF or PMF of a sample $\mathbf{X}$. Suppose there exists a function $T(x)$ such that, for every two sample points, $\mathbf{x}$ and $\mathbf{y}$, the ratio $\frac{f(\mathbf{x}|\theta)}{f(\mathbf{y}|\theta)}$ is constant as a fucntion of $\theta$ iff and only if $T(\mathbf{x}) = T(\mathbf{y})$. Then $T(\mathbf{x})$ is a minimal sufficient statistic for $\theta$.
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 270 \end{field}
  \begin{field}
    Example of finding a minimal sufficient statistic:
    Suppose that $X_1, \ldots , X_n$ are idd Bernoulli($p$). What is a minimal sufficient statistic for $p$?
  \end{field}
  \begin{field}
    \begin{align*}
      f(\mathbf{x}|p) &= \prod_{i = 1}^n p^{x_i}(1-p)^{1 - x_i}\\
      &= p^{\sum _{i = 1}^n x_i}(1-p)^{n - \sum _{i = 1}^nx_i}
    \end{align*}

    And therefore for any two sample points $\mathbf{x}$ and $\mathbf{y}$, we obtain

    \begin{align*}
      \frac{f(\mathbf{x}|p)}{f(\mathbf{y}|p)} &= \frac{p^{\sum _{i = 1}^n x_i}(1-p)^{n - \sum _{i = 1}^nx_i}}{p^{\sum _{i = 1}^n y_i}(1-p)^{n - \sum _{i = 1}^ny_i}}\\
      &= p^{\sum _{i = 1}^n x_i - \sum _{i = 1}^n y_i}(1-p)^{\sum _{i = 1}^n y_i - \sum _{i = 1}^nx_i}
    \end{align*}

    Which is constant as a function of $p$ iff $\sum _{i = 1}^n x_i = \sum _{i = 1}^n y_i$

    Hence it follows from Lehman-Sheffe that $T(\mathbf{x}) = \sum _{i = 1}^n x_i$ is minimal sufficient for $p$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 271 \end{field}
  \begin{field}
    Minimal sufficient statistic for $\mu,\sigma^2$, where the Xs are $N(\mu,\sigma^2)$
  \end{field}
  \begin{field}
    $T(\mathbf{x}) = (\bar{x},S_x^2)$ by Lehmann-Schaffe is minimal sufficient.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 272 \end{field}
  \begin{field}
    Facts about sufficiency
  \end{field}
  \begin{field}
    \begin{itemize}
      \item The entire sample $\mathbf{X}$ is always sufficeint.
      \item Any one-to-one funciton of a minimal sufficient statisitc is also a minimal sufficient statistic
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 273 \end{field}
  \begin{field}
    Ancillary Statistic
  \end{field}
  \begin{field}
    A statistic $S(\mathbf{X})$ whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic for $\theta$
  \end{field}
\end{note}



%%end_tag
% Maybe add some stuff about ancilary statistics
%%start_tag Wk7
\begin{note} \begin{field} \tiny 274 \end{field}
  \begin{field}
    Complete statistic
  \end{field}
  \begin{field}
    Let $f(t|\theta)$ be the family of pdf's or pmfs for a statistic $T= T(\mathbf{x})$.

    The family of probability distributions is called \textbf{complete} (with respect to $\theta$) if $E_\theta(g(t)) = 0$ for all $\theta$, implies $P_\theta(g(T) = 0) = 1$ for all $\theta$

    Equivalently, we say that $T = T(\mathbf{X})$ is a complete statistic.

    In short, a statistic $T = T(\mathbf{x})$ is complete, if $E_\theta(g(T)) = 0$ for all $\theta$ implies $g(t) = 0$ with probability 1
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 275 \end{field}
  \begin{field}
    (Binomial complete sufficient statistic)
  \end{field}
  \begin{field}
    Suppose the statistic $T \sim Binom(n,p)$, $0 < p < 1$, and let $g$ be a function such that $E_p(g(T)) = 0$ for all $p$.

    Then, with $r = (\frac{p}{1-p})^t$

    \begin{align*}
      0 &= E_p(g(T))\\
      &= \sum_{t=0}^n g(t) \binom{n}{t}p^t(1-p)^{n-1}\\
      &= (1-p)^n \sum_{t = 0}^n g(t)\binom{n}{t}(\frac{p}{1-p})^t\\
      &= (1-p)^n \sum_{t = 0}^n g(t)\binom{n}{t}r^t\\
      &= \neq 0 \cdot \text{This is a polynomial of degree }n \text{ in } r \text{ with coefficients } g(t)\binom{n}{t}
    \end{align*}

    For the polynomial to be $0$ for all $r$ (and consequently for all $p$) each coefficient must be zero and therefore it must be the case that $g(t) = 0$ for $t = 0, 1, 2, \cdots , n$ Since $T \sim Binom(n,p)$, we have that $T$ takes on the values $t = 0,1,2,\ldots n$ with probability $1$ and therefore, we obtain $P_p(g(T)=0)=1$. Hence T is a complete statistic.
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 276 \end{field}
  \begin{field}
    Uniform complete sufficient statistic
  \end{field}
  \begin{field}
    Suppose that $X_1, \ldots , X_n$ are iid Uniform($0,\theta$), $\theta > 0$. We know that $T(\mathbf{X}) = X_{(n)}$ (the max order statistic) is sufficient for $\theta$. Furtheremore ,
    $$ f(t|\theta) = nt^{n-1}\theta^{-n} \quad 0 < t < \theta$$
    Now suppose that $g(t)$ is a function satisfying $E_\theta(g(T)) =0, \forall \theta$
    Differentiating on both sides with respect to $\theta$,

    \begin{align*}
       0 &= \frac{d}{d\theta} E_\theta(g(t))\\
       &= \frac{d}{d\theta} \int_0^\theta g(t)nt^{n-1}\theta^{-n} dt\\
       &= \theta^{-n}\frac{d}{d\theta} \int_0^\theta g(t)nt^{n-1} dt + (\frac{d}{d\theta}\theta^{-n}) \int_0^\theta g(t)nt^{n-1}dt \\
       &= \theta^{-n}g(\theta)n\theta^{n-1} + 0
    \end{align*}

    Since $n\theta^{-1} \neq 0$, we must have that $g(\theta) =0 \quad \forall \theta> 0$. And therefore $T$ is complete.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 277 \end{field}
  \begin{field}
    Does minimal sufficent imply complete?
  \end{field}
  \begin{field}
    No

    Suppose that $X_1, \ldots X_n$ are iid $N(\theta,\theta^2)$ where $\theta \in \mathbb{R}$ is the unknown parameter of interest.

    We have

    \begin{align*}
      \frac{f(\mathbf{x}|\theta)}{f(\mathbf{y}|\theta)}
      &= \frac{(2\phi\sigma^2)^{-n/2}e^{-\frac{1}{2\sigma^2}\sum_{i =1}^n (x_i -\theta)^2}}{(2\phi\sigma^2)^{-n/2}e^{-\frac{1}{2\sigma^2}\sum_{i =1}^n (y_i -\theta)^2}}\\
      &= \frac{e^{-\frac{1}{2\sigma^2}[\sum _{i = 1}^n x_i^2 - 2\theta \sum _{i = 1}^n x_i]}}{e^{-\frac{1}{2\sigma^2}[\sum _{i = 1}^n y_i^2 - 2\theta \sum _{i = 1}^n y_i]}}
    \end{align*}

    Which is free of $\theta$ if $\sum _{i = 1}^n x_i^2 = \sum _{i = 1}^n y_i^2$ and $\sum _{i = 1}^n x_i = \sum _{i = 1}^n y_i$

    It follows that $T(\mathbf{X}) = (\sum _{i = 1}^n x_i, \sum _{i = 1}^n x_i^2)$ is minimal sufficient for $\theta$

    Now observe that $T_1(\mathbf{X}) = \sum _{i = 1}^n x_i \sim N(n\theta, n\theta^2)$ and therefore

    \begin{align*}
      E(T_1^2) &= V(T_1) + [E(T_1)]^2\\
      &= n\theta^2 + n^2\theta^2\\
      &= n\theta^2(1 + n)
    \end{align*}

    On the other hand, for $T_2 = \sum _{i = 1}^n x_i^2$,

    \begin{align*}
      E(T_2) &= n E(X_1)^2\\
      &= n[V(X_1) + [E(X_1)]^2]\\
      &= n\theta^2 + n\theta^2\\
      &= 2n\theta^2
    \end{align*}

    Then, taking $h(t_1,t_2) = 2t_1^2 - (n+1)t_2$, we have
    \begin{align*}
      E_\theta[h(T_1,T_2)] &= E_\theta[2T_1^2 - (n+1)T_2]\\
      &= 2E_\theta(T_1^2) - (n+1)E(T_2)\\
      &= 2n(n+1)\theta^2 - 2n(n+1)\theta^2\\
      &= 0 \quad \forall \theta
    \end{align*}

    But because $h(\mathbf{t}) \neq 0 \quad \forall \theta$, we have that $T(\mathbf{X})$ is not complete.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 278 \end{field}
  \begin{field}
    Complete statistics in the exponential family
  \end{field}
  \begin{field}
    Let $X_1, \ldots , X_n$ be iid observations from an exponential family. with PDF or PMF of the form
    $$ f(x|\theta) = h(x)c(\theta)e^{\sum_{j=1}^k \omega_j(\theta_j)t_j(x)}$$

    Where $\boldsymbol\theta = (\theta_1, \ldots , \theta_k)$

    Then, the statistic $T(\mathbf{X}) = (\sum_{i=1}^n t_1(x_i), \sum _{i = 1}^n t_2(x_i), \ldots , \sum _{i = 1}^n t_k(x_i))$ is complete, as long as the parameter space $\Theta$ contains an open set in $\mathbb{R}^k$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 279 \end{field}
  \begin{field}
    Suppose that a statistic $T$ is complete and let $g$ be a one-to-one function. Is the statistic $U = g(T)$ also complete?
  \end{field}
  \begin{field}
    Yes
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 280 \end{field}
  \begin{field}
    Does complete statistic imply minimial sufficient statistic?
  \end{field}
  \begin{field}
    If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 281 \end{field}
  \begin{field}
    Basu's Theorem
  \end{field}
  \begin{field}
    If $T(\mathbf{x})$ is a complete and minimal sufficient statistic, then $T (\mathbf{x})$ is an independent of every ancillary statistic.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 282 \end{field}
  \begin{field}
    Likelihood function
  \end{field}
  \begin{field}
    Let $f(\mathbf{x}|\theta)$ denote the joint pdf or pmf of the sample $\mathbf{X} = (X_1, \ldots , X_n)$, then given that $\mathbf{X} = \mathbf{x}$ is observed, the function of $\theta$ defined as $$L(\theta|\mathbf{x}) = f(\mathbf{x}|\theta)$$
    is called the Likelihood Function
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 283 \end{field}
  \begin{field}
    Idea of likelihood function
  \end{field}
  \begin{field}
    Suppose that $\mathbf{X}$ is a discrete random vector (so we can interpret probabilities easier)

    Then $L(\theta|\mathbf{x}) = P_\theta(\mathbf{X} = \mathbf{x})$. Now if we compare the likelihood function at two parameter values $\theta_1, \theta_2$ and we observe that $$P_{\theta_1}(\mathbf{X} = \mathbf{x}) = L(\theta_1|\mathbf{x}) > L(\theta_2|\mathbf{x}) = P_{\theta_2}(\mathbf{X} = \mathbf{x}$$
    Then, the sample point $\mathbf{x}$ that we actually observed is more likely to have occured if $\theta = \theta_1$, than if $\theta= \theta_2$, which can be interpreted as that $\theta_1$, is a more plausible value for the true value of $\theta$ than $\theta_2$ is.
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 284 \end{field}
  \begin{field}
    Fisher information - one parameter case
  \end{field}
  \begin{field}
    Let $X$ be a random variable with pdf or pmf $f(x |\theta)$ where $\theta \in \Theta \subseteq \mathbb{R}$

    (Fisher ) information about $\theta$ contained in $X$ is

    $$ I_{X}(\theta) = E_\theta[ \big(\frac{\partial}{\partial \theta} \log f(x|\theta)\big)^2]$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 285 \end{field}
  \begin{field}
    Example of one parameter case Fisher information
    Suppose that $X \sim Bern(p)$ What is the information that $X$ contains about the parameter $p$?
  \end{field}
  \begin{field}
    We have that $f(x|p) = p^x(1-p)^{1-x}$. Then $$ \log f(x|p) = x \log p + (1-x)\log(1-p)$$
    $$ \frac{\partial}{\partial p} \log f(x|p) = \frac{x}{p} - \frac{1-x}{1-p}$$

    We obtain


    \begin{align*}
      \big(\frac{\partial}{\partial p} \log f(x|p) \big)^2 &= \big(\frac{x}{p} - \frac{1-x}{1-p}\big)^2\\
      &= \frac{x^2}{p^2} - \frac{2x(1-x)}{p(1-p)} + \frac{(1-x)^2}{(1-p)^2}\\
      &= \frac{x^2}{p^2} - \frac{2(x-x^2)}{p(1-p)} + \frac{(1-2x+x^2)}{(1-p)^2}
    \end{align*}

    Therefore,

    \begin{align*}
      I_x(p) &= E_p[(\frac{\partial}{\partial p} \log f(x|p))^2]\\
      &= \frac{p}{p^2} - \frac{2(p-p)}{p(1-p)} + \frac{1 - 2p + p}{(1-p)^2}\\
      &= \frac{1}{p} + \frac{1}{1-p}\\
      &= \frac{1}{p(1-p)}
    \end{align*}
  \end{field}
\end{note}




%%end_tag
%%start_tag Wk8

\begin{note} \begin{field} \tiny 286 \end{field}
  \begin{field}
    $$ I_x(\theta) = E_\theta\big[(\frac{\partial}{\partial \theta} \log f(x|\theta))^2\big] = $$

  \end{field}
  \begin{field}
    If $f(x|\theta)$ satisfies

    $$ \frac{\partial}{\partial \theta} E_\theta \big( \frac{\partial}{\partial \theta} \log f(x|\theta)\big) = \int \frac{\partial}{\partial \theta}\big[\frac{\partial}{\partial \theta}\log f(x|\theta)\big]f(x|\theta)dx $$

    $$ I_x(\theta) = E_\theta\big[(\frac{\partial}{\partial \theta} \log f(x|\theta))^2\big] = - E_\theta (\frac{\partial^2}{\partial \theta^2} \log f(x|\theta))$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 287 \end{field}
  \begin{field}
    Suppose that $X_1, \ldots , X_n$ are iid observations with common pdf or pmf $f(x|\theta)$. Then, the information about $\theta$ contained in the sample $\mathbf{X} = (X_1, \ldots , X_n)$ is
  \end{field}
  \begin{field}
    $$I_{\mathbf{X}}(\theta) = n I_{X_1}(\theta)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 288 \end{field}
  \begin{field}
    Fisher Information - multiparameter case
  \end{field}
  \begin{field}
    Let $X$ be a random variable with pdf or pmf $f(x|\boldsymbol\theta)$, where $\boldsymbol\theta = (\theta_1,\theta_2) \in \Theta \subseteq \mathbb{R}^2$. Denote by $$I_{ij}(\boldsymbol\theta) = E_{\boldsymbol\theta}\big[(\frac{\partial}{\partial \theta_i}\log f(x|\boldsymbol\theta))(\frac{\partial}{\partial \theta_j} \log f(x|\boldsymbol\theta))\big] = -E_{\boldsymbol\theta}[\frac{\partial}{\partial \theta_i\theta_j}\log f(x|\boldsymbol\theta)]$$

    For $i,j = 1,2$.
    Then the (fisher) information matrix about $\boldsymbol\theta$ is

    $$ I_x(\boldsymbol\theta) = \begin{pmatrix}
      I_{11}(\boldsymbol\theta) & I_{12}(\boldsymbol\theta)\\
      I_{21}(\boldsymbol\theta) & I_{12}(\boldsymbol\theta)
    \end{pmatrix}$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 289 \end{field}
  \begin{field}
    Find Fisher information for Normal RVs
  \end{field}
  \begin{field}
    We have that $\boldsymbol\theta = (\mu,\sigma^2)$ and $f(x|\boldsymbol\theta) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x - \mu)^2}$

    Then,
    $$ \frac{\partial}{\partial \mu} \log f(x|\boldsymbol\theta) = \frac{\partial}{\partial}[-\frac{1}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}(x - \mu)^2] = \frac{(x-\mu)}{\sigma^2}$$

    $$ \frac{\partial}{\partial\sigma^2} = \frac{1}{2\sigma^2}[ \frac{(x-\mu)^2}{\sigma^2} - 1]$$

    Therefore $I_{11} = E_\theta[(\frac{\partial}{\partial \mu}\log f(x|\boldsymbol\theta))^2] = E_\theta[\frac{(x-\mu)^2}{\sigma^4}] = \frac{1}{\sigma^4}\sigma^2 = \frac{1}{\sigma^2}$

    \begin{align*}
      I_{22}(\boldsymbol\theta) &= E_{\theta}[\frac{\partial}{\partial\sigma^2}\log f(x|\boldsymbol\theta)^2]\\
      &= E_\theta \big\{ [ \frac{1}{2\sigma^2} ( \frac{(x-\mu)^2}{\sigma^2} - 1)]^2\big\}\\
      &= \frac{1}{4\sigma^4}E_\theta[ (\frac{(x-\mu)^2}{\sigma^2} - 1)^2]\\
      &= \frac{1}{4\sigma^4 \cdot 2} \\
      &= \frac{1}{2\sigma^4} \quad \text{Since } = V(\chi^2_1)
    \end{align*}

    Now for the off diagonal elements,

    \begin{align*}
      I_{12}(\boldsymbol\theta) = I_{22}(\boldsymbol\theta) &= E_\theta \big[ ( \frac{\partial{}{}}{\partial{\mu}{}}\log f(x|\theta)(\frac{\partial{}{}}{\partial{\sigma^2}{    }} \log f(x|\theta)))\big]\\
      &=E_\theta \big[ \frac{(x-\mu)}{\sigma^2
      } \frac{1}{2\sigma^2} [\frac{x-\mu}{\sigma^2}\cdot 1]\big]\\
      &= \frac{1}{2\sigma^4} E_\theta[ \frac{(x-\mu)^3}{\sigma^3} - (x-\mu)]
    \end{align*}

    But $E_\theta[(x-\mu)^3] = E_\theta[(x - \mu)] = 0$, because $X$ is symmetric around $\mu$, and we obtain $I_{12}(\boldsymbol\theta) = I_{21}(\boldsymbol\theta) = 0$

    We obtain that

    \begin{align*}
      I_{x_1}(\boldsymbol\theta) &= \begin{pmatrix}
        I_{11}(\boldsymbol\theta) & I_{12}(\boldsymbol\theta)\\
        I_{21}(\boldsymbol\theta) & I_{22}(\boldsymbol\theta)
    \end{pmatrix}\\
    &= \begin{pmatrix}
      \frac{1}{\sigma^2} & 0 \\ 0 & \frac{1}{2\sigma^4}
    \end{pmatrix}
    \end{align*}


    And hence $$ I_{\mathbf{x}}(\boldsymbol\theta) = n I_{X_1}(\boldsymbol\theta) = \begin{pmatrix}
      \frac{n}{\sigma^2} & 0 \\ 0 & \frac{n}{2\sigma^4}
    \end{pmatrix} $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 290 \end{field}
  \begin{field}
    $I_T(\theta) \leq $
  \end{field}
  \begin{field}
    $I_T(\theta) \leq I_{\mathbf{X}}(\theta)$
    (The information of the statistic is less than or equal to the information of the sample)
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 291 \end{field}
  \begin{field}
    Let $\mathbf{X} = X_1, \ldots , X_n$ denote the entire data, and let $T = T(\mathbf{X})$ be some statistic. Then, for all $\theta \in \Theta \subseteq \mathbb{R}$, $I_{\mathbf{X}} (\theta) \geq I_t(\theta)$
    Where the equality is attained...
  \end{field}
  \begin{field}
    Let $\mathbf{X} = X_1, \ldots , X_n$ denote the entire data, and let $T = T(\mathbf{X})$ be some statistic. Then, for all $\theta \in \Theta \subseteq \mathbb{R}$, $I_{\mathbf{X}} (\theta) \geq I_t(\theta)$
    Where the equality is attained if and only iff $T(\mathbf{X})$ is sufficient for $\theta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 292 \end{field}
  \begin{field}
    Let $\mathbf{X} = (X_1, \ldots , X_n)$, denote a sample of iid observations and suppose the statistic $T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))$ is such that $T_1$ and $T_2$ are independent. Then $$ I_{T}(\boldsymbol\theta) = $$
  \end{field}
  \begin{field}
    Let $\mathbf{X} = (X_1, \ldots , X_n)$, denote a sample of iid observations and suppose the statistic $T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))$ is such that $T_1$ and $T_2$ are independent. Then $$ I_{T}(\boldsymbol\theta) = I_{T_1}(\boldsymbol\theta) + I_{T_2}(\boldsymbol\theta)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 293 \end{field}
  \begin{field}
    Point estimator
  \end{field}
  \begin{field}
    Any statistic $T(\mathbf{X})$ that is used to estimate the value of a parameter is called a point estimator of $\theta$. We write $\hat{\theta} = T(\mathbf{X})$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 294 \end{field}
  \begin{field}
    Method of moments
  \end{field}
  \begin{field}
    \begin{align*}
      m_1 &= \frac{1}{n}\sum _{i = 1}^n X_i^1, \quad \mu_1 = E(X^1)\\
      m_2 &= \frac{1}{n}\sum _{i = 1}^n X_i^2, \quad \mu_2 = E(X^2)\\
      \vdots & \\
      m_k &= \frac{1}{n} \sum _{i = 1}^n X_i^k \quad \mu_k = E(X^k)
    \end{align*}
    Equating and solving for $\theta$ gives the MoM estimators
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 295 \end{field}
  \begin{field}
    Example Method of Moments
    Suppose that $X_1, \ldots , X_n$ are iid Binomial($k,p$), where both $k$ and $p$ are unknown.
  \end{field}
  \begin{field}
    We have that
    $$ P(X_i = x|k,p) = \binom{k}{x}p^x(1-p)^{k-x}, x = 0,1,\ldots,k$$
     and we obtain $E(X_1) = kp$, $E(X_1^2) = kp(1-p) + k^2p^2$

    Solving the sytem of equations we obtain

    \begin{align*}
      m_1 &= \frac{1}{n} \sum _{i = 1}^n X_i = kp\\
      m_2 &= \frac{1}{n} \sum _{i = 1}^n X_i^2 = kp(1-p) + k^2p^2
    \end{align*}

    Sovling the system of equations:

    \begin{align*}
      \tilde{p} &= \frac{\bar{x}}{\tilde{k}}\\
      \tilde{k} &= \frac{\bar{x}^2}{\bar{x} - \frac{1}{n} \sum _{i = 1}^n (x_i - \bar{x})^2}
    \end{align*}

    Possible problems: $k$ has to be an integer, and not negative. (Estimates of parameters that are outside of the parameter space. )
  \end{field}
  \begin{field}

  \end{field}
\end{note}


%%end_tag
%%start_tag Wk9

\begin{note} \begin{field} \tiny 296 \end{field}
  \begin{field}
    Maximum Likelihood Estimator
  \end{field}
  \begin{field}
    In this context, we define the \textbf{Maximum Likelihood Estimator (MLE)} of $\theta$ as the parameter value $\hat{\theta}_{ML} = \hat{\theta}(\mathbf{x})$ that satisfies
    $$L(\hat{\theta}_{ML}|\mathbf{x}) = \text{sup}_{\theta \in \Theta} L(\theta|\mathbf{x})$$

    Note this often proceedes as taking the derivative of the log likelihood function and setting to zero to solve for parameters - not always
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 297 \end{field}
  \begin{field}
    Example of MLE
    Suppose that $X_1, \ldots , X_n$ are iid Exponential$(\lambda)$. Find the MLE $\hat{\lambda}_{ML}$ of $\lambda$
  \end{field}
  \begin{field}
    Suppose that $X_1, \ldots , X_n$ are iid Exponential$(\lambda)$. Find the MLE $\hat{\lambda}_{ML}$ of $\lambda$

    We have that $f(x|\lambda) = \frac{1}{\lambda}e^{x/\lambda}$, $x>0$, and therefore

    $$L(\lambda|x) = \prod _{i = 1}^n \frac{1}{\lambda}e^{x_i/\lambda} = \lambda^{-n}e^{-\frac{1}{\lambda}\sum _{i = 1}^n x_i}$$


    Since $\log(\cdot)$ is a strictly monotone (one-to-one) and increasing, we consider instead the maximization of the log-likelihood

    $$l(\lambda|\mathbf{x}) = \log L(\lambda|\mathbf{x}) = -n\log \lambda - \frac{1}{\lambda} \sum _{i = 1}^n x_i$$

    $$\frac{\partial  }{\partial \lambda } l(\lambda | \mathbf{x}) = \frac{-n}{\lambda} + \frac{1}{\lambda^2}\sum _{i = 1}^n x_i$$

    Solving $\frac{\partial  }{\partial \lambda } l(\lambda|\mathbf{x})= 0$, we obtain

    $$ \frac{-n}{\lambda} + \frac{1}{\lambda^2}\sum _{i = 1}^n x_i = 0$$

    $$ -n\lambda + n \bar{x} = 0$$
    $$\lambda = \bar{x}$$

  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 298 \end{field}
  \begin{field}
    Example of MLE when can't differemtiate

    Suppose that $X_1, \ldots , X_n$ are iid Uniform($0,\theta$), $\theta > 0$. Find the MLE of $\theta$
  \end{field}
  \begin{field}
    We have that $f(x|\theta) = \frac{1}{\theta}I(0<x<\theta)$

    And therefore
    \begin{align*}
      L(\theta|\mathbf{x}) &= \prod _{i = 1}^n \frac{1}{\theta} I(0 < x_i < \theta)\\
      &= \frac{1}{\theta^n} I(X_{(1)} > 0)I(X_{(n)}< \theta)
    \end{align*}

    In this case, the support of $X$ depends on $\theta$ and the maximization problem only makes sense whenever $L(\theta|\mathbf{x}) > 0$. We cannot simply approach the problem by taking partial derivatives, but assuming the likelihood is positive, we notice that $L(\theta|\mathbf{x})$ is decerasing as a function of $\theta$, for $\theta > X_{(n)}$

    Picture with $L(\theta)$ as zero untill $X_{(n)}$ on x axis, goes up to $1/X_{(n)}$ there and decreases with $\frac{1}{\theta^n}$

    It follows the MLE of $\theta$ is $\hat{\theta}_{ML} = X_{(n)}$

  \end{field}
\end{note}





%%end_tag


\begin{note} \begin{field} \tiny 299 \end{field}
  \begin{field}
    If $\hat{\theta}_{ML}$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\eta =\tau(\theta)$ is $\hat{\eta}_{ML} =$
  \end{field}
  \begin{field}
    If $\hat{\theta}_{ML}$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\eta =\tau(\theta)$ is $\hat{\eta}_{ML} = \tau(\hat{\theta}_{ML})$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 300 \end{field}
  \begin{field}
    Bias
  \end{field}
  \begin{field}
    Let $\hat{\theta} = T(\mathbf{X})$ be an estimatro of $\theta$. Then the Bias of $\hat{\theta}$ as an estimator of $\theta$ is defined as
    $$B_{\theta}(\hat{\theta}) = E_\theta(\hat{\theta} - \theta) = E_\theta(\hat{\theta}) - \theta$$

    That is the difference between the expected value of $\hat{\theta}$ and $\theta$.

    An estimator $\hat{\theta}$ of $\theta$ is said to be unbiased if $B_\theta(\hat{\theta}) = 0 \quad \forall \theta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 301 \end{field}
  \begin{field}
    Mean Squared Error
  \end{field}
  \begin{field}
    Let $\hat{\theta} = T(\mathbf{X})$ be an estimate of $\theta$. Then, the \textbf{Mean Squared Error} (MSE) of $\hat{\theta}$ as an estimator of $\theta$ is defined as:

    $$ MSE (\hat{\theta}) = E_{\theta}[(\hat{\theta} - \theta)^2] = V_\theta(\hat{\theta}) + [B_\theta(\hat{\theta})]^2$$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 302 \end{field}
  \begin{field}
    Do unbiased estimators always exist?
  \end{field}
  \begin{field}
    No, Suppose that $X \sim $ Binomial($n,p$) and let $\theta = 1/p$ be the parameter of interest. Can we find an unbiased estimator for $\theta$?- No
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 303 \end{field}
  \begin{field}
    UMVUE
  \end{field}
  \begin{field}
    An estimator $W^*$ is called a best unbiased estimator of $\tau(\theta)$ if it satisfies $E_\theta(W^*) = \tau(\theta)$, for all $\theta$, and for any other estimator $W$ with $E_\theta(W) = \tau(\theta)$, we have $V_\theta(W^*)\leq V_\theta(W), \forall \theta$. Equivalently $W^*$ is also called a \textbf{Uniform Minimal Variance Unbiased Estimator} (UMVUE) of $\tau(\theta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 304 \end{field}
  \begin{field}
    Finding a UMVUE
  \end{field}
  \begin{field}
    Start with a complete statistic, (find min suff statistic, prove completeness), Find bias (ie $E(T(\mathbf{X}))$). Then adjust $T(\mathbf{X})$ to be unbiased. (ie center or scale )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 305 \end{field}
  \begin{field}
    Cramer-Rao Inequality
  \end{field}
  \begin{field}
    Let $X_1, \ldots , X_n$ be a sample with joint pdf or pmf $f(\mathbf{x}|\theta)$ and let $W(\mathbf{X}) = W(X_1, \ldots , X_n)$ be any estimator satisfying
    $$\frac{d}{d\theta} E_\theta(W(X)) = \int \frac{d}{d\theta} [W(\mathbf{X})f(\mathbf{x}|\theta)] d \mathbf{x}$$

    and $V_\theta(W(\mathbf{X}))< \infty$

    Then,
    $$V_\theta(W(\mathbf{X})) \geq \frac{(\frac{d}{d\theta}E_\theta(W(\mathbf{X})))^2}{E_\theta[( \frac{\partial}{\partial \theta}\log f(\mathbf{x}|\theta))^2]}$$

    Observe that if the sample $X_1, \ldots , X_n$ is iid with common pdf or pmf $f(x|\theta)$, we obtain

    $$ V_\theta (W(\mathbf{X})) \geq \frac{[\frac{d}{d\theta}E_\theta(W(\mathbf{X}))]^2}{nE_\theta[(\log f(\mathbf{x}|\theta))^2]}$$

    The denominator is the information in the sample about $\theta$

    We have that as the information number gets bigger we have a smaller bound for the variance. of the best unbiased estimator and therefore more information is available.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 306 \end{field}
  \begin{field}
    Cramer-Rao and UMVUE example
    UMVUE of $\lambda$ for Poisson
  \end{field}
  \begin{field}
    Poisson example, we have $\tau(\lambda) = \lambda$, so $\frac{d}{d\lambda}\tau(\lambda) = 1$

    On the other hand,

    \begin{align*}
      nE_\lambda[(\frac{d}{d\lambda}\log f(x|\lambda))^2] &= -n E_\lambda(\frac{\partial ^2 }{\partial \lambda ^2}) \log f(x|\lambda))\\
      &= -n E_\lambda(\frac{\partial ^2 }{\partial \lambda ^2} \log (\frac{e^{-\lambda}\lambda^x}{x!}))\\
      &= -n E_\lambda[\frac{\partial ^2 }{\partial \lambda ^2}(-\lambda + x \log \lambda - \log(x!))]\\
      &= -n E_\lambda(\frac{-x}{\lambda^2})\\
      &= \frac{n}{\lambda}
    \end{align*}

    Therefore, for any unbiased estimator $W$ of $\lambda$, we must have $V_\lambda(W) \geq \lambda /n$. Since $V_\lambda(\bar{X}) = \frac{\lambda}{n}$, we have that $\bar{X}$ is an UMVUE of $\lambda$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 307 \end{field}
  \begin{field}
    Does $S^t$ for Normal attain cramer rao?
  \end{field}
  \begin{field}
    No -
    Suppose that $X_1, \ldots , X_n$ are iid N($\mu,\sigma^2$)
    and consider the estimation of $\sigma^2$ when $\mu$ is unknown.

    We have that

    $$ \frac{\partial ^2 }{\partial (\sigma^2) ^2} \log [ \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}] = \frac{1}{2\sigma^4} - \frac{(x-\mu)^2}{\sigma^6}$$

    and
    \begin{align*}
      -E[\frac{\partial ^2 }{\partial (\sigma^2)^2 } \log f(x|\mu,\sigma^2)] &= -E(\frac{1}{2\sigma^4} - \frac{(x-\mu)^2}{\sigma^6})\\
      &= -\frac{1}{2\sigma^4} + \frac{\sigma^2}{\sigma^6}\\
      &= \frac{1}{2\sigma^4}
    \end{align*}

    and therefore, any unbiased estimator $W$ of $\sigma^2$ must satisfy $V(W) \geq \frac{2\sigma^4}{n}$. Recall that for $S^2$ we have $$V(S^2) = \frac{2\sigma^4}{n-1} > \frac{2\sigma^4}{n}$$

    and therefore $S^2$ does not attain the cramer-rao lower bound.

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 308 \end{field}
  \begin{field}
    Rao-Blackwell
  \end{field}
  \begin{field}
    Let $W$ be any unbiased estimator $\tau(\theta)$ and let $T$ be a sufficient statistic for $\theta$. Define $\phi(T) = E(W|T)$. Then $E_\theta(\phi(T)) = \tau(\theta)$ and $V_\theta(\phi(T))\leq V_\theta(W)$, for all $\theta$
    That is, $\phi(T)$ is a uniformly better unbiased estimator of $\tau(\theta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 309 \end{field}
  \begin{field}
    Use of Rao-Blackwell
  \end{field}
  \begin{field}
    Estimators can be improved (their MSE) using sufficiency (already sufficient statistics, or functions of sufficient statistics cannot be improved)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 310 \end{field}
  \begin{field}
    Are unbiased estimators based on complete sufficient statistics unique.
  \end{field}
  \begin{field}
    Unbiased estimators based on complete sufficient statistics are unique.
  \end{field}
\end{note}


%%end_tag
% Todo
% Exponential families
% Approximate (limit ) relationships from wiki page
\end{document}
