\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,mathrsfs}
\newenvironment{note}{\paragraph{NOTE:}}{}
\newenvironment{field}{\paragraph{field:}}{}
%\newenvironment{tags}{\paragraph{tags:}}{}
\newcommand*{\tags}[1]{\paragraph{tags: }#1}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%start_tag Distribution Relationships
\tags{distributionrelationships dist}

\begin{note}
  \begin{field}
    \tiny 54
  \end{field}
  \begin{field}
    $X \sim Gamma(a,b)$
    $P(X \leq x) = P(Y \geq a)$ Where $Y \sim $
  \end{field}
  \begin{field}
    $X \sim Gamma(a,b)$
    $P(X \leq x) = P(Y \geq a)$
    Where $Y \sim Pois (x/b)$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 55
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(0,1)$$
    $$ \sum X_i \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(0,1)$$
    $$ \sum X_i \sim N(0,n)$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 56
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(\mu_i,\sigma_i^2)$$
    $$ \sum X_i \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$X_1, \ldots, X_n \sim iid N(\mu_i,\sigma_i^2)$$
    $$ \sum X_i \sim N(\sum \mu_i,\sum \sigma^2_i)$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 57
  \end{field}
  \begin{field}
    $$X  \sim N(\mu,\sigma^2)$$
    $$ aX + b \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ aX + Y \sim N(a\mu + b, a^2\sigma^2)$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 58
  \end{field}
  \begin{field}
    $X \sim Binom(1,p) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Bern(p)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 59
  \end{field}
  \begin{field}
    $X \sim NegBinom(1,p) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Geom(p)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 60
  \end{field}
  \begin{field}
    $X \sim Gamma(1,\theta) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Exp(\theta)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 61
  \end{field}
  \begin{field}
    $X \sim Exp(\theta) \overset{?}{\sim}$
  \end{field}
  \begin{field}
    $X \sim Gamma(1,\theta)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 62
  \end{field}
  \begin{field}
    $X \sim Gamma(v/2,1/2) \overset{?}{\sim} $
  \end{field}
  \begin{field}
    $X \sim \chi^2(v)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 63
  \end{field}
  \begin{field}
    $$X \sim \chi^2(v) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$X \sim Gamma(v/2,2)$$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 64 \end{field}
  \begin{field}
    $$ X \sim \chi^2(2) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim exp(2)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 65 \end{field}
  \begin{field}
    $$ X \sim Weibull(1,\beta) \overset{?}{\sim} $$
  \end{field}
  \begin{field}
    $$X \sim Exp(\beta)$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 66 \end{field}
  \begin{field}
    $X_1, X_2 \sim \chi^2(v_i)$ independent
    $ \frac{X_1/v_1}{X_2/v_2}$
  \end{field}
  \begin{field}
    $$ \frac{(X_1/v_1)}{(X_2/v_2)} \sim F(v_1,v_2)$$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 67 \end{field}
  \begin{field}
    $$ X \sim beta(1,1) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim Unif(0,1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 68 \end{field}
  \begin{field}
    $$ X \sim Unif(0,1) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim beta(1,1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 69 \end{field}
  \begin{field}
    Special case of t
    $$ X \sim t_1 \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ X \sim Caucy(0,1)$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 70 \end{field}
  \begin{field}
    Scaled Gamma
    $$X \sim Gamma(\alpha,\beta), Y = aX \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ Y \sim Gamma(\alpha,a\beta)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 71 \end{field}
  \begin{field}
    Scaled Exponential
    $$ X \sim Exp(\lambda), Y = aX \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$ Y \sim Exp(a\lambda)$$
  \end{field}
\end{note}

%check
\begin{note} \begin{field} \tiny 72 \end{field}
  \begin{field}
    Sum of Exponential, equal rate
    $X_i \sim Exp(\lambda), Y = \sum X_i$
  \end{field}
  \begin{field}
    $$Y \sim Gamma(n,\lambda)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 73 \end{field}
  \begin{field}
    $$ X \sim Exp(\lambda), Y  = e^{-x}$$
  \end{field}
  \begin{field}
    $$Y \sim Beta(\lambda,1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 74 \end{field}
  \begin{field}
    Min of Exponential (using $\lambda$ parametrization)
    $$X_1, \ldots , X_n \sim  Exp(\lambda_i), Y = \min(X_i) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $Y \sim exp(\sum \lambda_i)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 75 \end{field}
  \begin{field}
    Min of Uniform
    $$ X_i \sim Unif(0,1), Y = \lim n \min(X_i) \overset{?}{\sim}$$
  \end{field}
  \begin{field}
    $$Y \sim Exp(1)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 76 \end{field}
  \begin{field}
    $$ X \sim Beta(\alpha,\beta), Y = (1-X)$$
  \end{field}
  \begin{field}
    $$ Y \sim Beta(\beta,\alpha)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 77 \end{field}
  \begin{field}
    $X \sim F_X(X), Y = F^{-1}_X(X)$
  \end{field}
  \begin{field}
    $Y \sim Unif(0,1)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 78 \end{field}
  \begin{field}
    $X \sim N(\mu,\sigma^2), Y = e^X$
  \end{field}
  \begin{field}
    $Y \sim lognormal(\mu,\sigma^2)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 79 \end{field}
  \begin{field}
    $X \sim exp(\beta), Y = X^{1/z}$
  \end{field}
  \begin{field}
    $Y \sim Weibull(z,\beta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 80 \end{field}
  \begin{field}
    Square of Normal
    $X \sim N(0,1), Y = X^2$
  \end{field}
  \begin{field}
    $Y \sim \chi^2(1)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 81 \end{field}
  \begin{field}
    Square of t
    $X \sim t(v), Y = X^2$
  \end{field}
  \begin{field}
    $Y \sim F(1,v)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 82 \end{field}
  \begin{field}
    Sum of Poisson
    $X_i \sim Poisson(\mu_i) Y = \sum X_i$
  \end{field}
  \begin{field}
    $Y \sim Poisson(\sum \mu_i)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 83 \end{field}
  \begin{field}
    Sum of Gamma
    $X_i \sim Gamma(\alpha_i, \beta), Y = \sum X_i$
  \end{field}
  \begin{field}
    $Y \sim Gamma(\sum \alpha_i, \beta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 84 \end{field}
  \begin{field}
    Sum of independent Chi-squared
    $ X_i \sim \chi^2(v_i) Y = \sum X_i$
  \end{field}
  \begin{field}
    $Y \sim \chi^2(\sum v_i)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 85 \end{field}
  \begin{field}
    $X,Y$ independent
    $X,Y \sim N(0,1), X/Y$
  \end{field}
  \begin{field}
    $X/Y \sim Cauchy(0,1)$
  \end{field}
\end{note}





\begin{note} \begin{field} \tiny 86 \end{field}
  \begin{field}
    $X_1,X_2 \sim gamma(\alpha_i,1)$ independent, $\frac{X_1}{X_1 + X_2} $
  \end{field}
  \begin{field}
    $$ \frac{X_1}{X_1 + X_2} \sim beta(\alpha_1,\alpha_2)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 87 \end{field}
  \begin{field}
    $X_1,X_2 \sim gamma(\alpha_i,\beta_i)$ independent, $\frac{\beta_2X_1}{\beta_2X_1 + \beta_1X_2} $
  \end{field}
  \begin{field}
    $$ \frac{\beta_2X_1}{\beta_2X_1 + \beta_1X_2}\sim beta(\alpha_1,\alpha_2)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 88 \end{field}
  \begin{field}
    $X,Y $ independent $exp(\mu)$ $X-Y$
  \end{field}
  \begin{field}
    $X-Y \sim $ double exponential$(0,\mu)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 89 \end{field}
  \begin{field}

    $X \sim Gamma(\alpha,\beta)$ $Y = 1/X$
  \end{field}
  \begin{field}
      Inverted Gamma
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 90 \end{field}
  \begin{field}
    Bernoulii$(p), E(X) = , V(X) = $
  \end{field}
  \begin{field}
     Bernoulii$(p), E(X) = p, V(X) = p(1-p)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 91 \end{field}
  \begin{field}
    Discrete Uniform $N, E(X) = , V(X) = $
  \end{field}
  \begin{field}
    Discrete Uniform $N, E(X) = \frac{N+1}{2}, V(X) = \frac{(N+1)(N-1)}{12}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 92 \end{field}
  \begin{field}
    Cauchy($\theta,\sigma), E(X) = ,V(X)=$
  \end{field}
  \begin{field}
    Cauchy($\theta,\sigma), E(X) = na,V(X)=na$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 93 \end{field}
  \begin{field}
    Double Exponential$(\mu,\sigma), E(X) = , V(X) = $
  \end{field}
  \begin{field}
    Double Exponential$(\mu,\sigma), E(X) = \mu, V(X) = 2\sigma^2$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 94 \end{field}
  \begin{field}
    F($v_1,v_2), E(X) = , V(X) = $
  \end{field}
  \begin{field}
F($v_1,v_2), E(X) = \frac{v_1}{v_2-2}, V(X) = 2\big(\frac{v_2}{v_2-2}\big)^2\frac{(v_1+v_2-2)}{v_1(v)2-4}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 95 \end{field}
  \begin{field}
    Mean and Variance for Distributions not on bible (but in CB)
      \begin{itemize}

        \item Double Exponential$(\mu,\sigma), E(X) = , V(X) = $
        \item F($v_1,v_2), E(X) = , V(X) = $
        \item Logistic$(\mu,\beta), E(X) = , V(X) = $
        \item Lognormal$(\mu,\sigma^2), E(X)=, V(X) = $
        \item Pareto$(\alpha,\beta), E(X) =, V(X) =$
        \item t$(v), E(X) = , V(X) = $
        \item Weibull$(\gamma,\beta), E(X) =, V(X) = $
      \end{itemize}
  \end{field}
  \begin{field}
    Mean and Variance. for Distributions not on bible (but in CB)
      \begin{itemize}

        \item Logistic$(\mu,\beta), E(X) = \mu, V(X) = \frac{\phi^2\beta^2}{3}$
        \item Lognormal$(\mu,\sigma^2), E(X)=e^{\mu + (\sigma^2/2)}) V(X) = e^{2(\mu+\sigma^2)} - e^{2\mu + \sigma^2}$
        \item Pareto$(\alpha,\beta), E(X) = \frac{\beta\alpha}{\beta-1}, V(X) = \frac{\beta\alpha^2}{(\beta-1)^2(\beta-2)}$
        \item t$(v), E(X) = 0, V(X) = \frac{v}{v-2}$
        \item Weibull$(\gamma,\beta), E(X) = \beta^{1/\gamma}\Gamma(1 + 1/\gamma), V(X) = \beta^{2/\gamma}(\Gamma(1 + 2/\gamma) - \Gamma^2(1 + 1/\gamma))$
      \end{itemize}
  \end{field}
\end{note}

%%end_tag
%%start_tag Calculus

\tags{Calculus calc}

\begin{note} \begin{field} \tiny 96 \end{field}
  \begin{field}
    $\int_0^\infty e^{-x^2/2} = $
  \end{field}
  \begin{field}
    $\int_0^\infty e^{-x^2/2} = \sqrt{\pi/2}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 97 \end{field}
  \begin{field}
    $\int_0^\infty x^{a-1}e^{-x/b} = $
  \end{field}
  \begin{field}
    $\int_0^\infty x^{a-1}e^{-x/b} = \Gamma(a)b^a$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 98 \end{field}
  \begin{field}
    $\int_0^1 x^{a - 1}(1 - x)^{b-1} = $
  \end{field}
  \begin{field}
    $\int_0^1 x^{a - 1}(1 - x)^{b-1} = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 99 \end{field}
  \begin{field}
    $\log(x) = y , x = $
  \end{field}
  \begin{field}
    $\log(x) = y , x = e^y$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100 \end{field}
  \begin{field}
    $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = $
  \end{field}
    \begin{field}
      $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = e^a$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 101 \end{field}
  \begin{field}
    $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = $
  \end{field}
  \begin{field}
    $\lim_{x \to \infty}( 1 + \frac{a}{x})^x = e^a$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 102 \end{field}
  \begin{field}
    $\frac{d}{dx}f(g(x)) = $
  \end{field}
  \begin{field}
    $\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)$ (Chain rule)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 103 \end{field}
  \begin{field}
    $\frac{d}{dx} \int_{a}^x f(t)dt = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} \int_{a}^x f(t)dt = f(x)$ (fundamental theorem of calculus )
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 104 \end{field}
  \begin{field}
    $\int_a^b u dv = $

    ex: $\int x e^{-x}$
  \end{field}
  \begin{field}
    $\int_a^b u dv = uv|_a^b - \int_a^b v du $

    ex: $u = x, dv = e^{-x}, du = dx, v = -e^{-x}$
    \begin{align*}
      \int x e^{-x} &= -x e^{-x} + \int e^{-x} \\
      &= -x e^{-x}  -e^{-x} + c
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 105 \end{field}
  \begin{field}
    $\sum_{k=0}^\infty \frac{x^k}{k!} = $
  \end{field}
  \begin{field}
    $\sum_{k=0}^\infty \frac{x^k}{k!} = e^x$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 106 \end{field}
  \begin{field}
    $e^x = $
  \end{field}
  \begin{field}
    $e^x = \sum_{k=0}^\infty \frac{x^k}{k!}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 107 \end{field}
  \begin{field}
    $\sum_{k=0}^\infty x^k = $
  \end{field}
  \begin{field}
    $\sum_{k=0}^\infty x^k = \frac{1}{1-x}$ for $|x| < 1$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 108 \end{field}
  \begin{field}
    $\sum_{k=0}^n x^k =$
  \end{field}
  \begin{field}
    $\sum_{k=0}^n x^k = \frac{1 - x^{n+1}}{1-x}$ for $x \neq 1$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 109 \end{field}
  \begin{field}
    $\lim_{x \to -\infty} e^{-x} = $
  \end{field}
  \begin{field}
    $\lim_{x \to -\infty} e^{-x} = \infty$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 110 \end{field}
  \begin{field}
    $\lim_{x \to \infty} e^{-x} = $
  \end{field}
  \begin{field}
    $\lim_{x \to -\infty} e^{-x} = 0$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 111 \end{field}
  \begin{field}
    $$ (fg)' = $$
  \end{field}
  \begin{field}
    $$ (fg)' = f'g + g'f$$ (product rule )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 112 \end{field}
  \begin{field}
    $\frac{d}{dx} x^n = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} x^n = nx^{n-1}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 113 \end{field}
  \begin{field}
    $\frac{d}{dx} a^x = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} a^x = a^x ln(a)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 114 \end{field}
  \begin{field}
    $\frac{d}{dx} ln(x) = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} ln(x) = \frac{1}{x}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 115 \end{field}
  \begin{field}
    $\frac{d}{dx} (f(x))^n = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} (f(x))^n = n(f(x))^{n-1}f'(x)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 116 \end{field}
  \begin{field}
    $ \frac{d}{dx} ln(f(x)) = $
  \end{field}
  \begin{field}
    $ \frac{d}{dx} ln(f(x)) = \frac{f'(x)}{f(x)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 117 \end{field}
  \begin{field}
    $\frac{d}{dx} e^{f(x)} = $
  \end{field}
  \begin{field}
    $\frac{d}{dx} e^{f(x)} = f'(x)e^{f(x)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 118 \end{field}
  \begin{field}
    $\int x^n = $
  \end{field}
  \begin{field}
    $\int x^n = \frac{1}{n+1}x^{n+1}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 119 \end{field}
  \begin{field}
    $\int \frac{1}{x} = $
  \end{field}
  \begin{field}
    $\int \frac{1}{x} = ln(|x|)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 120 \end{field}
  \begin{field}
    $\int \frac{1}{ax + b} = $
  \end{field}
  \begin{field}
    $\int \frac{1}{ax + b} = \frac{1}{a}ln(|ax + b|)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 121 \end{field}
  \begin{field}
    $\int e^{cx} = $
  \end{field}
  \begin{field}
    $\int e^{cx} = \frac{1}{c}e^{cx}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 122 \end{field}
  \begin{field}
    $\int x e^{-cx^2} = $
  \end{field}
  \begin{field}
    $\int x e^{-cx^2} = -\frac{1}{2c}e^{-cx^2}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 123 \end{field}
  \begin{field}
    U substitution:

    example; $\int_1^2 5x^2\cos(x^3)$
  \end{field}
  \begin{field}
    $\int_a^bf(g(x))g'(x) = \int_{g(a)}^g(b) f(u)du$

    Where $u = g(x), du = g' dx$

    Ex: $u = x^3, du = 3x^2, x^2 du = 1/3 du$
    $\int_1^2 5x^2\cos(x^3) = \int_{1}^8 5/3 \cos(u)du$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 124 \end{field}
  \begin{field}
    $\Gamma(a) = $
  \end{field}
  \begin{field}
    $\int_0^\infty t^{a-1}e^{-t}dt $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 125 \end{field}
  \begin{field}
    $\int_0^\infty t^{a-1}e^{-t}dt $
  \end{field}
  \begin{field}
    $= \Gamma(a) $
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 126 \end{field}
  \begin{field}
    $\Gamma(a + 1) = $
  \end{field}
  \begin{field}
    $\Gamma(a + 1) = a\Gamma(a)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 127 \end{field}
  \begin{field}
    $\Gamma(n) = $
  \end{field}
  \begin{field}
    $\Gamma(n) = (n-1)!$ (for $n$ an integer)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 128 \end{field}
  \begin{field}
    $\Gamma(1/2) = $
  \end{field}
  \begin{field}
    $\Gamma(1/2) = \sqrt{\pi}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 129 \end{field}
  \begin{field}
    $\Gamma(1) = $
  \end{field}
  \begin{field}
    $\Gamma(1) = 1$
  \end{field}
\end{note}


%%end_tag
%%start_tag Theory1
\tags{Theory1}
%%start_tag Casella Ch1
\begin{note} \begin{field} \tiny 130 \end{field}
  \begin{field}
    \begin{tabular}{|c| c c |}
    \hline \\
    & replace & no replacement \\
    number of trials &  &  \\
    Draw till nth success & & \\
    \hline
    \end{tabular}
  \end{field}
  \begin{field}
    \begin{tabular}{|c| c c |}
    \hline \\
    & replace & no replacement \\
    number of trials & Binom & Hypergeometric \\
    Draw till nth success & Nbinom & Negative hypergeometric \\
    \hline
    \end{tabular}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 131 \end{field}
  \begin{field}
    Plug uniform into inverse CDF
  \end{field}
  \begin{field}
    Get cdf
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 132 \end{field}
  \begin{field}
    Sample Space
  \end{field}
  \begin{field}
    The set, $S$, of all possible outcomes of a particular experiment is called the \textit{sample space} for the experiment.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 133 \end{field}
  \begin{field}
    Event
  \end{field}
  \begin{field}
    An \textit{event} is any collection of possible outcomes of an experiment, that is, any subset of $S$ (including $S$ itself).
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 134 \end{field}
  \begin{field}
    Union
  \end{field}
  \begin{field}
    $ A \cup B = \{x:x \in A \text{ or } x \in B\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 135 \end{field}
  \begin{field}
    Intersection
  \end{field}
  \begin{field}
    $A \cap B = \{x:x \in A \text{ and } x \in B\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 136 \end{field}
  \begin{field}
    Complementation
  \end{field}
  \begin{field}
    $A^c = \{x:x\notin A\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 137 \end{field}
  \begin{field}
    Commutativity
    $$ A \cup B = $$
    $$ A \cap B = $$
  \end{field}
  \begin{field}
    Commutativity
    $$ A \cup B = B \cup A$$
    $$ A \cap B = B \cap A$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 138 \end{field}
  \begin{field}
    Associativity
    $$ A \cup (B \cup C) = $$
    $$ A \cap (B \cap C) = $$
  \end{field}
    \begin{field}
      Associativity
        $$ A \cup (B \cup C) = (A \cup B) \cup C$$
        $$ A \cap (B \cap C) = (A \cap B) \cap C$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 139 \end{field}
  \begin{field}
    Distributive Laws
    $$A \cap ( B \cup C) = $$
    $$ A \cup ( B \cap C) = $$
  \end{field}
    \begin{field}
      Distributive Laws
        $$A \cap ( B \cup C) = (A \cap B) \cup (A \cap C)$$
        $$ A \cup ( B \cap C) = (A \cup B) \cap ( A \cup C)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 140 \end{field}
  \begin{field}
    DeMorgan's Laws
    $$(A \cup B)^c = $$
    $$(A \cap B)^c = $$
  \end{field}
    \begin{field}
      DeMorgan's Laws
        $$(A \cup B)^c = A^c \cap B^c$$
        $$(A \cap B)^c = A^c \cup B^c$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 141 \end{field}
  \begin{field}
    Disjoint
  \end{field}
    \begin{field}
        Disjoint: Two events $A$ and $B$ are disjoint ( or mutually exclusive) if $ A \cap B = \emptyset $
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 142 \end{field}
  \begin{field}
    $$P(A_1 \cap A_2 \cap \cdots \cap A_n)  = $$
  \end{field}
  \begin{field}
    $$P(A_1)P(A_2|A_1)P(A_3|A_1A_2) \ldots P(A_n|A_1\cdots A_{n-1}) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 143 \end{field}
  \begin{field}
    $$ P(A,B,C) = $$
  \end{field}
  \begin{field}
    $$ P(A,B,C) = P(A)P(B|A)P(C|A,B)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 144 \end{field}
  \begin{field}
    $$P(A \cup B \cup C) =  $$
  \end{field}
  \begin{field}
    $$P(A \cup B \cup C) =  P(A) + P(B) + P(C) - P(A \cap B) - P(B\cap C) - P(A \cap C) + P(A \cap B \cap C)$$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 145 \end{field}
  \begin{field}
    Pairwise disjoint
  \end{field}
    \begin{field}
        Two Events $A_1, A_2$ are pairwise disjoint ( or mutually exclusive) if $A_i \cap A_j = \emptyset $ for all $i \neq j$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 146 \end{field}
    \begin{field}
      Partition
    \end{field}
    \begin{field}
        If $A_1, A_2, \ldots$ are pairwise disjoint and $\cup_{i=1}^\infty A_i = S$, then the collection $A_1, A_2, \ldots$ forms a partition of $S$.
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 147 \end{field}
    \begin{field}
        \text{Sigma Algebra}
    \end{field}
    \begin{field}
        \text{A} collection of subsets of $S$ is called a sigma algebra (or Borel field), denoted by $\mathcal{B}$, if it satisfies the following three properties:
            \begin{enumerate}
              \item $\emptyset \in \mathcal{B}$ (the empty set is an element of $\mathcal{B}$)
              \item If $A \in \mathcal{B}$, then $A^c \in \mathcal{B}$ ($\mathcal{B}$ is closed under complementation)
              \item If $A_1, A_2, \ldots \in \mathcal{B}$, then $\cup_{i-1}^\infty A_i \in \mathcal{B} \mathcal{B}$ is closed under countable unions)
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 148 \end{field}
    \begin{field}
        \text{Probability} Function / Kolmogorov Axioms
    \end{field}
    \begin{field}
        \text{Given} a sample space $S$ and an associated sigma algebra $\mathcal{B}$, a probability function is a function $P$ with domain $\mathcal{B}$ that satisfies:
            \begin{enumerate}
              \item $P(A) \geq 0$ for all $A \in \mathcal{B}$
              \item $P(S) = 1$
              \item If $A_1, A_2, \ldots \mathcal{B}$ are pairwise disjoint, then $P(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)$ (Axiom of Countable Additivity)
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 149 \end{field}
    \begin{field}
      If $A\in \mathcal{B}$ and $B \in \mathcal{B}$ are disjoint, then
          $$P(A\cup B) = P(A) + P(B) $$
        Axiom of Finite Additivity
    \end{field}
    \begin{field}
        \text{If} $A\in \mathcal{B}$ and $B \in \mathcal{B}$ are disjoint, then
            $$P(A\cup B) = P(A) + P(B) $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 150 \end{field}
    \begin{field}
        Properties of probability functions
        \begin{enumerate}
              \item $P(\emptyset) = $
              \item $P(A) $
              \item $P(A^c) = $
            \end{enumerate}
    \end{field}
    \begin{field}
      Properties of probability functions
        \begin{enumerate}
              \item $P(\emptyset) = 0$
              \item $P(A) \leq 1$
              \item $P(A^c) = 1 - P(A)$
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 151 \end{field}
    \begin{field}
        \text{If} $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(B\cap A^c) = $$
    \end{field}
    \begin{field}
        \text{If} $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(B\cap A^c) = P(B) - P(A \cap B)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 152 \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(A\cup B) = $$
    \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(A\cup B) = P(A) + P(B) - P(A \cap B)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 153 \end{field}
    \begin{field}
        \text{If} $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then if $A \subset B$ then
    \end{field}
    \begin{field}
        \text{If} $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then if $A \subset B$ then $P(A) \leq P(B)$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 154 \end{field}
    \begin{field}
        Bonferroni's Inequality
        $$P(A\cap B)  $$
    \end{field}
    \begin{field}
      Bonferroni's Inequality:
        $$P(A\cap B) \geq P(A) + P(B) -1 $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 155 \end{field}
    \begin{field}If $P$ is a probability function, then for any partition $C_1, C_2, \ldots P(A) = $
    \end{field}
    \begin{field}If $P$ is a probability function, then for any partition $C_1, C_2, \ldots P(A) = \sum_{i=1}^\infty P(A \cap C_i)$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 156 \end{field}
    \begin{field}
        Boole's Inequality
        $$P(\cup_{i=1}^\infty A_i)$$
    \end{field}
    \begin{field}
        If $P$ is a probability function,
            $$P(\cup_{i=1}^\infty A_i) \leq \sum_{i=1}^\infty P(A_i) \text{ for any sets } A_1, A_2, \ldots $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 157 \end{field}
    \begin{field}
        \text{Fundamental} Theorem of Counting
    \end{field}
    \begin{field}
        If a job consists of $k$ separate tasks, the $i$th of which can be done in $n_i$ ways, $i = 1, \ldots, k$, then the entire job can be done in $n_1 \times n_2 \times \cdots, \times n_k$ ways.
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 158 \end{field}
    \begin{field}
        Ordered without replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
        $$\frac{n!}{(n-r)!}$$

        eg lottery with $n=44$ choices for $r=6$ values, cant use same number twice, order matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 159 \end{field}
    \begin{field}
        Unordered without replacement: number of arrangements of size $r$ from $n$ objects
      \end{field}
    \begin{field}
        $$\binom{n}{r} = \frac{n!}{r!(n-r!)}$$

        eg lottery with $n=44$ choices for $r=6$ values, cant use same number twice, order does not matter (Use ordered without replacement and divide by redundant orderings )
    \end{field}
\end{note}


\begin{note} \begin{field} \tiny 160 \end{field}
    \begin{field}
        Ordered with replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
      Ordered with replacement: number of arrangements of size $r$ from $n$ objects
        $$n^r$$
        eg lottery with $n=44$ choices for $r=6$ values, can use same number twice, order matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 161 \end{field}
    \begin{field}
        Unordered with replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
      Unordered with replacement: number of arrangements of size $r$ from $n$ objects
        $$\binom{n+r-1}{r} = \frac{(n+r-1)!}{r!(n-1)!}$$

        eg lottery with $n=44$ choices for $r=6$ values, can use same number twice, order does not matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 162 \end{field}
  \begin{field}
    Number of arrangements of size $r$ from $n$ objects

    \begin{tabular}{|c| c c|}
      \hline \\
       & Without Replacement & With replacement\\
       \hline \\
       Ordered & & \\
       Unordered & & \\
       \hline
    \end{tabular}
  \end{field}
  \begin{field}
    Number of arrangements of size $r$ from $n$ objects

    \begin{tabular}{|c| c c|}
      \hline \\
       & Without Replacement & With replacement\\
       \hline \\
       Ordered & $\frac{n!}{(n-r)!}$ & $n^r$ \\
       Unordered & $\binom{n}{r}$& $\binom{n+r-1}{r}$ \\
       \hline
    \end{tabular}
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 163 \end{field}
    \begin{field}
        \text{Binomial} Coefficient $\binom{n}{r}$
    \end{field}
    \begin{field}
      Binomial Coefficient
        $$\binom{n}{r} = \frac{n!}{r!(n-r)!}$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 164 \end{field}
  \begin{field}
    $$P(A|B)=$$
  \end{field}
  \begin{field}
    $$ P(A|B) = \frac{P(A \cap B)}{P(B)} $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 165 \end{field}
  \begin{field}
    Statistically independent
    $P(A \cap B) = $
  \end{field}
  \begin{field}
    Statistically independent
    $P(A \cap B) = P(A)P(B)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 166 \end{field}
  \begin{field}
    If $A$ and $B$ are independent events, what else is independent?
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $A $ and $B^c$
      \item $A^c$ and $B$
      \item $A^c$ and $B^c$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 167 \end{field}
  \begin{field}
    Mutually independent
  \end{field}
  \begin{field}
    A collection of events $A_1, \ldots , A_n$ are mutually independent for any subcollection $A_{i1}, \ldots , A_{ik}$, we have
    $$ P((\cap_{j=1}^k A_{ij})) = \prod_{j=1}^k P(A_{ij}) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 168 \end{field}
  \begin{field}
    Random variable
  \end{field}
  \begin{field}
    A random variable is a function from a sample space $S$ into the real numbers
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 169 \end{field}
  \begin{field}
    Definition of a pdf
  \end{field}
  \begin{field}
    A function $f_X(x)$ is a pdf (or pmf) of a random variable $X$ if and only if
    \begin{enumerate}
      \item $f_x(x) \geq 0$ for all $x$
      \item $\sum_x f_x(x) = 1$ or $\int_{-\infty}^\infty f_x(x) dx = 1$
    \end{enumerate}
  \end{field}
\end{note}


%%end_tag
%%start_tag Casella Ch2
\begin{note} \begin{field} \tiny 170 \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{enumerate}
      \item If $g$ is an increasing function on $X$, $F_Y(y) = $ for $y \in Y$
      \item If $g$ is a decreasing function on $X$ and $X$ is a continuous random variable, $F_y(y) = $ for $y \in Y$
    \end{enumerate}
  \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{enumerate}
      \item If $g$ is an increasing function on $X$, $F_Y(y) = F_X(g^{-1}(y))$ for $y \in Y$
      \item If $g$ is a decreasing function on $X$ and $X$ is a continuous random variable, $F_y(y) = 1 - F_X(g^{-1}(y))$ for $y \in Y$
    \end{enumerate}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 171 \end{field}
  \begin{field}
    Method of pdf + conditions
  \end{field}
  \begin{field}
    Conditions:
    \begin{enumerate}
      \item $g$ is a monotone function
      \item $f_X(x)$ is continuous on $X$
      \item $g^{-1}(y)$ has a continuous derivative
    \end{enumerate}
    Let $X$ have pdf $f_X(x)$ and let $Y = g(Y)$

    $$f_Y(y) = f_X(g^{-1}(y))\bigg|\frac{d}{dy}g^{-1}(y)\bigg|$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 172 \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{itemize}
      \item If $g$ is an increasing function, $F_Y(y) = $
      \item If $g$ is a decreasing function, and $X$ is a continuous random variable, $F_Y(y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{itemize}
      \item If $g$ is an increasing function, $F_Y(y) = F_x(g^{-1}(y))$
      \item If $g$ is a decreasing function, and $X$ is a continuous random variable, $F_Y(y) = 1 - F_X(g^{-1}(y))$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 173 \end{field}
  \begin{field}
    eg: $X \sim Unif(0,1)$, $Y = -log(X)$
    $F_Y(y) = $
  \end{field}
  \begin{field}
    $F_Y(y) = 1 - F_x(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 173x  \end{field}
  \begin{field}
    $X$ is a continuous random variable. For $y > 0$, $Y = X^2$
    $F_Y(y) = $
  \end{field}
  \begin{field}
    \begin{align*}
      F_Y(y) &= P(Y \leq y)\\
      &= P(X^2 \leq y)\\
      &= P(- \sqrt{y} \leq X \leq \sqrt{y})\\
      &= P(X \leq \sqrt{y}) - P(X \leq -\sqrt{y})\\
      &= F_X(\sqrt{y}) - F_X( - \sqrt{y})
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 174 \end{field}
  \begin{field}
    Pdf of $F_X(g(X))$, where $Y = g(X)$
  \end{field}
  \begin{field}
    Chain rule:
    $f_Y(y) = g'(y)f(g(y))$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 175 \end{field}
  \begin{field}
    Method of pdf if $g$ is not monotone all entire domain
  \end{field}
  \begin{field}
    $f_Y = \sum f_x(g_i^{-1}(y))|\frac{d}{dy}g_i^{-1}(y)|$ $y \in Y$, 0 otherwise

    eg: $Y = X^2$,
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 176 \end{field}
  \begin{field}
    $P(Y  \leq y)$ when $Y = F_X(x)$
  \end{field}
  \begin{field}
    \begin{align*}
      P(Y \leq y) &= P(X \leq F_x^{-1}(y))\\
      &= F_X(F_X^{-1}(y))\\
      &= y
    \end{align*}
    $Y$ is uniformly distributed
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 177 \end{field}
  \begin{field}
    $M_x(t) = $ (discrete )
  \end{field}
  \begin{field}
    $M_x(t) = E(e^{tX}) = \sum_x e^{tX}P(X)$ (discrete )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 178 \end{field}
  \begin{field}
    $M_x(t) = $ (continuous)
  \end{field}
  \begin{field}
    $M_x(t) = E(e^{tX}) = \int_{-\infty}^\infty e^{tX}f_x(x)dx$ (continuous)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 179 \end{field}
  \begin{field}
    $E(X^n) = $
  \end{field}
  \begin{field}
    $E(X^n) = M_x^n(0) = \frac{d^n}{dt^n}M_x(t)|_{t=0}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 180 \end{field}
  \begin{field}
    $M(aX + b)(t) = $
  \end{field}
  \begin{field}
    $M(aX + b)(t) = e^{bt}M_x(at)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 181 \end{field}
  \begin{field}
    If $E(X^n)$ exists then...
  \end{field}
  \begin{field}
    If $E(X^n)$ exists then $E(X^m)$ exists for $m \leq n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 182 \end{field}
  \begin{field}
    If $X_i$ are independent and $Y = a_1X_1 + \cdots + a_nX_n + b$, then $M_Y(t) = $
  \end{field}
  \begin{field}
    If $X_i$ are independent and $Y = a_1X_1 + \cdots + a_nX_n$, then $M_Y(t) = e^{bt}\prod_{i=1}^n M_{X_i}(a_i t) $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 183 \end{field}
  \begin{field}
    Example of using MGF for finding expected value:
    MGF gamma: $(\frac{1}{1 - \beta t})^\alpha$: $E(X) = $
  \end{field}
  \begin{field}
    $E(X) = \frac{\alpha\beta}{(1 - \beta t)^{\alpha+1}}|_{t=0} = \alpha \beta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 184 \end{field}
  \begin{field}
    Using MGF to relate distributions: MGF exp = $( 1 - \beta t)^{-1}$
  \end{field}
  \begin{field}
    $Y = \sum X_i$ is gamma as MGF gamma is $(1 - \beta t)^{-\alpha}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 185 \end{field}
  \begin{field}
    First step in transforming a RV
  \end{field}
  \begin{field}
    Determine support
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 186 \end{field}
  \begin{field}
    $nth$ Moment of X
  \end{field}
  \begin{field}
    $E(X^n)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 187 \end{field}
  \begin{field}
    $n$th central moment of $X$
  \end{field}
  \begin{field}
    $E[(X - \mu)^n]$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 188 \end{field}
  \begin{field}
    $(a + b)^n = $
  \end{field}
  \begin{field}
    $(a + b)^n = \sum_{x = 0}^n \binom{n}{x}a^xb^{n-x}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 189 \end{field}
  \begin{field}
    $\sum_{x = 0}^n \binom{n}{x}a^xb^{n-x} = $
  \end{field}
  \begin{field}
    $(a + b)^n$
  \end{field}
\end{note}

%%end_tag
%%start_tag Casella Ch3
\begin{note} \begin{field} \tiny 190 \end{field}
  \begin{field}
    $N$ balls $r$ red $N - r$ green. Select $n$ balls. Probability that $y$ are red?
  \end{field}
  \begin{field}
    Hypergeometric distribution($N,r,n$)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 191 \end{field}
  \begin{field}
    Hypergeometric distribution description $(N,r,n)$
  \end{field}
  \begin{field}
    $N$ is total balls, $r$ is number red balls, $n$ is number balls selected.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 192 \end{field}
  \begin{field}
    Negative binomial description
  \end{field}
  \begin{field}
    Number of Bernoulli trials required to get a fixed number of successes. $r$ being the $r$th success
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 193 \end{field}
  \begin{field}
    Geometric description
  \end{field}
  \begin{field}
    Modeling waiting time. $X$ is the trial at which the first success occurs.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 194 \end{field}
  \begin{field}
    Location-scale family for $f(x)$
  \end{field}
  \begin{field}
    $ \frac{1}{\sigma} f(\frac{(x - \mu)}{\sigma})$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 195 \end{field}
  \begin{field}
    Given $X$ give the mean and variance for the location-scale random $Y= 1/\sigma f((y - \mu)/\sigma)$ variable
  \end{field}
  \begin{field}
    $E(Y) = \sigma E(X) + \mu$, $V(Y) = \sigma^2 V(X)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 196 \end{field}
  \begin{field}
    $X \sim Pois(\lambda)$ $P(X = x+1) = $
  \end{field}
  \begin{field}
    $X \sim Pois(\lambda)$ $P(X = x+1) = \frac{\lambda}{x + 1}P(X = x)$
  \end{field}
\end{note}

%%end_tag
%%start_tag Casella Ch4

\begin{note} \begin{field} \tiny 197 \end{field}
  \begin{field}
    $f(y|x) = $ (definition of conditional)
  \end{field}
  \begin{field}
    $f(y|x) = \frac{f(x,y)}{f_x(x)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 198 \end{field}
  \begin{field}
    $E(g(Y)|x) = $
  \end{field}
  \begin{field}
    $E(g(Y)|x) = \int_{-\infty}^\infty g(y)f(y|x) dy $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 199 \end{field}
  \begin{field}
    Example of calculating conditional pdfs
$f(x,y) = e^{-y}, 0 < x < y < \infty$. $f(y|x) = $

  \end{field}
  \begin{field}
    \begin{align*}
      f_x(x) &= \int_{-\infty}^\infty f(x,y) dy \\
      &= e^{-x}\\
      \\
      f(y|x) &= \frac{f(x,y)}{f_x(x)}\\
      &= \frac{e^{-y}}{e^{-x}} \text{ if } y > x\\
      &= \frac{0}{e^{-x}} \text{ if } y \leq x
    \end{align*}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 200 \end{field}
  \begin{field}
    Let ($X,Y$) be given as $f(x,y)$. Then $X$ and $Y$ are independent if
  \end{field}
  \begin{field}
    Let ($X,Y$) be given as $f(x,y)$. Then $X$ and $Y$ are independent if there exist functions $g(x), h(y)$ such that $f(x,y) = g(x)h(y)$ (factorization - don't need to compute marginals )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 201 \end{field}
  \begin{field}
    Let $X,Y$ be independent. Then $E(g(X)h(Y)) = $
  \end{field}
  \begin{field}
    $E(g(X)h(Y)) = (E(g(X)))(E(h(Y)))$

    example: $E(X^2Y) = E(X^2)E(Y)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 202 \end{field}
  \begin{field}
    $X,Y$ independent\\ $Z = X + Y$  \\ $M_Z(t) = $
  \end{field}
  \begin{field}
    $M_Z(t) = M_X(t)M_Y(t)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 203 \end{field}
  \begin{field}
    Method of pdf bivariate
  \end{field}
  \begin{field}
    $f_{u,v}(u,v) = f_{x,y}(h_1(u,v),h_2(u,v))|J|$\\
    Where $|J| = \begin{vmatrix}
      \frac{\partial  x}{\partial  u} & \frac{\partial  x }{\partial v }\\ \frac{\partial  y }{\partial u } & \frac{\partial  y }{\partial v }
    \end{vmatrix} = \frac{\partial  x }{\partial u } \frac{\partial  y}{\partial v } - \frac{\partial  y }{\partial u } \frac{\partial  x }{\partial v }$

    and $u = g_1(x,y), v = g_2(x,y)$ and $x = h_1(x,y), y = h_2(x,y)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 204 \end{field}
  \begin{field}
    $X,Y$ independent, $g(X)$ a function only of $X$ and $h(Y)$ a function only of $Y$. Then
  \end{field}
  \begin{field}
    $g(X)$ and $g(Y)$ are independent.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 205 \end{field}
  \begin{field}
    Correlation
  \end{field}
  \begin{field}
    $\rho_{XY} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 206 \end{field}
  \begin{field}
    $m$ independent trials, each trial resulting in one of $n$ outcomes, with probabilities $p_1, \ldots, p_n$. $X_i$ is the count of the number of times the $i$th outcome occured in the $m$ trials.
  \end{field}
  \begin{field}
    Multinomial distribution
    $f(x_1, \ldots, x_n) = \frac{m!}{x_1! \cdots x_n!}p_1^{x_i} \cdots p_n^{x_n}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 207 \end{field}
  \begin{field}
    $|E(XY)| $
  \end{field}
  \begin{field}
    $|E(XY)| \leq E(|XY|) \leq (E(|X|^2))^{1/2}(E(|Y|^2))^{1/2}$ (Cauchy-Schwartz)
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 208 \end{field}
  \begin{field}
    $E(g(X)) $ where $g$ is a convex function
  \end{field}
  \begin{field}
    $E(g(X)) \geq g(E(X))$ where $g$ is a convex function (Jensen's inequlity)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 209 \end{field}
  \begin{field}
    Ranking of types of means
  \end{field}
  \begin{field}
    $\mu_{\text{harmonic}} \leq \mu_{\text{geometric}} \leq \mu_{\text{arithmetic}}$ By Jensens inequality (using logs )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 210 \end{field}
  \begin{field}
    Linear transformations of multivariate normal $X \sim N(\vec{\mu},\Sigma) $\\
    $A \vec{X} + \vec{b}$
  \end{field}
  \begin{field}
    $A \vec{X} + \vec{b} \sim N(A \vec{\mu} + \vec{v}, A \Sigma A^t)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 211 \end{field}
  \begin{field}
    $X \sim N(\vec{\mu},\Sigma) $\\

    $\vec{X}_a | \vec{X}_b \sim $
  \end{field}
  \begin{field}
    $\vec{X}_a | \vec{X}_b \sim N\big(\vec{\mu_a} + \Sigma_{ab} \Sigma^{-1}_{bb}(\vec{x}_b - \vec{\mu}_b), \Sigma_{ba} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}$\\

ex: $(X_1,X_2,X_3), \vec{\mu} = (1,2,3)^t, \Sigma = \begin{pmatrix}
  3 & 1 & 0 \\ 1 & 3 & 1 \\ 0 & 1 & 3
\end{pmatrix}$
$X1,X_3 | X_2 = 1$\\
$a = \{1,3\}, b = \{2\}$\\
$\mu_a = (1,3)^t, \mu_b = 1$\\
$\Sigma_{aa} = \begin{pmatrix}
  3 & 0 \\ 0 & 3
\end{pmatrix}, \Sigma_{ab} = (1,1)^t$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 212 \end{field}
  \begin{field}
    $(X,Y)$ multinomial \\
    $aX + bY \sim $
  \end{field}
  \begin{field}
    $aX + bY \sim N(a \mu_x + b\mu_y, a^2 \sigma_x^2 + b^2 \sigma_y^2 + 2 ab \rho \sigma_x \sigma_y)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 213 \end{field}
  \begin{field}
    $(X,Y)$ multinomial \\
    $Y|X\sim $
  \end{field}
  \begin{field}
    $Y|X\sim  N(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x - \mu_x), \sigma_Y^2(1 - \rho^2))$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 214 \end{field}
  \begin{field}
    CDF for Max order statistic
  \end{field}
  \begin{field}
    $(F(x))^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 215 \end{field}
  \begin{field}
    PDF for Max order statistic
  \end{field}
  \begin{field}
    $nf(x)(F(x))^{n-1}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 216 \end{field}
  \begin{field}
    CDF for Min order statistic
  \end{field}
  \begin{field}
    $1 - (1 - F(x))^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 217 \end{field}
  \begin{field}
    PDF for Min order statistic
  \end{field}
  \begin{field}
    $n(1 - F(x))^{n-1}f(x)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 218 \end{field}
  \begin{field}
    CDF for $k$th order statistic
  \end{field}
  \begin{field}
    $F_{(k)}(x) = \sum_{j = k}^n \binom{n}{j}(F(x))^j(1 -F(x))^{n-j}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 219 \end{field}
  \begin{field}
    PDF for $k$th order statistic
  \end{field}
  \begin{field}
    $f_{(k)}(x) = k \binom{n}{k}f(x)F(x)^{k-1}(1 - F(x))^{n-k}$
  \end{field}
\end{note}

%%end_tag
%%start_tag From Stat Cheatsheet

\begin{note}
  \begin{field}
    \tiny 1
  \end{field}
  \begin{field}
    CDF of Geometric ($p$)
  \end{field}
  \begin{field}
    $1 - (1-p)^x$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 2
  \end{field}
  \begin{field}
    CDF of Exponential($\beta$)
  \end{field}
  \begin{field}
    $1 - e^{-\frac{x}{\beta}}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 3
  \end{field}
  \begin{field}
    \begin{itemize}
  \item $B = \Omega \cap B = (A \cup A^c) \cap B
    = $
  \item $P(A^c) = $
  \item $P(B) = $
  \item $P(\Omega) =  \qquad P(\varnothing) = $
  \item $\left(\bigcup_n A_n\right) =
    \quad
    \left(\bigcap_n A_n\right) =
    \qquad$
    \textsc{DeMorgan}
\end{itemize}
  \end{field}
  \begin{field}
    \begin{itemize}
  \item $B = \Omega \cap B = (A \cup A^c) \cap B
    = (A \cap B) \cup (A^c \cap B)$
  \item $P(A^c) = 1 - P(A)$
  \item $P(B) = P(A \cap B) + P(A^c \cap B)$
  \item $P(\Omega) = 1 \qquad P(\varnothing) = 0$
  \item $\left(\bigcup_n A_n\right) = \bigcap_n A_n
    \quad
    \left(\bigcap_n A_n\right) = \bigcup_n A_n
    \qquad$
    \textsc{DeMorgan}
\end{itemize}
  \end{field}
\end{note}




\begin{note}
  \begin{field}
    \tiny 4
  \end{field}
  \begin{field}
    Probability Set intersection
    \begin{itemize}
      \item $P(\bigcup_n A_n) = $
      \item $P(A \cup B) =
        \implies $
      \item $P(A \cup B)
        = $
      \item $P(A \cap B^c) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Probability Set intersection
    \begin{itemize}
      \item $P(\bigcup_n A_n)
        = 1 - P(\bigcap_n A_n)$
      \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)\\[1ex]
        \implies P(A \cup B) \leq P(A) + P(B)$
      \item $P(A \cup B)
        = P(A \cap B^c) + P(A^c \cap B) + P(A \cap B)$
      \item $P(A \cap B^c) = P(A) - P(A \cap B)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 5
  \end{field}
  \begin{field}
    $P(A \cap B) =  \text{ when }A \text{ and } B \text{independent}$
  \end{field}
  \begin{field}
    $P(A \cap B) = P(A)P(B) \text{ when }A \text{ and } B \text{independent}$
  \end{field}
  \end{note}

\begin{note}
  \begin{field}
    \tiny 6
  \end{field}
  \begin{field}
    $$P(A|B) = $$
  \end{field}
  \begin{field}
    $$P(A|B) = \frac{P(A\cap B)}{P(B)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 7
  \end{field}
  \begin{field}
    Law of total probability
  \end{field}
  \begin{field}
    Law of total probability
    $$P(B) = \sum_{i=1}^n P(B|A_i)P(A_i) \quad \Omega = \cap_{i=1}^n A_i$$

    $$P(B) = P(A \cap B) + P(A^c \cap B) $$
   \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 8
  \end{field}
  \begin{field}
    Bayes Theorem $P(A_i|B) =$
  \end{field}
  \begin{field}
    Bayes Theorem
    $$P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^n P(B|A_j)P(A_j)} \quad \Omega = \cup_{i=1}^n A_i$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 9
  \end{field}
  \begin{field}
    CDF Laws
  \end{field}
  \begin{field}
    CDF Laws
    \begin{enumerate}
      \item Nondecreasing: $x_1 < x_2 \implies F(x_1) \leq F(x_2)$
      \item Limits: $\lim_{x \to -\infty}=0$ and $\lim_{x\to \infty} = 1$
      \item Right-Continuous $\lim_{y \to x^+}F(y) = F(x)$
    \end{enumerate}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 10
  \end{field}
  \begin{field}
    $$f_{y|x}(y|x) = $$
  \end{field}
  \begin{field}
    $$f_{y|x}(y|x) = \frac{f(x,y)}{f_x(x)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 11
  \end{field}
  \begin{field}
    $X,Y \text{independent}$
    \begin{itemize}
      \item $P(X \leq x, Y \leq y) = $
      \item $f_{x,y}(x,y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    $X,Y \text{independent}$
    \begin{itemize}
      \item $P(X \leq x, Y \leq y) = P(X \leq x)P(Y \leq y)$
      \item $f_{x,y}(x,y) = f_x(x)f_y(y)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 12
  \end{field}
  \begin{field}
     Transformations $Z = \phi(X)$

    \begin{itemize}
      \item Discrete: $f_Z(z) = $
      \item Continuous: $F_Z(z)=$
      \item Cont, $\phi$ strictly monotone:
      $f_z(z)$
    \end{itemize}
  \end{field}
  \begin{field}
    Transformations $Z = \phi(X)$
  \begin{itemize}
    \item Discrete: $$f_Z(z) = P(\phi(X) = z) = P(X \in \phi^{-1}(z)) = \sum_{x \in \phi^{-1}(z)}f_x(x) $$
    \item Continuous (Method of CDF): $$F_Z(z)= P(\phi(X)\leq z) = \int_{x:\phi(x)\leq z} f(x) dx$$
    \item Cont, $\phi$ strictly monotone: (Method of PDF)
    $f_z(z) = f_x(\phi^{-1}(z))|\frac{d}{dz} \phi^{-1}(z)|$
  \end{itemize}
\end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 13
  \end{field}
  \begin{field}
     Rule of the Lazy Statistician: $ E[g(x)] = $
  \end{field}
  \begin{field}
    Rule of the Lazy Statistician:  $E[g(x)] = \int g(x)f_x(x)dx$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 14
  \end{field}
  \begin{field}
    Expectation rules
    \begin{itemize}
      \item $E(c) = $
      \item $E(cX) = $
      \item $E(X + Y) = $
      \item $E(\phi(X)) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Expectation rules
    \begin{itemize}
      \item $E(c) = c$
      \item $E(cX) = cE(X)$
      \item $E(X + Y) = E(X) + E(Y)$
      \item $E(\phi(X)) \neq \phi(E(X))$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 15
  \end{field}
  \begin{field}
    Conditional expectation
      \begin{itemize}
        \item $E(Y|X = x) = $
        \item $E(X) = $
        \item $E(Y+Z|X) = $
        \item $E(Y|X) = c \implies $
      \end{itemize}
  \end{field}
  \begin{field}
    Conditional expectation
      \begin{itemize}
        \item $E(Y|X = x) = \int y f(y|x) dy$
        \item $E(X) = E(E(X|Y))$
        \item $E(Y+Z|X) = E(Y|X) + E(Z|X)$
        \item $E(Y|X) = c \implies Cov(X,Y) = 0$
      \end{itemize}

  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 16
  \end{field}
  \begin{field}
    \text{Variance}
    \begin{itemize}
      \item $V(X) = \sigma_x^2 = $
      \item $V(X+Y) =$
      \item $V\bigg[\sum_{i=1}^n X_i\bigg] = $
    \end{itemize}
  \end{field}
  \begin{field}
    \text{Variance}
    \begin{itemize}
      \item $V(X) = \sigma_x^2 = E[(X - E(X))^2] = E(X^2) - E(X)^2$
      \item $V(X+Y) = V(X) + V(Y) + Cov(X,Y)$
      \item $V\bigg[\sum_{i=1}^n X_i\bigg] = \sum_{i=1}^n V(X_i) + \sum_{i\neq j} Cov(X_i,X_j)$
    \end{itemize}
  \end{field}
\end{note}








\begin{note}
  \begin{field}
    \tiny 17
  \end{field}
  \begin{field}
    Covariance
    \begin{itemize}
      \item $Cov(X,Y) = $
      \item $Cov(X,c) = $
      \item $Cov(Y,X) = $
      \item $Cov(aX,bY) =$
      \item $Cov(X + a, Y + b) =$
      \item $Cov\bigg(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\bigg) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Covariance
    \begin{itemize}
      \item $Cov(X,Y) = E[(X - E(X)(Y - E(Y)))] = E(XY) - E(X)E(Y)$
      \item $Cov(X,c) = 0$
      \item $Cov(Y,X) = Cov(X,Y)$
      \item $Cov(aX,bY) = abCov(X,Y)$
      \item $Cov(X + a, Y + b) = Cov(X,Y)$
      \item $Cov\bigg(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\bigg) = \sum_{i=1}^n \sum_{j=1}^m Cov(X_i,Y_j)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 18
  \end{field}
  \begin{field}
    Correlation: $\rho(X,Y)$
  \end{field}
  \begin{field}
    Correlation: $\rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{V(X)V(Y)}}$
  \end{field}
\end{note}



\begin{note}
  \begin{field}
    \tiny 19
  \end{field}
  \begin{field}
    Conditional Variance
    \begin{itemize}
      \item $V(Y|X) = $
      \item $V(Y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Conditional Variance
    \begin{itemize}
      \item $V(Y|X) = E\big[(Y - E(Y|X))^2|X\big] = E(Y^2|X) - E(Y|X)^2$
      \item $V(Y) = E(V(Y|X)) + V(E(Y|X))$
    \end{itemize}
  \end{field}
\end{note}

%%end_tag
%%start_tag Undergrad_Textbook
%Chapter 2

\begin{note}
  \begin{field}
    \tiny 20
  \end{field}
  \begin{field}
    Law of total probability $k=2$ (using conditional probability)
  \end{field}
  \begin{field}
    $P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$

  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 21
  \end{field}
  \begin{field}
    Bayes formula in terms of law of total probability,
  \end{field}
  \begin{field}
    $P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^c)P(B^c)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 22
  \end{field}
  \begin{field}
    $P(A \text{ and }B) = P(A,B) = $
  \end{field}
  \begin{field}
    $P(A \text{ and }B) = P(A|B)P(B) = P(B|A)P(A)$
  \end{field}
\end{note}


% Chapter 3

\begin{note}
  \begin{field}
    \tiny 23
  \end{field}
  \begin{field}
    Events $A$ and $B$ are independent if
  \end{field}
  \begin{field}
    $P(A|B) = P(A)$ equivalently $P(A \text{ and } B)  = P(A)P(B)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 24
  \end{field}
  \begin{field}
    Poisson setting
  \end{field}
  \begin{field}
    The Poisson setting arises in the context of discrete counts of events that occur over space or time with the small probability and where successive events are independent

    Eg: 2 on average calls a minute, $X$ is number of calls a minute, $X \sim Pois $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 25
  \end{field}
  \begin{field}
    Poisson approximation of binomial distribution
  \end{field}
  \begin{field}
    Suppose $X \sim Binom(n,p)$, $Y \sim Pois (\lambda)$. If $n\to \infty$, and $p \to 0$, in such a way that $np \to \lambda > 0$, then for all $k$, $P(X = k) \to P(Y = k)$. The Poisson distribution with parameter $\lambda = np$ serves as a good approiximation for the binomial distribution when $n$ is large and $p$ is small.
  \end{field}
\end{note}

% Chapter 4


\begin{note}
  \begin{field}
    \tiny 26
  \end{field}
  \begin{field}
    $E(f(X,Y))$ when $X,Y$ are discrete
  \end{field}
  \begin{field}
    $E(f(X,Y)) = \sum_x \sum_y f(x,y)P(X=x,Y=y)$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 27
  \end{field}
  \begin{field}
    If $X,Y$ are independent, then $f(X),g(Y)$
  \end{field}
  \begin{field}
    are also independent
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 28
  \end{field}
  \begin{field}
    If $X,Y$ independent, $E(XY) = , E(f(X)g(Y)) = $
  \end{field}
  \begin{field}
    If $X,Y$ independent, $E(XY) = E(X)E(Y), E(f(X)g(Y)) = E(f(X))E(g(Y))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 29
  \end{field}
  \begin{field}
    Sum of independent discrete random variables $X,Y$: $P(X+Y = k)$
  \end{field}
  \begin{field}
    $P(X+Y = k) = \sum_i P(X=i)P(Y=k-i)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 30
  \end{field}
  \begin{field}
    $V(X) = 0$
  \end{field}
  \begin{field}
    If and only if $X $ is a constant
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 31
  \end{field}
  \begin{field}
    $E(I_A) = , V(I_A)$ Where $I_A$ is an indicator function
  \end{field}
  \begin{field}
    $E(I_A) = P(A), V(I_A) = P(A)P(A^c)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 32
  \end{field}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = $$
  \end{field}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = \frac{P(X=x,Y=y)}{P(X=x)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 33
  \end{field}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = $
  \end{field}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = \sum_y y P(Y = y | X = x)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 34
  \end{field}
  \begin{field}
    Problem solving strategy for expected value of counting
  \end{field}
  \begin{field}
    Use indicator functions for each trial , where $X = \sum I$ and use linearity of expectation
  \end{field}
\end{note}

 % Chapter 5

\begin{note}
  \begin{field}
    \tiny 35
  \end{field}
  \begin{field}
    $P(X > s + t|X > t)$ for geometric, exponential
  \end{field}
  \begin{field}
    $P(X > s + t|X > t) = P(X > s)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 36
  \end{field}
  \begin{field}
    Distribution for: A bag of $N$ balls which conatins $r$ red balls and $N-r$ blue balls, $X$ is number of red balls in a sample of size $n$ taken without replacement.
  \end{field}
  \begin{field}
    Hypergeometric.
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 37
  \end{field}
  \begin{field}
    Distribution for modeling arrival time
  \end{field}
  \begin{field}
    Exponential
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 38
  \end{field}
  \begin{field}
    $E(g(X,Y)) = $ (continuous )
  \end{field}
  \begin{field}
    $E(g(X,Y)) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty g(x,y)f(x,y)dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 39
  \end{field}
  \begin{field}
    $Cov(X,Y) = $ (integration )
  \end{field}
  \begin{field}
    $Cov(X,Y) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty (x - E(X))(y - E(Y))dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 40
  \end{field}
  \begin{field}
    Problem solving strategies for functions of random variables
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Methods of cdf: $Y = g(X)$, find cdf $P(Y \leq y) = P(g(X) \leq y) = P(X \leq g^{-1}(y))$
      \item For finding $P(X < Y)$, set up integrals that cover
      \item For finding probabilities of independent uniform random variables, use geometric (area) properties
    \end{itemize}
  \end{field}
\end{note}

% Chapter 7

\begin{note}
  \begin{field}
    \tiny 41
  \end{field}
  \begin{field}
    Quantile
  \end{field}
  \begin{field}
    If $X$ is a continuous random variable, then the $p$th quantile is is the number $q$ that satisfies $P(X \leq q) = p/100 $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 42
  \end{field}
  \begin{field}
    Poisson process
  \end{field}
  \begin{field}
    Times between arrivals are modeled as iid exponential random variables with parameter $\lambda = 1/\beta$. Let $N_t$ be the number of arrivals up to time $t$. Then $N_t \sim Pois(\lambda t)$
% go back and do properties of poisson process
  \end{field}
\end{note}

% Chapter 8

\begin{note}
  \begin{field}
    \tiny 43
  \end{field}
  \begin{field}
    Conditional density function $f_{Y|X}(y|x) = $
  \end{field}
  \begin{field}
    $f_{Y|X}(y|x) = \frac{f(x,y)}{f_x(x)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 44
  \end{field}
  \begin{field}
    Continuous bayes formula
  \end{field}
  \begin{field}
    $f_{X|Y}(x|y)  = \frac{f_{Y|X}(y|x)f_x(x)}{\int_{t = -\infty}^\infty f_{Y|X}(y|t)f_x(t)dt}$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 45
  \end{field}
  \begin{field}
    Conditional expectation for continuous random variables
    $E(Y|X = x)$
  \end{field}
  \begin{field}
    $E(Y|X = x) = \int_y y f_{Y|X}(y|x)dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 46
  \end{field}
  \begin{field}
    Law of total expectation
  \end{field}
  \begin{field}
    $E(Y) = E(E(Y|X))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 47
  \end{field}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = $
      \item $E(g(Y)|X=x) = $
      \item If $X,Y$ independent, $E(Y|X) = $
      \item If $Y = g(X)$, then $E(Y|X) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = a E(Y|X) + bE(Z|X)$
      \item $E(g(Y)|X=x) = \int_y g(y) f_{Y|X}(y|x)dy$
      \item If $X,Y$ independent, $E(Y|X) = E(Y)$
      \item If $Y = g(X)$, then $E(Y|X) = Y$
    \end{itemize}
  \end{field}
\end{note}



\begin{note}
  \begin{field}
  \tiny 49
\end{field}
  \begin{field}
    Conditional variance $V(Y|X = x)$
  \end{field}
  \begin{field}
    $$V(Y|X = x) = \sum_y (y - E(Y|X=x))^2P(Y=y|X=x)$$ discrete
    $$V(Y|X = x) = \int_y (y - E(Y|X=x))^2 f_{Y|X}(y|x)dy$$ continuous
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 50
  \end{field}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = $
      \item $V(aY + b | X = x) = $
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = E(Y^2 | X=x) - (E(Y|X=x))^2$
      \item $V(aY + b | X = x) = a^2 V(Y|X=x)$
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = V(Y|X=x) + V(Z|X=x)$
    \end{itemize}
  \end{field}
\end{note}

% Chapter 9

\begin{note}
  \begin{field}
    \tiny 51
  \end{field}
  \begin{field}
    $P(X \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(X \geq \epsilon) \leq E(X) / \epsilon$ (Markov's Inequality )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 52
  \end{field}
  \begin{field}
    $P(|X-\mu| \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(|X-\mu| \geq \epsilon) \leq \sigma^2/\epsilon^2$ (Chebyshev's inequality, if mean and variance finite )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 53
  \end{field}
  \begin{field}
    $P(\lim_{n\to \infty} S_n/n = \mu) = $
  \end{field}
  \begin{field}
  $P(\lim_{n\to \infty} S_n/n = \mu) = 1$ (Strong law of large numbers )
\end{field}
\end{note}

%%end_tag
%%end_tag
%%start_tag Theory 2
\tags{TheoryTwo t2}

\begin{note} \begin{field} \tiny 220 \end{field}
  \begin{field}
    Definition of Convergence
  \end{field}
  \begin{field}
    A sequence $\{a_n\}_{n > 1}$ of real numbers is said to \textbf{converge} to a point $a \in \mathbb{R}$ if for any $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that for all $m > N $ we have $|a_m - a| < \epsilon $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 221 \end{field}
  \begin{field}
    Example of convergence: $a_n = \frac{1}{n}$
  \end{field}
  \begin{field}
    For any $\epsilon > 0$, choose $N$ such that $\frac{1}{N} < \epsilon$. Then for any $m > N$ we have that

    $$a_n = \frac{1}{n} < \frac{1}{N} < \epsilon$$

    and therefore $|a_m - 0| = \frac{1}{n} < \epsilon$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 222 \end{field}
  \begin{field}
    Given two convergent sequences $\{a_n\}$ and $\{b_m\}$ such that $a_m \to a$ and $b_m \to b$\\
    $\lim_{n \to \infty} a_nb_n = $
  \end{field}
  \begin{field}
    Given two convergent sequences $\{a_n\}$ and $\{b_m\}$ such that $a_m \to a$ and $b_m \to b$\\
    $\lim_{n \to \infty} a_nb_n = (\lim_{n \to \infty}a_n)(\lim_{n \to \infty}b_n) = ab$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 223 \end{field}
  \begin{field}
    Definition: Convergence in probability
  \end{field}
  \begin{field}
    A sequence of random variables $\{X_n\}_{n \geq 1}$ \textbf{converges in probability } to a random variable $X$, if for every $\epsilon > 0$, $$\lim_{n \to \infty}P(|X_n - X| \geq \epsilon) = 0 $$
    We write $X_n \overset{p}{\to} X$

    Equivalently, $x_m \overset{p}{\to} x$ if $\lim_{n \to \infty}P(|x_n - x| < \epsilon) = 1$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 224 \end{field}
  \begin{field}
    Convergence in probability example:
    Let $\{x_n\}$ be a sequence of random variables such that $x_n \sim N(0,1/m^2)$\\
    Show that $x_n \overset{p}{\to} 0$:
  \end{field}
  \begin{field}
    Let $\epsilon > 0$. We obtain $P(|x_n - 0|) = P(x_n > \epsilon) + P(X_n < -\epsilon )$. ie we are looking at the tail probabilities.

    Now,
    \begin{align*}
      P(X_n < -\epsilon) + P(x_n > \epsilon) &=
      P(nx_n < n\epsilon) + P(nx_n > n\epsilon)\\
      &= \Phi(n\epsilon) + 1 - \Phi(n \epsilon)\\
      &= 2\Phi(-n\epsilon) \underset{n\to \infty}{\to}  0
    \end{align*}

    Therefore $x_n \overset{p}{\to} 0$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 225 \end{field}
  \begin{field}
    Example convergence in probability
    Let $W \sim N(0,1)$ and $U \sim Unif(0,1)$, and define the sequence $\{x_n\}_{n \geq 1}$ as $x_n = W $with prob $1 - 1/n, U$ with prob $ 1/n$

    Show that $x_n \overset{p}{\to} W $

  \end{field}
  \begin{field}
    Let $\epsilon > 0$ Then.

    \begin{align*}
      P(|X_n - W| > \epsilon) &= P(|X_n - W| > \epsilon | X_n = W)P(X_n = W) \\
      &+ P(|X_n - W| > \epsilon | X_n = U)P(X_n = U)\\
      &= 0 \cdot (1 - 1/n) + p_n (1/n)
    \end{align*}

    Where $p_n  $ is a probability, and therefore $0 \leq p_n \leq 1 $

    It follows that $p_n \frac{1}{n} \underset{n \to \infty}{\to} 0$, and therefore $P(|X_n - W| > \epsilon ) \underset{n \to \infty}{\to} 0$, for all $\epsilon > 0$, so that $X_n \overset{p}{\to} W$.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 226 \end{field}
  \begin{field}
    Does $X_n \overset{p}{\to} c$ imply $E(X_n) \to c $?
  \end{field}
  \begin{field}
    No


    Let $X_n = 0$ with probability $1 - 1/n$, $n^2 $ with probability $1/n$
    Then $P(|X_n - 0| > \epsilon) \leq P(X_n = n^2) = 1/n \underset{n \to \infty}{\to} 0$
    On the other hand,
    $E(X_n) = 0 \cdot P(X_n =0) + n^2P(X_n = n^2) = 0 + n^2 \frac{1}{n} = n \underset{n \to \infty}{\to} \infty$.
    Therefore $X_n \overset{p}{\to} c$ does not imply $E(X_n) \to c $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 227 \end{field}
  \begin{field}
    Does $E(X_n) \to c $ imply $X_n \overset{p}{\to}c$?
  \end{field}
  \begin{field}
    No.

    Let $X_n = 0, $ with prob $1 - 1/n$, $n$ with prob $1/n$.
    Then $E(X_n) = 0 \cdot P(X_n = 0) + n P(X_n = n) = 0  + n 1/n = 1$ for all n.
    But $P(|X_n - 0| > \epsilon) \leq P(X_n = n) = \frac{1}{n } \underset{n \to \infty}{\to} 0$
    It follows, $X_n \overset{p}{\to} 0 $, and therefore we have $E(X_n) \to c $does not imply $X_n \overset{p}{\to}c$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 228 \end{field}
  \begin{field}
     Suppose $\{X_n\}_{n \geq 1}$ and $\{Y_n\}_{n \geq 1}$ be two sequences of random variables such that $X_n \overset{p}{\to} x_0$ and $Y_n \overset{p}{\to} y_0$ as $n \to \infty$, where $x_o, y_0 \in \mathbb{R}$

     What properties do we have?
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $X_n \pm Y_m \overset{p}{\to} x_0 \pm y_0$ as $n$ increases to $\infty$
      \item $X_nY_n \overset{p}{\to} x_0y_0$ as $n$ increases to $\infty$
      \item $X_n/Y_n \overset{p}{\to} x_0/y_0$ as $n$ increases to infinity, provided that $P(Y_n = 0) = 0 $ fro all $n$ and $y_0 \neq 0$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 229 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $x_n \overset{p}{\to} x_0 \in \mathbb{R}$, as $n \to \infty$, and let $g: \mathbb{R} \to \mathbb{R}$ be a continuous function . Then $$g(X_n) \overset{p}{\to}  \text{ as } n \to \infty$$
  \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $x_n \overset{p}{\to} x_0 \in \mathbb{R}$, as $n \to \infty$, and let $g: \mathbb{R} \to \mathbb{R}$ be a continuous function . Then $$g(X_n) \overset{p}{\to} g(x_0) \text{ as } n \to \infty$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 230 \end{field}
  \begin{field}
    Proof of: Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $x_n \overset{p}{\to} x_0 \in \mathbb{R}$, as $n \to \infty$, and let $g: \mathbb{R} \to \mathbb{R}$ be a continuous function . Then $$g(X_n) \overset{p}{\to} g(x_0) \text{ as } n \to \infty$$
  \end{field}
  \begin{field}
    Since $g$ is continuous at $X = x_0$, we have that for any $\epsilon > 0$, there exits $\delta > 0$ such that $|g(x) - g(x_0)| > \epsilon $ implies $|x - x_0| > \delta $

    We obtain

    $$0 \leq P(|g(X_n) - g(x_0)| > \epsilon ) \leq P(|X_n - x_0| > \delta) \underset{n \to \infty}{\to} 0$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 231 \end{field}
  \begin{field}
    Weak Law of Large numbers
  \end{field}
  \begin{field}
    Let $X_1, X_2, X_3 \ldots$ Be a sequence of iid random variables with $E(X_1) = \mu$ (finite) and $V(X_1) = \sigma^2 < \infty$, and define $\bar{X_n} = \frac{1}{n} \sum_{i = 1}^n X_i$ (the sample mean).

    Then $$ \bar{X_n }\overset{p}{\to}\mu \text{ as } n \to \infty$$

  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 232 \end{field}
  \begin{field}
    Proof of Weak Law of Large Numbers
  \end{field}
  \begin{field}
    \begin{align*}
      P(|\bar{X_n} - \mu| > \epsilon ) &= P((\bar{X_n} - \mu)^2 > \epsilon^2)\\
      &\leq \frac{E((\bar{X_n} - \mu)^2)}{\epsilon^2} \text{ by Chebyshev's Inequality}\\
      &= \frac{V(\bar{X_n})}{\epsilon^2} \text{ by def of variance}\\
      &= \frac{\sigma^2}{n \epsilon^2} \underset{n \to \infty}{\to} 0
    \end{align*}

    Therefore $\bar{X_n} \overset{p}{\to} \mu$
  \end{field}
\end{note}


%%start_tag Wk2

\begin{note} \begin{field} \tiny 233 \end{field}
  \begin{field}
    Consistency
  \end{field}
  \begin{field}
    If our estimate converges in probability to the value of the parameter of interest as the sample size $n$ increases
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 234 \end{field}
  \begin{field}
    Consistency of $S^2$
  \end{field}
  \begin{field}
    Suppose $X_1, X_2, \ldots$ is a sequence of iid random varialbes with $E(X_1) = \mu$ finite and $V(X_1) = \sigma^2 < \infty$

    and define $$ S_n^2 = \frac{1}{n-1}\sum_{i = 1}^n(X_i - \bar{X_n})^2 \quad \text{The sample variance}$$

    Can we show that $S_n^2$ is a consistent estimate of $\sigma^2$? In other words, can we show taht $S_n^2 \overset{p}{\to} \sigma^2 \text{ as } n \to \infty$

    Using Chebychev's inequality, we obtain

    \begin{align*}
      P(|S_n^2 - \sigma^2| > \epsilon ) &\leq \frac{E[(S_n^2 - \sigma^2)^2]}{\epsilon^2}\\
      &= \frac{V(S_n^2)}{\epsilon^2}
    \end{align*}

    There fore, a sufficient condition that $S_n^2$ converges in probablility to $\sigma^2$ is that the variance of $S_n^2$  $V(S_n^2) \to 0$, as $n \to \infty$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 235 \end{field}
  \begin{field}
    $V(S_n^2) \to 0$ as long as
  \end{field}
  \begin{field}
    $V(S_n^2) \to 0$ as long as the fourth central moment $\mu_4 = E[(X_1 - \mu)^4]$ is finite.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 236 \end{field}
  \begin{field}
    Khinchin's WLLN
  \end{field}
  \begin{field}
    Let $X_1, X_2, \ldots$ be a sequence of iid random variables with $E(X_1) = \mu$ (finite). Then, $\bar{X_n} \overset{p}{\to} \mu$ as $n \to \infty$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 237 \end{field}
    \begin{field}
      Let $X_1, X_2 \ldots$ be a sequence of random variables, such that for some $r > 0$ and $c \in \mathbb{R}$, $E[|X_n - c|^r] \underset{n \to \infty}{\to} 0$. Then $X_n \overset{p}{\to} $, as $n \to \infty$
    \end{field}
    \begin{field}
      (A general result to establish convergence in probability )

      Let $X_1, X_2 \ldots$ be a sequence of random variables, such that for some $r > 0$ and $c \in \mathbb{R}$, $E[|X_n - c|^r] \underset{n \to \infty}{\to} 0$. Then $X_n \overset{p}{\to} c$, as $n \to \infty$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 238 \end{field}
  \begin{field}
    Consistent estimator for $X_1, X_2, \ldots X_n \sim $ iid Univorm($0,\theta$), $\theta > 0$. ( and sketch of proof )
  \end{field}
  \begin{field}
    $X_{(n)} = \max(X_1, \ldots X_n)$ (the largest order statistic)

    Proof

    First recall that the pdf of $X_{(n)}$ is given by $$f(x) = nx^{n-1} \theta^{-n}, 0 < x < \theta, 0 \text{otherwise} $$

    We obtain

    \begin{align*}
      E(X_{(n)}) &= \int_{0}^\theta x f(x) dx \\
      &= n \theta^{-n} \int_0^\theta x^n dx \\
      &= \frac{n}{n-1}\theta\\
      E(X_{(n)}^2) &= \int_0^\theta x^2f(x)dx \\
      &= n\theta^{-n} \int_0^\theta x^{n+1}dx \\
      &= \frac{n}{n + 2} \theta^2
    \end{align*}

    We have

    \begin{align*}
      E[(X_{(n)} - \theta)^2] &= E(X_{(n)}^2) - 2\theta E(X_{(n)}) + \theta^2 \\
      &= \frac{n}{n + 2}\theta^2 - 2\theta \frac{n}{n + 1} \theta + \theta^2\\
      &\cdots\\
      &= \frac{2\theta^2}{(n + 1)(n + 2)} \underset{n \to \infty}{\to} 0
    \end{align*}

    Hence, taking $c = 0$ and $r = 2$, from the previous theorem, we obtain $X_{(n)} \overset{p}{\to} \theta$ as $n \to \infty$

  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 239 \end{field}
  \begin{field}
    Definition Almost Sure Convergence
  \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables is said to converge \textbf{Almost Surely} to a random variable $X$ if for every $\epsilon > 0$,
    $$ P(\lim_{n \to \infty}|X_n - X| > \epsilon) = 0$$

    We write $X_n \overset{a.s}{\to} X$ as $n \to \infty$

  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 240 \end{field}
  \begin{field}
    Strong Law of Large Numbers
  \end{field}
  \begin{field}
    Let $X_1, X_2, \ldots$ be an iid sequence of random variables, with $E(X_1) = \mu$ (finite) and $V(X_1) = \sigma^2 < \infty$. Then,
    $$\bar{X_n} \overset{a.s}{\to} \mu \quad \text{as } \mu \to \infty$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 241 \end{field}
  \begin{field}
    Does convergence in probability imply convergence almost surely?
  \end{field}
  \begin{field}
    No.
    Let $\Omega = [0.1]$, with uniform probability distribution.  Define the sequence $\{X_n\}_{n \geq 1}$ as:
    \begin{align*}
      X_1(\omega) &= \omega + \mathbb{I}_{[0,1]}(\omega)\\
      X_2(\omega) &= \omega + \mathbb{I}_{0,1/2}(\omega)\\
      X_3(\omega) &= \omega + \mathbb{I}_{1/2,1}(\omega)\\
      X_4(\omega) &= \omega + \mathbb{I}_{0,1/3}(\omega)\\
      X_5(\omega) &= \omega + \mathbb{I}_{1/3,2/3}(\omega)\\
      &\vdots
    \end{align*}
    $X_5(\omega) = \omega + 1$

    Let $X(\omega) = \omega$, then it is easy to show that $X_n \overset{p}{\to} X$ because $P(|X_n - X| \geq \epsilon) = P([a_n,b_n])$, where $l_n = \text{length}([a_n, b_n]) \underset{n \to \infty}{\to} 0$.

    However $X_n$ does not converge to $X$ almost surely, because for every $\omega \in [0,1]$, alternates between $\omega$ and $\omega + 1$, infinetly often as $n \to \infty$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 242 \end{field}
  \begin{field}
    Convergence in Distribution
  \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables converges in distribution to a random variable $X$ if,
    $$ \lim_{n \to \infty} F_{X_n}(x) = F_X(x)$$

    at all points $x$ where $F_X(x)$ is continuous

    We write $X_n \overset{d}{\to} X$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 243 \end{field}
  \begin{field}
    Example of convergence in distribution

    Let $X_n \sim N(0, \frac{n+1}{n})$, and $X \sim N(0,1)$. We want to show that $X_n \overset{d}{\to} X$.
  \end{field}
  \begin{field}
    \begin{align*}
      P(X_n \leq X) &= P(\sqrt{\frac{n}{n + 1}}X_n \leq \sqrt{\frac{n}{n+1}}x)\\
      &= \Phi(\sqrt{\frac{n}{n + 1}}x) \underset{n \to \infty}{\to} \Phi(x)
    \end{align*}

    And we obtain that $F_{X_n} \to \Phi(x) = F_X(x), \forall x$, and therefore $X_n \overset{d}{\to} X$
  \end{field}
\end{note}

%%end_tag
%%start_tag Wk3

\begin{note} \begin{field} \tiny 244 \end{field}
  \begin{field}
    Does Convergence in probability imply convergence in distribtuion?
  \end{field}
  \begin{field}
    Yes
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 245 \end{field}
  \begin{field}
    Does Convergence in distribution imply convergence in probability?
  \end{field}
  \begin{field}
    No - unless converges in distribution to a constant
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 246 \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables converges in probability to a constant $c \in \mathbb{R}$ if and only if
  \end{field}
  \begin{field}
    A sequence $\{X_n\}_{n \geq 1}$ of random variables converges in probability to a constant $c \in \mathbb{R}$ if and only if the sequence converges in distribution to $c$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 247 \end{field}
  \begin{field}
    If $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} Y$ we have that
    \begin{enumerate}
      \item $X_n \pm Y_n $
      \item $X_nY_n $
    \end{enumerate}
  \end{field}
  \begin{field}
    In general it is not true that if $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} Y$ we have that
    \begin{enumerate}
      \item $X_n \pm Y_n \overset{d}{\to} X + Y$
      \item $X_nY_n \overset{d}{\to} XY$
    \end{enumerate}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 248 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $X_n \overset{d}{\to}X$, for some random variable $X$ (possibly a constant). Then for any continuous funciton $g:\mathbb{R} \to \mathbb{R}$, we have $g(X_n) \overset{d}{\to} $
  \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that $X_n \overset{d}{\to}X$, for some random variable $X$ (possibly a constant). Then for any continuous funciton $g:\mathbb{R} \to \mathbb{R}$, we have $g(X_n) \overset{d}{\to} g(X)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 249 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ and $\{Y_n\}_{n \geq 1}$ be two sequences of random variables such that $X_n \overset{d}{\to}X$ for some random variable $X$ (possibly a constant) and $Y_n \overset{p}{\to} c \in \mathbb{R}$

    Then, as $n \to \infty$,

    \begin{enumerate}
      \item $X_n \pm Y_n \overset{d}{\to} $
      \item $X_nY_n \overset{d}{\to} $
      \item $X_n/Y_n \overset{d}{\to} \quad \text{ provided } P(Y_n = 0) = 0 \forall n \text{ and } c \neq 0$
    \end{enumerate}
  \end{field}
  \begin{field}
    Slutsky's Theorem
    Let $\{X_n\}_{n \geq 1}$ and $\{Y_n\}_{n \geq 1}$ be two sequences of random variables such that $X_n \overset{d}{\to}X$ for some random variable $X$ (possibly a constant) and $Y_n \overset{p}{\to} c \in \mathbb{R}$

    Then, as $n \to \infty$,

    \begin{enumerate}
      \item $X_n \pm Y_n \overset{d}{\to} X \pm c$
      \item $X_nY_n \overset{d}{\to} cX$
      \item $X_n/Y_n \overset{d}{\to} X/c \quad \text{ provided } P(Y_n = 0) = 0 \forall n \text{ and } c \neq 0$
    \end{enumerate}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 250 \end{field}
  \begin{field}
    Central Limit Theorem
  \end{field}
  \begin{field}
    Let $X_1, X_2, \ldots$ be an iid sequence of random variables, with $E(X_1) = \mu$(finite) and $V(X_1) = \mu^2 < \infty$

    Then, for $\bar{X_n} = \frac{1}{n }\sum_{i = 1}^\infty X_i$ (the sample mean), we have that
    $$ \frac{\sqrt{n}(\bar{X_n} - \mu)}{\sigma} \overset{d}{\to} N(0,1) \quad \text{as } n \to
    \infty$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 251 \end{field}
  \begin{field}
    Equivalent results of CLT
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $\frac{(\bar{X_n} - \mu)}{\frac{\sigma}{\sqrt{n}}} \overset{d}{\to}N(0,1)$
      \item $\sqrt{n}(\bar{X_n} - \mu) \overset{d}{\to} N(0,\sigma^2)$
      \item $\frac{\sum_{i=1}^n X_i - n\mu}{\sqrt{n}\sigma} \overset{d}{\to}N(0,1)$
      \item $\bar{X_n} \overset{d}{\to} N(\mu, \sigma^2/n)$
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 252 \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that the mgf $M_{X_n}(t)$ of $X_n$ exists in a neighborhood of 0, for all , and suppose that

    $$ \lim_{n \to \infty} M_{X_n}(t) = M_X(t) \quad \text{for all } t \text{ in a neighborhood of }0$$
    where $M_X(t)$ is the mgf for some random variable $X$. Then,
  \end{field}
  \begin{field}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables such that the mgf $M_{X_n}(t)$ of $X_n$ exists in a neighborhood of 0, for all , and suppose that

    $$ \lim_{n \to \infty} M_{X_n}(t) = M_X(t) \quad \text{for all } t \text{ in a neighborhood of }0$$
    where $M_X(t)$ is the mgf for some random variable $X$. Then, there exists a unique cdf $F_x(x)$ whose moments are determined by $M_y(t)$ and for all $x$, where $F_x(x)$ is continuous we have $\lim_{n \to \infty} F_{X_n}(x) = F_x(x)$
  \end{field}
\end{note}
%%end_tag
%%start_tag Wk4

\begin{note} \begin{field} \tiny 253 \end{field}
  \begin{field}
    $\frac{\sqrt{n}(\bar{X} - \mu)}{S_n} \overset{d}{\to} $
  \end{field}
  \begin{field}
    Using the CLT, and slutsky's theorem, we have

    $$\frac{\sqrt{n}(\bar{X} - \mu)}{S_n} = \frac{\sigma}{S_n}\cdot \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \overset{d}{\to} N(0,1) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 254 \end{field}
  \begin{field}
    $ g(X) \approx $ \\
$E(g(X)) \approx $, $V(g(X))\approx $
  \end{field}
  \begin{field}
    $$ g(X) \approx g(\mu) + g'(X)(X-\mu)$$ Using a first order taylor approximation
    $E(g(X)) \approx g(\mu)$, $V(g(X))\approx [g'(\mu)]^2V(X)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 255 \end{field}
  \begin{field}
    Delta Method
  \end{field}
  \begin{field}
    Let $\{Y_n\}_{n \geq 1}$ be a sequence of random variables such that $\sqrt{n}(Y_n - \theta) \overset{d}{\to} N(0,\sigma^2)$ as $n \to \infty$. Suppose that for a given function $g$ and a specific value of $\theta$, $g'(\theta)$ exists and is not equal to zero. Then
    $$\sqrt{n}(g(Y_n) - g(\theta)) \overset{d}{\to} N(0,\sigma^2[g'(\theta)]^2)$$

    as $n\to \infty$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 256 \end{field}
  \begin{field}
    Second Order delta method
  \end{field}
  \begin{field}
    Let $\{Y_n\}_{n \geq 1}$ be a sequence of random variables such that $\sqrt{n}(Y_n - \theta ) \overset{d}{\to} N(0,\sigma^2)$ as $n\to \infty$.
    And that for a given function $g$ as specific value of $\theta$, we hvae $g'(\theta)=0$, but $g''(\theta)$ Exists and is not equal to 0. Then

    $$\sqrt{n}(g(Y_n) - g(\theta)) \overset{d}{\to} \sigma^2 \frac{g''(\theta)}{2}\chi^2_1 \quad \text{as } n \to \infty$$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 257 \end{field}
  \begin{field}
    $\chi_n^2 \dot{\sim} $ for sufficiently large $n$
  \end{field}
  \begin{field}
    $\chi_n^2 \dot{\sim} N(n,2n)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 258 \end{field}
  \begin{field}
    Definition Statistic
  \end{field}
  \begin{field}
    Let $X_1, \ldots, X_n$ be a random sample from a given population. Then, any \underline{observable} real-valued (or vector-valued) function $T(\mathbf{X}) = T(X_1, \ldots, X_n)$ of the random variables $X_1, \ldots, X_n$ is called a \textbf{Statistic}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 259 \end{field}
  \begin{field}
    Sampling Distribution
  \end{field}
  \begin{field}
    The probability distribution of the statitic $T(\mathbf{X})$ is called the \textbf{Sampling Distribution} of $T(\mathbf{X})$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 260 \end{field}
  \begin{field}
    Sufficient Statistic
  \end{field}
  \begin{field}
    A statistic $T(\mathbf{X})$ is a \textbf{Sufficient Statistic} for $\theta$, if the conditional distribution of the sample $\mathbf{X}$ given the value of $T(\mathbf{X})$ does not depend on $\theta$
  \end{field}
\end{note}
%%end_tag
%%start_tag Wk5

\begin{note} \begin{field} \tiny 261 \end{field}
  \begin{field}
    Determine if $T(\mathbf{X}) = \sum X_i$ where $X_i\sim Bern(p)$ is sufficient for $p$ using definition of sufficiency
  \end{field}
  \begin{field}
    \begin{align*}
      P(\mathbf{X} = \mathbf{x}\big| T = t) &= \frac{P(\cap_{i = 1}^n X_i = x_i)}{P(T = t)}\\
      &= \prod_{i = 1}^n \frac{P(X_i = x_i)}{P(T = t)} \quad \text{by independence}\\
      &= \frac{p^{\sum_{i = 1}^n x_i}(1 - p)^{n - \sum_{i = 1}^n x_i}}{\binom{n}{t}p^t(1 -p)^{n-t}} \quad \text{Because } T \sim \text{Binom}(n,p)\\
      &= \frac{p^t(1-p)^{n-t}}{\binom{n}{t}p^t(1-p)^{n-t}} \quad \text{because }t = \sum_{i = 1}^n x_i\\
      &= \frac{1}{\binom{n}{t}} \quad \text{which is free of  }p
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 262 \end{field}
  \begin{field}
    How to show sufficiency (not using factorization)
  \end{field}
  \begin{field}
    Let $p(\mathbf{X}|\theta)$ be the joint PDF or PMF of $\mathbf{X}$ and $q(t|\theta)$ the PDF or PMF of the statistic $T(\mathbf{X})$. Then $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if for every $\mathbf{X}$ in the sample space, the ratio

    $$ \frac{p(\mathbf{x}|\theta)}{q(T(\mathbf{x})|\theta)}$$

    is constant as a function of $\theta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 263 \end{field}
  \begin{field}
    Suppose that $X_1, \ldots X_n$ are iid $N(\mu,\sigma^2)$ where $\sigma^2$ is known. If the statistic $T(\mathbf{X}) = \bar{X_n}$ sufficient for $\mu$?
  \end{field}
  \begin{field}
    \begin{align*}
       \frac{f(\mathbf{x}|\mu)}{q(T(\mathbf{X})|\mu)} &=
       \frac{(2\pi\sigma^2)^{n/2}e^{-\frac{1}{2\sigma^2}[\sum_{i = 1}^n(x_i - \bar{x})^2 + n(\bar{x} - \mu)^2]}}{(2\pi\sigma/n)^{-1/2}e^{-\frac{1}{2\sigma^2}(\bar{x}-\mu)^2}}\\
       &= n^{-1/2}(2\pi\sigma^2)^{-(n-1)/2}e^{-\frac{1}{2\sigma^2}\sum_{i = 1}^n(x_i - \bar{x})^2}
    \end{align*}

    Which does not depend on $\mu$, and therefore $\bar{X_n}$ is suffient for $\mu$ as long as $\sigma^2$ is known
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 264 \end{field}
  \begin{field}
  Find the joint pdf of the sample $\mathbf{X} =(X_1, X_2, \ldots X_n)$ given that $X_1, \ldots X_n$ are iid $N(\mu,\sigma^2)$ where $\sigma^2$ is known.
  \end{field}
  \begin{field}
    \begin{align*}
      f(\mathbf{x}|\mu) &= \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-1}{2\sigma^2}(x_i - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2} e^{-1/2\sigma^2 \sum_{i = 1}^n (x_i - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2} e^{-1/2\sigma^2 \sum_{i = 1}^n (x_i - \bar{x} + \bar{x} - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2} e^{-1/2\sigma^2 \sum_{i = 1}^n (x_i - \bar{x})^2 + 2(\bar{x} - \mu)\sum_{i = 1}^n (x_i - \bar{x}) + n(\bar{x} - \mu)^2}\\
      &= (2\pi\sigma^2)^{n/2}e^{-1/2\sigma^2 (\sum_{i = 1}^n (x_i - \bar{x})^2 + n(\bar{x} - \mu)^2)}
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 265 \end{field}
  \begin{field}
    Show a statistic $T(\mathbf{X})$ is sufficient
  \end{field}
  \begin{field}
    Neyman factorization theorem
    Let $f(\mathbf{x}|\theta)$ denote the joint pdf or pmf of the sample $\mathbf{X}$, A statistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if and only if there exists functions $g(t|\theta)$ and $h(\mathbf{x})$ such that for all sample points $\mathbf{x}$ and all values of $\theta$ we can write
    $$ f(\mathbf{x}|\theta) = g(T(\mathbf{}x)|\theta)h(\mathbf{x})$$

    Note, in the theorem
    \begin{itemize}
      \item The function $g(T(\mathbf{X})|\theta)$ depends on $\mathbf{x} = (x_1, \ldots x_n)$ only through the statistic $T(\mathbf{X})$.
      \item The function $h(\mathbf{X})$ does not depend on $\theta$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 266 \end{field}
  \begin{field}
    Exponential Family
  \end{field}
  \begin{field}
    $$f(\mathbf{X}|\bf{\theta}) = h(\mathbf{x})c(\mathbf{\theta})e^{\sum _{i = 1}^n w_i(\bf(\theta))t_i(x)}$$
  \end{field}
\end{note}



%%end_tag
%%start_tag Wk6

\begin{note} \begin{field} \tiny 267 \end{field}
  \begin{field}
    Sufficiency in the exponential family
  \end{field}
  \begin{field}
    Let $X_1, \ldots , X_n$ be iid observations from a PDF or PMF, $f(x|\boldsymbol\theta)$ that belongs to an exponential family of the form

    $$ f(x|\boldsymbol\theta) = h(x)c(\boldsymbol\theta)e^{\sum _{i = 1}^k w_i(\boldsymbol\theta)t_i(x)}$$

    Where $\boldsymbol\theta = (\theta_1, \ldots, \theta_d)$, $d\leq k$. Then
    $$ T(\mathbf{X}) = \big(\sum _{j = 1}^k t_i(x_j), \cdots , \sum _{j = 1}^k t_k(x_j)\big)$$
    is sufficient
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 268 \end{field}
  \begin{field}
    Minimal Sufficient Statistic
  \end{field}
  \begin{field}
    A sufficient statistic $T(\mathbf{X})$ is called a \textbf{Minimal Sufficient Statistic} if for any other sufficient statistic $T'(\mathbf{X})$, $T(\mathbf{X})$ is a function of $T'(\mathbf{X})$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 269 \end{field}
  \begin{field}
    Determining if a statistic is minimal sufficient
  \end{field}
  \begin{field}
    Let $f(x|\theta)$ be the PDF or PMF of a sample $\mathbf{X}$. Suppose there exists a function $T(x)$ such that, for every two sample points, $\mathbf{x}$ and $\mathbf{y}$, the ratio $\frac{f(\mathbf{x}|\theta)}{f(\mathbf{y}|\theta)}$ is constant as a fucntion of $\theta$ iff and only if $T(\mathbf{x}) = T(\mathbf{y})$. Then $T(\mathbf{x})$ is a minimal sufficient statistic for $\theta$.
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 270 \end{field}
  \begin{field}
    Example of finding a minimal sufficient statistic:
    Suppose that $X_1, \ldots , X_n$ are idd Bernoulli($p$). What is a minimal sufficient statistic for $p$?
  \end{field}
  \begin{field}
    \begin{align*}
      f(\mathbf{x}|p) &= \prod_{i = 1}^n p^{x_i}(1-p)^{1 - x_i}\\
      &= p^{\sum _{i = 1}^n x_i}(1-p)^{n - \sum _{i = 1}^nx_i}
    \end{align*}

    And therefore for any two sample points $\mathbf{x}$ and $\mathbf{y}$, we obtain

    \begin{align*}
      \frac{f(\mathbf{x}|p)}{f(\mathbf{y}|p)} &= \frac{p^{\sum _{i = 1}^n x_i}(1-p)^{n - \sum _{i = 1}^nx_i}}{p^{\sum _{i = 1}^n y_i}(1-p)^{n - \sum _{i = 1}^ny_i}}\\
      &= p^{\sum _{i = 1}^n x_i - \sum _{i = 1}^n y_i}(1-p)^{\sum _{i = 1}^n y_i - \sum _{i = 1}^nx_i}
    \end{align*}

    Which is constant as a function of $p$ iff $\sum _{i = 1}^n x_i = \sum _{i = 1}^n y_i$

    Hence it follows from Lehman-Sheffe that $T(\mathbf{x}) = \sum _{i = 1}^n x_i$ is minimal sufficient for $p$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 271 \end{field}
  \begin{field}
    Minimal sufficient statistic for $\mu,\sigma^2$, where the Xs are $N(\mu,\sigma^2)$
  \end{field}
  \begin{field}
    $T(\mathbf{x}) = (\bar{x},S_x^2)$ by Lehmann-Schaffe is minimal sufficient.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 272 \end{field}
  \begin{field}
    Facts about sufficiency
  \end{field}
  \begin{field}
    \begin{itemize}
      \item The entire sample $\mathbf{X}$ is always sufficeint.
      \item Any one-to-one funciton of a minimal sufficient statisitc is also a minimal sufficient statistic
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 273 \end{field}
  \begin{field}
    Ancillary Statistic
  \end{field}
  \begin{field}
    A statistic $S(\mathbf{X})$ whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic for $\theta$
  \end{field}
\end{note}



%%end_tag
% Maybe add some stuff about ancilary statistics
%%start_tag Wk7
\begin{note} \begin{field} \tiny 274 \end{field}
  \begin{field}
    Complete statistic
  \end{field}
  \begin{field}
    Let $f(t|\theta)$ be the family of pdf's or pmfs for a statistic $T= T(\mathbf{x})$.

    The family of probability distributions is called \textbf{complete} (with respect to $\theta$) if $E_\theta(g(t)) = 0$ for all $\theta$, implies $P_\theta(g(T) = 0) = 1$ for all $\theta$

    Equivalently, we say that $T = T(\mathbf{X})$ is a complete statistic.

    In short, a statistic $T = T(\mathbf{X})$ is complete, if $E_\theta(g(T)) = 0$ for all $\theta$ implies $g(t) = 0$ with probability 1
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 275 \end{field}
  \begin{field}
    (Binomial complete sufficient statistic)
  \end{field}
  \begin{field}
    Suppose the statistic $T \sim Binom(n,p)$, $0 < p < 1$, and let $g$ be a function such that $E_p(g(T)) = 0$ for all $p$.

    Then, with $r = (\frac{p}{1-p})^t$

    \begin{align*}
      0 &= E_p(g(T))\\
      &= \sum_{t=0}^n g(t) \binom{n}{t}p^t(1-p)^{n-1}\\
      &= (1-p)^n \sum_{t = 0}^n g(t)\binom{n}{t}(\frac{p}{1-p})^t\\
      &= (1-p)^n \sum_{t = 0}^n g(t)\binom{n}{t}r^t\\
      &= \neq 0 \cdot \text{This is a polynomial of degree }n \text{ in } r \text{ with coefficients } g(t)\binom{n}{t}
    \end{align*}

    For the polynomial to be $0$ for all $r$ (and consequently for all $p$) each coefficient must be zero and therefore it must be the case that $g(t) = 0$ for $t = 0, 1, 2, \cdots , n$ Since $T \sim Binom(n,p)$, we have that $T$ takes on the values $t = 0,1,2,\ldots n$ with probability $1$ and therefore, we obtain $P_p(g(T)=0)=1$. Hence T is a complete statistic.
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 276 \end{field}
  \begin{field}
    Uniform$(0,\theta)$ complete sufficient statistic
  \end{field}
  \begin{field}
    $T(\mathbf{X}) = X_{(n)}$

    Suppose that $X_1, \ldots , X_n$ are iid Uniform($0,\theta$), $\theta > 0$. We know that $T(\mathbf{X}) = X_{(n)}$ (the max order statistic) is sufficient for $\theta$. Furtheremore ,
    $$ f(t|\theta) = nt^{n-1}\theta^{-n} \quad 0 < t < \theta$$
    Now suppose that $g(t)$ is a function satisfying $E_\theta(g(T)) =0, \forall \theta$
    Differentiating on both sides with respect to $\theta$,

    \begin{align*}
       0 &= \frac{d}{d\theta} E_\theta(g(t))\\
       &= \frac{d}{d\theta} \int_0^\theta g(t)nt^{n-1}\theta^{-n} dt\\
       &= \theta^{-n}\frac{d}{d\theta} \int_0^\theta g(t)nt^{n-1} dt + (\frac{d}{d\theta}\theta^{-n}) \int_0^\theta g(t)nt^{n-1}dt \\
       &= \theta^{-n}g(\theta)n\theta^{n-1} + 0
    \end{align*}

    Since $n\theta^{-1} \neq 0$, we must have that $g(\theta) =0 \quad \forall \theta> 0$. And therefore $T$ is complete.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 277 \end{field}
  \begin{field}
    Does minimal sufficent imply complete?
  \end{field}
  \begin{field}
    No

    Suppose that $X_1, \ldots X_n$ are iid $N(\theta,\theta^2)$ where $\theta \in \mathbb{R}$ is the unknown parameter of interest.

    We have

    \begin{align*}
      \frac{f(\mathbf{x}|\theta)}{f(\mathbf{y}|\theta)}
      &= \frac{(2\phi\sigma^2)^{-n/2}e^{-\frac{1}{2\sigma^2}\sum_{i =1}^n (x_i -\theta)^2}}{(2\phi\sigma^2)^{-n/2}e^{-\frac{1}{2\sigma^2}\sum_{i =1}^n (y_i -\theta)^2}}\\
      &= \frac{e^{-\frac{1}{2\sigma^2}[\sum _{i = 1}^n x_i^2 - 2\theta \sum _{i = 1}^n x_i]}}{e^{-\frac{1}{2\sigma^2}[\sum _{i = 1}^n y_i^2 - 2\theta \sum _{i = 1}^n y_i]}}
    \end{align*}

    Which is free of $\theta$ if $\sum _{i = 1}^n x_i^2 = \sum _{i = 1}^n y_i^2$ and $\sum _{i = 1}^n x_i = \sum _{i = 1}^n y_i$

    It follows that $T(\mathbf{X}) = (\sum _{i = 1}^n x_i, \sum _{i = 1}^n x_i^2)$ is minimal sufficient for $\theta$

    Now observe that $T_1(\mathbf{X}) = \sum _{i = 1}^n x_i \sim N(n\theta, n\theta^2)$ and therefore

    \begin{align*}
      E(T_1^2) &= V(T_1) + [E(T_1)]^2\\
      &= n\theta^2 + n^2\theta^2\\
      &= n\theta^2(1 + n)
    \end{align*}

    On the other hand, for $T_2 = \sum _{i = 1}^n x_i^2$,

    \begin{align*}
      E(T_2) &= n E(X_1)^2\\
      &= n[V(X_1) + [E(X_1)]^2]\\
      &= n\theta^2 + n\theta^2\\
      &= 2n\theta^2
    \end{align*}

    Then, taking $h(t_1,t_2) = 2t_1^2 - (n+1)t_2$, we have
    \begin{align*}
      E_\theta[h(T_1,T_2)] &= E_\theta[2T_1^2 - (n+1)T_2]\\
      &= 2E_\theta(T_1^2) - (n+1)E(T_2)\\
      &= 2n(n+1)\theta^2 - 2n(n+1)\theta^2\\
      &= 0 \quad \forall \theta
    \end{align*}

    But because $h(\mathbf{t}) \neq 0 \quad \forall \theta$, we have that $T(\mathbf{X})$ is not complete.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 278 \end{field}
  \begin{field}
    Complete statistics in the exponential family
  \end{field}
  \begin{field}
    Let $X_1, \ldots , X_n$ be iid observations from an exponential family. with PDF or PMF of the form
    $$ f(x|\theta) = h(x)c(\theta)e^{\sum_{j=1}^k \omega_j(\theta_j)t_j(x)}$$

    Where $\boldsymbol\theta = (\theta_1, \ldots , \theta_k)$

    Then, the statistic $T(\mathbf{X}) = (\sum_{i=1}^n t_1(x_i), \sum _{i = 1}^n t_2(x_i), \ldots , \sum _{i = 1}^n t_k(x_i))$ is complete, as long as the parameter space $\Theta$ contains an open set in $\mathbb{R}^k$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 279 \end{field}
  \begin{field}
    Suppose that a statistic $T$ is complete and let $g$ be a one-to-one function. Is the statistic $U = g(T)$ also complete?
  \end{field}
  \begin{field}
    Yes
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 280 \end{field}
  \begin{field}
    Does complete statistic imply minimial sufficient statistic?
  \end{field}
  \begin{field}
    If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 281 \end{field}
  \begin{field}
    Basu's Theorem
  \end{field}
  \begin{field}
    If $T(\mathbf{x})$ is a complete and minimal sufficient statistic, then $T (\mathbf{x})$ is an independent of every ancillary statistic.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 282 \end{field}
  \begin{field}
    Likelihood function
  \end{field}
  \begin{field}
    Let $f(\mathbf{x}|\theta)$ denote the joint pdf or pmf of the sample $\mathbf{X} = (X_1, \ldots , X_n)$, then given that $\mathbf{X} = \mathbf{x}$ is observed, the function of $\theta$ defined as $$L(\theta|\mathbf{x}) = f(\mathbf{x}|\theta)$$
    is called the Likelihood Function
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 283 \end{field}
  \begin{field}
    Idea of likelihood function
  \end{field}
  \begin{field}
    Suppose that $\mathbf{X}$ is a discrete random vector (so we can interpret probabilities easier)

    Then $L(\theta|\mathbf{x}) = P_\theta(\mathbf{X} = \mathbf{x})$. Now if we compare the likelihood function at two parameter values $\theta_1, \theta_2$ and we observe that $$P_{\theta_1}(\mathbf{X} = \mathbf{x}) = L(\theta_1|\mathbf{x}) > L(\theta_2|\mathbf{x}) = P_{\theta_2}(\mathbf{X} = \mathbf{x})$$
    Then, the sample point $\mathbf{x}$ that we actually observed is more likely to have occurred if $\theta = \theta_1$, than if $\theta= \theta_2$, which can be interpreted as that $\theta_1$, is a more plausible value for the true value of $\theta$ than $\theta_2$ is.
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 284 \end{field}
  \begin{field}
    Fisher information - one parameter case
  \end{field}
  \begin{field}
    Let $X$ be a random variable with pdf or pmf $f(x |\theta)$ where $\theta \in \Theta \subseteq \mathbb{R}$

    (Fisher ) information about $\theta$ contained in $X$ is

    $$ I_{X}(\theta) = E_\theta[ \big(\frac{\partial}{\partial \theta} \log f(x|\theta)\big)^2]$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 285 \end{field}
  \begin{field}
    Example of one parameter case Fisher information
    Suppose that $X \sim Bern(p)$ What is the information that $X$ contains about the parameter $p$?
  \end{field}
  \begin{field}
    We have that $f(x|p) = p^x(1-p)^{1-x}$. Then $$ \log f(x|p) = x \log p + (1-x)\log(1-p)$$
    $$ \frac{\partial}{\partial p} \log f(x|p) = \frac{x}{p} - \frac{1-x}{1-p}$$

    We obtain


    \begin{align*}
      \big(\frac{\partial}{\partial p} \log f(x|p) \big)^2 &= \big(\frac{x}{p} - \frac{1-x}{1-p}\big)^2\\
      &= \frac{x^2}{p^2} - \frac{2x(1-x)}{p(1-p)} + \frac{(1-x)^2}{(1-p)^2}\\
      &= \frac{x^2}{p^2} - \frac{2(x-x^2)}{p(1-p)} + \frac{(1-2x+x^2)}{(1-p)^2}
    \end{align*}

    Therefore,

    \begin{align*}
      I_x(p) &= E_p[(\frac{\partial}{\partial p} \log f(x|p))^2]\\
      &= \frac{p}{p^2} - \frac{2(p-p)}{p(1-p)} + \frac{1 - 2p + p}{(1-p)^2}\\
      &= \frac{1}{p} + \frac{1}{1-p}\\
      &= \frac{1}{p(1-p)}
    \end{align*}
  \end{field}
\end{note}




%%end_tag
%%start_tag Wk8

\begin{note} \begin{field} \tiny 286 \end{field}
  \begin{field}
    $$ I_x(\theta) = E_\theta\big[(\frac{\partial}{\partial \theta} \log f(x|\theta))^2\big] = $$

  \end{field}
  \begin{field}
    If $f(x|\theta)$ satisfies

    $$ \frac{\partial}{\partial \theta} E_\theta \big( \frac{\partial}{\partial \theta} \log f(x|\theta)\big) = \int \frac{\partial}{\partial \theta}\big[\frac{\partial}{\partial \theta}\log f(x|\theta)\big]f(x|\theta)dx $$

    $$ I_x(\theta) = E_\theta\big[(\frac{\partial}{\partial \theta} \log f(x|\theta))^2\big] = - E_\theta (\frac{\partial^2}{\partial \theta^2} \log f(x|\theta))$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 287 \end{field}
  \begin{field}
    Suppose that $X_1, \ldots , X_n$ are iid observations with common pdf or pmf $f(x|\theta)$. Then, the information about $\theta$ contained in the sample $\mathbf{X} = (X_1, \ldots , X_n)$ is
  \end{field}
  \begin{field}
    $$I_{\mathbf{X}}(\theta) = n I_{X_1}(\theta)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 288 \end{field}
  \begin{field}
    Fisher Information - multiparameter case
  \end{field}
  \begin{field}
    Let $X$ be a random variable with pdf or pmf $f(x|\boldsymbol\theta)$, where $\boldsymbol\theta = (\theta_1,\theta_2) \in \Theta \subseteq \mathbb{R}^2$. Denote by $$I_{ij}(\boldsymbol\theta) = E_{\boldsymbol\theta}\big[(\frac{\partial}{\partial \theta_i}\log f(x|\boldsymbol\theta))(\frac{\partial}{\partial \theta_j} \log f(x|\boldsymbol\theta))\big] = -E_{\boldsymbol\theta}[\frac{\partial}{\partial \theta_i\theta_j}\log f(x|\boldsymbol\theta)]$$

    For $i,j = 1,2$.
    Then the (fisher) information matrix about $\boldsymbol\theta$ is

    $$ I_x(\boldsymbol\theta) = \begin{pmatrix}
      I_{11}(\boldsymbol\theta) & I_{12}(\boldsymbol\theta)\\
      I_{21}(\boldsymbol\theta) & I_{12}(\boldsymbol\theta)
    \end{pmatrix}$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 289 \end{field}
  \begin{field}
    Find Fisher information matrix for Normal RVs
  \end{field}
  \begin{field}
    We have that $\boldsymbol\theta = (\mu,\sigma^2)$ and $f(x|\boldsymbol\theta) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x - \mu)^2}$
    Then,
    $$ \frac{\partial}{\partial \mu} \log f(x|\boldsymbol\theta) = \frac{\partial}{\partial}[-\frac{1}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}(x - \mu)^2] = \frac{(x-\mu)}{\sigma^2}$$

    $$ \frac{\partial}{\partial\sigma^2} = \frac{1}{2\sigma^2}[ \frac{(x-\mu)^2}{\sigma^2} - 1]$$

    Therefore $I_{11} = E_\theta[(\frac{\partial}{\partial \mu}\log f(x|\boldsymbol\theta))^2] = E_\theta[\frac{(x-\mu)^2}{\sigma^4}] = \frac{1}{\sigma^4}\sigma^2 = \frac{1}{\sigma^2}$

    \begin{align*}
      I_{22}(\boldsymbol\theta) &= E_{\theta}[\frac{\partial}{\partial\sigma^2}\log f(x|\boldsymbol\theta)^2]\\
      &= E_\theta \big\{ [ \frac{1}{2\sigma^2} ( \frac{(x-\mu)^2}{\sigma^2} - 1)]^2\big\}\\
      &= \frac{1}{4\sigma^4}E_\theta[ (\frac{(x-\mu)^2}{\sigma^2} - 1)^2]\\
      &= \frac{1}{4\sigma^4 \cdot 2} \\
      &= \frac{1}{2\sigma^4} \quad \text{Since } = V(\chi^2_1)
    \end{align*}

    Now for the off diagonal elements,

    \begin{align*}
      I_{12}(\boldsymbol\theta) = I_{22}(\boldsymbol\theta) &= E_\theta \big[ ( \frac{\partial{}{}}{\partial{\mu}{}}\log f(x|\theta)(\frac{\partial{}{}}{\partial{\sigma^2}{    }} \log f(x|\theta)))\big]\\
      &=E_\theta \big[ \frac{(x-\mu)}{\sigma^2
      } \frac{1}{2\sigma^2} [\frac{x-\mu}{\sigma^2}\cdot 1]\big]\\
      &= \frac{1}{2\sigma^4} E_\theta[ \frac{(x-\mu)^3}{\sigma^3} - (x-\mu)]
    \end{align*}
    But $E_\theta[(x-\mu)^3] = E_\theta[(x - \mu)] = 0$, because $X$ is symmetric around $\mu$, and we obtain $I_{12}(\boldsymbol\theta) = I_{21}(\boldsymbol\theta) = 0$
    We obtain that
    \begin{align*}
      I_{x_1}(\boldsymbol\theta) &= \begin{pmatrix}
        I_{11}(\boldsymbol\theta) & I_{12}(\boldsymbol\theta)\\
        I_{21}(\boldsymbol\theta) & I_{22}(\boldsymbol\theta)
    \end{pmatrix}\\
    &= \begin{pmatrix}
      \frac{1}{\sigma^2} & 0 \\ 0 & \frac{1}{2\sigma^4}
    \end{pmatrix}
    \end{align*}
    And hence $$ I_{\mathbf{x}}(\boldsymbol\theta) = n I_{X_1}(\boldsymbol\theta) = \begin{pmatrix}
      \frac{n}{\sigma^2} & 0 \\ 0 & \frac{n}{2\sigma^4}
    \end{pmatrix} $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 290 \end{field}
  \begin{field}
    $I_T(\theta) \leq $
  \end{field}
  \begin{field}
    $I_T(\theta) \leq I_{\mathbf{X}}(\theta)$
    (The information of the statistic is less than or equal to the information of the sample)
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 291 \end{field}
  \begin{field}
    Let $\mathbf{X} = X_1, \ldots , X_n$ denote the entire data, and let $T = T(\mathbf{X})$ be some statistic. Then, for all $\theta \in \Theta \subseteq \mathbb{R}$, $I_{\mathbf{X}} (\theta) \geq I_t(\theta)$
    Where the equality is attained...
  \end{field}
  \begin{field}
    Let $\mathbf{X} = X_1, \ldots , X_n$ denote the entire data, and let $T = T(\mathbf{X})$ be some statistic. Then, for all $\theta \in \Theta \subseteq \mathbb{R}$, $I_{\mathbf{X}} (\theta) \geq I_t(\theta)$
    Where the equality is attained if and only iff $T(\mathbf{X})$ is sufficient for $\theta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 292 \end{field}
  \begin{field}
    Let $\mathbf{X} = (X_1, \ldots , X_n)$, denote a sample of iid observations and suppose the statistic $T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))$ is such that $T_1$ and $T_2$ are independent. Then $$ I_{T}(\boldsymbol\theta) = $$
  \end{field}
  \begin{field}
    Let $\mathbf{X} = (X_1, \ldots , X_n)$, denote a sample of iid observations and suppose the statistic $T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))$ is such that $T_1$ and $T_2$ are independent. Then $$ I_{T}(\boldsymbol\theta) = I_{T_1}(\boldsymbol\theta) + I_{T_2}(\boldsymbol\theta)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 293 \end{field}
  \begin{field}
    Point estimator
  \end{field}
  \begin{field}
    Any statistic $T(\mathbf{X})$ that is used to estimate the value of a parameter is called a point estimator of $\theta$. We write $\hat{\theta} = T(\mathbf{X})$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 294 \end{field}
  \begin{field}
    Method of moments
  \end{field}
  \begin{field}
    \begin{align*}
      m_1 &= \frac{1}{n}\sum _{i = 1}^n X_i^1, \quad \mu_1 = E(X^1)\\
      m_2 &= \frac{1}{n}\sum _{i = 1}^n X_i^2, \quad \mu_2 = E(X^2)\\
      \vdots & \\
      m_k &= \frac{1}{n} \sum _{i = 1}^n X_i^k \quad \mu_k = E(X^k)
    \end{align*}
    Equating and solving for $\theta$ gives the MoM estimators
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 295 \end{field}
  \begin{field}
    Example Method of Moments
    Suppose that $X_1, \ldots , X_n$ are iid Binomial($k,p$), where both $k$ and $p$ are unknown.
  \end{field}
  \begin{field}
    We have that
    $$ P(X_i = x|k,p) = \binom{k}{x}p^x(1-p)^{k-x}, x = 0,1,\ldots,k$$
     and we obtain $E(X_1) = kp$, $E(X_1^2) = kp(1-p) + k^2p^2$

    Solving the sytem of equations we obtain

    \begin{align*}
      m_1 &= \frac{1}{n} \sum _{i = 1}^n X_i = kp\\
      m_2 &= \frac{1}{n} \sum _{i = 1}^n X_i^2 = kp(1-p) + k^2p^2
    \end{align*}

    Sovling the system of equations:

    \begin{align*}
      \tilde{p} &= \frac{\bar{x}}{\tilde{k}}\\
      \tilde{k} &= \frac{\bar{x}^2}{\bar{x} - \frac{1}{n} \sum _{i = 1}^n (x_i - \bar{x})^2}
    \end{align*}

    Possible problems: $k$ has to be an integer, and not negative. (Estimates of parameters that are outside of the parameter space. )
  \end{field}
  \begin{field}

  \end{field}
\end{note}


%%end_tag
%%start_tag Wk9

\begin{note} \begin{field} \tiny 296 \end{field}
  \begin{field}
    Maximum Likelihood Estimator
  \end{field}
  \begin{field}
    In this context, we define the \textbf{Maximum Likelihood Estimator (MLE)} of $\theta$ as the parameter value $\hat{\theta}_{ML} = \hat{\theta}(\mathbf{x})$ that satisfies
    $$L(\hat{\theta}_{ML}|\mathbf{x}) = \text{sup}_{\theta \in \Theta} L(\theta|\mathbf{x})$$

    Note this often proceedes as taking the derivative of the log likelihood function and setting to zero to solve for parameters - not always
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 297 \end{field}
  \begin{field}
    Example of MLE
    Suppose that $X_1, \ldots , X_n$ are iid Exponential$(\lambda)$. Find the MLE $\hat{\lambda}_{ML}$ of $\lambda$
  \end{field}
  \begin{field}
    Suppose that $X_1, \ldots , X_n$ are iid Exponential$(\lambda)$. Find the MLE $\hat{\lambda}_{ML}$ of $\lambda$

    We have that $f(x|\lambda) = \frac{1}{\lambda}e^{x/\lambda}$, $x>0$, and therefore

    $$L(\lambda|x) = \prod _{i = 1}^n \frac{1}{\lambda}e^{x_i/\lambda} = \lambda^{-n}e^{-\frac{1}{\lambda}\sum _{i = 1}^n x_i}$$


    Since $\log(\cdot)$ is a strictly monotone (one-to-one) and increasing, we consider instead the maximization of the log-likelihood

    $$l(\lambda|\mathbf{x}) = \log L(\lambda|\mathbf{x}) = -n\log \lambda - \frac{1}{\lambda} \sum _{i = 1}^n x_i$$

    $$\frac{\partial  }{\partial \lambda } l(\lambda | \mathbf{x}) = \frac{-n}{\lambda} + \frac{1}{\lambda^2}\sum _{i = 1}^n x_i$$

    Solving $\frac{\partial  }{\partial \lambda } l(\lambda|\mathbf{x})= 0$, we obtain

    $$ \frac{-n}{\lambda} + \frac{1}{\lambda^2}\sum _{i = 1}^n x_i = 0$$

    $$ -n\lambda + n \bar{x} = 0$$
    $$\lambda = \bar{x}$$

  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 298 \end{field}
  \begin{field}
    Example of MLE when can't differemtiate

    Suppose that $X_1, \ldots , X_n$ are iid Uniform($0,\theta$), $\theta > 0$. Find the MLE of $\theta$
  \end{field}
  \begin{field}
    We have that $f(x|\theta) = \frac{1}{\theta}I(0<x<\theta)$

    And therefore
    \begin{align*}
      L(\theta|\mathbf{x}) &= \prod _{i = 1}^n \frac{1}{\theta} I(0 < x_i < \theta)\\
      &= \frac{1}{\theta^n} I(X_{(1)} > 0)I(X_{(n)}< \theta)
    \end{align*}

    In this case, the support of $X$ depends on $\theta$ and the maximization problem only makes sense whenever $L(\theta|\mathbf{x}) > 0$. We cannot simply approach the problem by taking partial derivatives, but assuming the likelihood is positive, we notice that $L(\theta|\mathbf{x})$ is decerasing as a function of $\theta$, for $\theta > X_{(n)}$

    Picture with $L(\theta)$ as zero untill $X_{(n)}$ on x axis, goes up to $1/X_{(n)}$ there and decreases with $\frac{1}{\theta^n}$

    It follows the MLE of $\theta$ is $\hat{\theta}_{ML} = X_{(n)}$

  \end{field}
\end{note}





%%end_tag


\begin{note} \begin{field} \tiny 299 \end{field}
  \begin{field}
    If $\hat{\theta}_{ML}$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\eta =\tau(\theta)$ is $\hat{\eta}_{ML} =$
  \end{field}
  \begin{field}
    If $\hat{\theta}_{ML}$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\eta =\tau(\theta)$ is $\hat{\eta}_{ML} = \tau(\hat{\theta}_{ML})$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 300 \end{field}
  \begin{field}
    Bias
  \end{field}
  \begin{field}
    Let $\hat{\theta} = T(\mathbf{X})$ be an estimatro of $\theta$. Then the Bias of $\hat{\theta}$ as an estimator of $\theta$ is defined as
    $$B_{\theta}(\hat{\theta}) = E_\theta(\hat{\theta} - \theta) = E_\theta(\hat{\theta}) - \theta$$

    That is the difference between the expected value of $\hat{\theta}$ and $\theta$.

    An estimator $\hat{\theta}$ of $\theta$ is said to be unbiased if $B_\theta(\hat{\theta}) = 0 \quad \forall \theta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 301 \end{field}
  \begin{field}
    Mean Squared Error
  \end{field}
  \begin{field}
    Let $\hat{\theta} = T(\mathbf{X})$ be an estimate of $\theta$. Then, the \textbf{Mean Squared Error} (MSE) of $\hat{\theta}$ as an estimator of $\theta$ is defined as:

    $$ MSE (\hat{\theta}) = E_{\theta}[(\hat{\theta} - \theta)^2] = V_\theta(\hat{\theta}) + [B_\theta(\hat{\theta})]^2$$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 302 \end{field}
  \begin{field}
    Do unbiased estimators always exist?
  \end{field}
  \begin{field}
    No, Suppose that $X \sim $ Binomial($n,p$) and let $\theta = 1/p$ be the parameter of interest. Can we find an unbiased estimator for $\theta$?- No
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 303 \end{field}
  \begin{field}
    UMVUE
  \end{field}
  \begin{field}
    An estimator $W^*$ is called a best unbiased estimator of $\tau(\theta)$ if it satisfies $E_\theta(W^*) = \tau(\theta)$, for all $\theta$, and for any other estimator $W$ with $E_\theta(W) = \tau(\theta)$, we have $V_\theta(W^*)\leq V_\theta(W), \forall \theta$. Equivalently $W^*$ is also called a \textbf{Uniform Minimal Variance Unbiased Estimator} (UMVUE) of $\tau(\theta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 304 \end{field}
  \begin{field}
    Finding a UMVUE
  \end{field}
  \begin{field}
    Start with a complete statistic, (find min suff statistic, prove completeness), Find bias (ie $E(T(\mathbf{X}))$). Then adjust $T(\mathbf{X})$ to be unbiased. (ie center or scale )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 305 \end{field}
  \begin{field}
    Cramer-Rao Inequality
  \end{field}
  \begin{field}
    Let $X_1, \ldots , X_n$ be a sample with joint pdf or pmf $f(\mathbf{x}|\theta)$ and let $W(\mathbf{X}) = W(X_1, \ldots , X_n)$ be any estimator satisfying
    $$\frac{d}{d\theta} E_\theta(W(X)) = \int \frac{d}{d\theta} [W(\mathbf{X})f(\mathbf{x}|\theta)] d \mathbf{x}$$

    and $V_\theta(W(\mathbf{X}))< \infty$

    Then,
    $$V_\theta(W(\mathbf{X})) \geq \frac{(\frac{d}{d\theta}E_\theta(W(\mathbf{X})))^2}{E_\theta[( \frac{\partial}{\partial \theta}\log f(\mathbf{x}|\theta))^2]}$$

    Observe that if the sample $X_1, \ldots , X_n$ is iid with common pdf or pmf $f(x|\theta)$, we obtain

    $$ V_\theta (W(\mathbf{X})) \geq \frac{[\frac{d}{d\theta}E_\theta(W(\mathbf{X}))]^2}{nE_\theta[(\log f(\mathbf{x}|\theta))^2]}$$

    The denominator is the information in the sample about $\theta$

    We have that as the information number gets bigger we have a smaller bound for the variance. of the best unbiased estimator and therefore more information is available.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 306 \end{field}
  \begin{field}
    Cramer-Rao and UMVUE example
    UMVUE of $\lambda$ for Poisson
  \end{field}
  \begin{field}
    Poisson example, we have $\tau(\lambda) = \lambda$, so $\frac{d}{d\lambda}\tau(\lambda) = 1$

    On the other hand,

    \begin{align*}
      nE_\lambda[(\frac{d}{d\lambda}\log f(x|\lambda))^2] &= -n E_\lambda(\frac{\partial ^2 }{\partial \lambda ^2}) \log f(x|\lambda))\\
      &= -n E_\lambda(\frac{\partial ^2 }{\partial \lambda ^2} \log (\frac{e^{-\lambda}\lambda^x}{x!}))\\
      &= -n E_\lambda[\frac{\partial ^2 }{\partial \lambda ^2}(-\lambda + x \log \lambda - \log(x!))]\\
      &= -n E_\lambda(\frac{-x}{\lambda^2})\\
      &= \frac{n}{\lambda}
    \end{align*}

    Therefore, for any unbiased estimator $W$ of $\lambda$, we must have $V_\lambda(W) \geq \lambda /n$. Since $V_\lambda(\bar{X}) = \frac{\lambda}{n}$, we have that $\bar{X}$ is an UMVUE of $\lambda$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 307 \end{field}
  \begin{field}
    Does $S^2$ for Normal attain cramer rao?
  \end{field}
  \begin{field}
    No -
    Suppose that $X_1, \ldots , X_n$ are iid N($\mu,\sigma^2$)
    and consider the estimation of $\sigma^2$ when $\mu$ is unknown.

    We have that

    $$ \frac{\partial ^2 }{\partial (\sigma^2) ^2} \log [ \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}] = \frac{1}{2\sigma^4} - \frac{(x-\mu)^2}{\sigma^6}$$

    and
    \begin{align*}
      -E[\frac{\partial ^2 }{\partial (\sigma^2)^2 } \log f(x|\mu,\sigma^2)] &= -E(\frac{1}{2\sigma^4} - \frac{(x-\mu)^2}{\sigma^6})\\
      &= -\frac{1}{2\sigma^4} + \frac{\sigma^2}{\sigma^6}\\
      &= \frac{1}{2\sigma^4}
    \end{align*}

    and therefore, any unbiased estimator $W$ of $\sigma^2$ must satisfy $V(W) \geq \frac{2\sigma^4}{n}$. Recall that for $S^2$ we have $$V(S^2) = \frac{2\sigma^4}{n-1} > \frac{2\sigma^4}{n}$$

    and therefore $S^2$ does not attain the cramer-rao lower bound.

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 308 \end{field}
  \begin{field}
    Rao-Blackwell
  \end{field}
  \begin{field}
    Let $W$ be any unbiased estimator $\tau(\theta)$ and let $T$ be a sufficient statistic for $\theta$. Define $\phi(T) = E(W|T)$. Then $E_\theta(\phi(T)) = \tau(\theta)$ and $V_\theta(\phi(T))\leq V_\theta(W)$, for all $\theta$
    That is, $\phi(T)$ is a uniformly better unbiased estimator of $\tau(\theta)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 309 \end{field}
  \begin{field}
    Use of Rao-Blackwell
  \end{field}
  \begin{field}
    Estimators can be improved (their MSE) using sufficiency (already sufficient statistics, or functions of sufficient statistics cannot be improved)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 310 \end{field}
  \begin{field}
    Are unbiased estimators based on complete sufficient statistics unique.
  \end{field}
  \begin{field}
    Unbiased estimators based on complete sufficient statistics are unique.
  \end{field}
\end{note}


%%end_tag
%%start_tag Theory 3
\tags{Theory3}

\begin{note}
    \begin{field}
        \tiny 311
    \end{field}
    \begin{field}
        Data summaries vs Prediciton vs Inference
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Data summaries: descriptive statistics summarizing a dataset (ie sample mean)
          \item Prediction: Use patterns in a data-set to make predictions regarding values of new observations
          \begin{itemize}
            \item Prediction setting is more flexible than inference setting, as we are not trying to make probabilistic inference, assumptions only matter if they affect prediction quality.
          \end{itemize}
          \item Inference: Use observations in data set to infer information concerning population parameters
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 312
    \end{field}
    \begin{field}
        Parametric Inference
    \end{field}
    \begin{field}
        Inference (estimation and/or hypothesis testing performed under the assumption taht the data come from a popluation distribution that belongs to some family of distributions $F(x;\theta)$) parametrized by a finite-dimensional parameter $\theta$


        Parameter space: The set $\Theta$ of all possible values of the parameter $\theta$

        Vs Nonparametric Inference - where no or limited assumptions or specifications of the form of the population distributions
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 313
    \end{field}
    \begin{field}
        Are the following tests parametric, semiparametric, or nonparametric
        \begin{itemize}
          \item F-test
          \item Exact binomial test
          \item Fisher's exact test
          \item t-test
          \item Wilcoxon rank sum
          \item Permutation tests
          \item Sign test
          \item Mood's test
          \item KS test
          \item t-test
        \end{itemize}
    \end{field}
    \begin{field}
      \begin{itemize}
        \item F-test - Parametric
        \item Exact binomial test
        \item Fisher's exact test
        \item t-test
        \item Wilcoxon rank sum: semiparametric
        \item Permutation tests
        \item Sign test: nonparametric
        \item Mood's test
        \item KS test
        \item t-test
      \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 314
    \end{field}
    \begin{field}
        Definition: Simple hypothesis, composite hypothesis
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Simple hypothesis: Completely specifies the parameter value and therefore the population distribution. Simple hypothesis have the form $H_0: \theta = \theta_0$ and $H_1: \theta = \theta_1$, for specified values of $\theta_0$ and $\theta_1$
          \item Composite hypothesis: Includes more than one possible parameter value. Composite hypotheses have the form $H_0: \theta \in \Theta_0$ and $H_1: \theta \in \Theta_1$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 315
    \end{field}
    \begin{field}
        Test procedure:
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Random Sample (data): $X_1, \cdots, X_n$
          \item Sample Space $\mathscr{X}$ the set of all possible observed samples $X_1 = x_1, X_2 = x_2, \cdots, X_n = x_n$
          \item Hypothesis $H_0: \theta \in \Theta_0$ and $H_1: \theta \in \Theta_1$ with $\Theta_0 \cap \Theta_1 = \emptyset$
          \item Rejection Region $\mathscr{R} \subset \mathscr{X}$:
          \begin{itemize}
            \item If ($X_1, \ldots , X_n \in \mathscr{R}$, Reject $H_0$ )
            \item If $X_1, \ldots , X_n \not\in \mathscr{R}$, Fail to reject $H_0$
          \end{itemize}
        \end{itemize}

        Equivalently

        \begin{itemize}
          \item Random Sample (data): $X_1, \cdots, X_n$
          \item Test statistic $ T(X_1, \cdots, X_n)$ is some function of the data, which is itself a random variable
          \item Test Statistic Sample Space $\mathscr{T}$ the set of all possible observed samples $T = t$
          \item Rejection Region $\mathscr{R}_T \subset \mathscr{T}$:
          \begin{itemize}
            \item If $T(X_1, \ldots , X_n) \in \mathscr{R}_t$, Reject $H_0$ )
            \item If $T(X_1, \ldots , X_n) \not\in \mathscr{R}_t$, Fail to reject $H_0$
          \end{itemize}
        \end{itemize}
    \end{field}
\end{note}




\begin{note}
    \begin{field}
        \tiny 316
    \end{field}
    \begin{field}
        Power function (definition)
    \end{field}
    \begin{field}
        We can summarize the performance of a test procedure through the power function:
        \begin{align*}
          \text{Power}(\theta) = \beta(\theta) &= P_\theta(\text{Reject } H_0 \text{ when } \theta \text{is the true value of the parameter of interest})\\
          &= P_\theta((X_1, \ldots X_n) \in \mathscr{R}) \\
          &= P_\theta(T(X_1, \ldots, X_n) \in \mathscr{R}_t)
        \end{align*}

        Equivalently, for a critical function $\psi$, $$\beta(\theta) = E_\theta(\psi(x_1, \ldots, x_n)) $$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 317
    \end{field}
    \begin{field}
        Calculating Type I and Type II errors from the power function
    \end{field}
    \begin{field}
        $$P( \text{Type I Error when } \theta = \theta_0 \in \Theta_0) = \beta(\theta_0) $$
        $$P(\text{Type II Error when } \theta = \theta_1 \in \Theta_1)  = 1 - \beta(\theta_1)$$

        (Note these are for simple hypotheses), for complex hypothesis, we want to look a the maximum possible error

        To work out these probabilities, we need to know the distribution of the test statistics under the null (For type I error) and alternative (For type II error )
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 318
    \end{field}
    \begin{field}
        Size of a test procedure
    \end{field}
    \begin{field}
        The size of a test procedure for a null hypothesis $H_0: \theta \in \Theta_0$ is the value $$ sup_{\theta \in \Theta_0}P_{\theta}(\text{Reject } H_0) = sup_{\theta \in \Theta_0} \beta(\theta)$$
        That is, the size of a test procedure is the largest value of the probability of a Type I Error, across all values of $\theta$ in the null hypothesis set $\Theta_0$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 319
    \end{field}
    \begin{field}
        Definition of a level $\alpha$ test
    \end{field}
    \begin{field}
        A hypothesis test procedure is said to be a level $\alpha$ test if
        $$ sup_{\theta \in \Theta_0}P_{\theta}(\text{Reject } H_0) = sup_{\theta \in \Theta_0} \beta(\theta) \leq \alpha$$

        That is if the size of the test is less than or equal to $\alpha$, the test is a level $\alpha$ test.
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 320
    \end{field}
    \begin{field}
        most powerful level $\alpha$ test (definition)
    \end{field}
    \begin{field}
        Let $\mathscr{C}_\alpha$ be the set of all tests of $H_0: \theta \in \Theta_0$ vs $H_1: \theta \in \Theta_1$ where $\Theta_0 \cap \Theta_1 = \emptyset$ that have level $\alpha$. A test belonging to $\mathscr{C}_\alpha$ is the most powerful level $\alpha$ test at $\theta_1 \in \Theta_1$ if
        $$\beta(\theta_1) \geq \beta^*(\theta_1)$$
        for any other test in $\mathscr{C}_\alpha$ with power function $\beta^*(\theta)$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 321
    \end{field}
    \begin{field}
        Uniformly most powerful level $\alpha$ test (definition )
    \end{field}
    \begin{field}
        A test belonging to $\mathscr{C}_\alpha$ with power function $\beta(\theta)$ is uniformly most powerful level $\alpha$ if it is the most powerful for every $\theta_1 \in \Theta_1$

    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 322
    \end{field}
    \begin{field}
        Critical Function / Test Function (definition )
    \end{field}
    \begin{field}
        A function $\psi: \mathscr{X} \to [0,1] $ such that $\psi(x_1, \ldots x_n)$ is the probability of rejecting $H_0$ when the sample $(X_1 = x_1, \ldots X_n = x_n)$ is observed is called a critical function of a test procedure.
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 323
    \end{field}
    \begin{field}
        Randomized Test (definition)
    \end{field}
    \begin{field}
        A test procedure with critical function $\psi$ for which there are some points in the sample space such that $0 < \psi < 1$ is called a randomized test

        (often used in discrete cases)
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 324
    \end{field}
    \begin{field}
        Finding the most powerful level $\alpha$ test of a simple null hypothesis vs a simple alternative hypothesis
    \end{field}
    \begin{field}
        $(Neyman-Pearson)$
        The most powerful level $\alpha$ test of a simple null hypothesis $H_0$ vs a simple alternative hypothesis $H_1$ based on data $\mathbf{X}$ is given by the critical function

          \[
          \psi(\mathbf{X}) =
          \begin{cases}
              1 & \text{if } \frac{L(H_0:x)}{L(H_1:x)} < k  \\
              c & \text{if } \frac{L(H_0:x)}{L(H_1:x)} = k\\
              0 & \text{if } \frac{L(H_0:x)}{L(H_1:x)} > k
          \end{cases}
        \]


         Where the constants $k$ and $c$ are chosen to ensure that $E_{H_0}(\phi(\mathbf{X})) = \alpha$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 325
    \end{field}
    \begin{field}
        Steps for using Neyman-Pearson to obtain most powerful tests for simple and alternative hypotheses:
    \end{field}
    \begin{field}
        \begin{enumerate}
          \item Identify the likelihood under the null $L(H_0:x)$ and alternative $L(H_0: x)$
          \item Determine how the ratio of the likelihoods $ \frac{L(H_0:x)}{L(H_1:x)}$ depends on the observed data $\mathbf{x}$ (ie is it an increasing or decreasing function of $T(\mathbf{X})$)?
          \item Identify the null distribution of the statistic $T(\mathbf{X})$
          \begin{enumerate}
            \item If $\frac{L(H_0:x)}{L(H_1:x)}$ is an increasing function of $T(\mathbf{x})$, rejecting for small values of $\frac{L(H_0:x)}{L(H_1:x)}$ is equivalent to rejecting for small values of $T(\mathbf{x})$, so find $k$ such that
            $$P_{H_0}(T(\mathbf{x}) < k) = \alpha $$
            \item If $\frac{L(H_0:x)}{L(H_1:x)}$ is a decreasing  function of $T(\mathbf{x})$, rejecting for large values of $\frac{L(H_0:x)}{L(H_1:x)}$ is equivalent to rejecting for large values of $T(\mathbf{x})$, so find $k$ such that
            $$P_{H_0}(T(\mathbf{x}) > k) = \alpha $$
          \end{enumerate}
        \end{enumerate}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 326
    \end{field}
    \begin{field}
        Uniformly most powerful (UMP) level $\alpha$ test procedure
    \end{field}
    \begin{field}
        Uniformly most powerful (UMP) level $\alpha$ test procedure for testing $H_0: \theta \in \Theta_0$ vs $H_1: \theta \in \Theta_1$ is one with power funciton $\beta(\theta)$ such that for every $\theta_1 \in \Theta_1$ we have
        $$ \beta(\theta) \geq \beta^*(\theta)$$
        for any other level $\alpha$ test prodedure with power function $\beta^*(\theta)$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 327
    \end{field}
    \begin{field}
        Monotone likelihood ratio
    \end{field}
    \begin{field}
        The family of distributions $\{F(x|\theta)\}$ indexed by parmeter $\theta \in \Theta$ has monotone likelihood ratio if there is a statistic $T(\mathbf{X})$ such that for all $\theta^* > \theta \in \Theta$ and $\mathbf{x} \in \mathscr{X}$, the likelihood ratio $$ \frac{L(\theta^*|\mathbf{x})}{L(\theta|\mathbf{x})} \quad \text{ is monotone nondecreasing in } T(\mathbf{x})$$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 328
    \end{field}
    \begin{field}
        How to find the UMP test of a simple null hypothesis vs a one sided complex alternative
    \end{field}
    \begin{field}
        See if the family has monotone likelihood ratio in $T(\mathbf{x})$
        UMP tests of one sided alternative hypothesis exist and are given by the form in Neyman-Pearson (by Karlin Rubin )
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 329
    \end{field}
    \begin{field}
        Karlin-Rubin Theorem
    \end{field}
    \begin{field}
        Suppose the family of distributions $\{F(x|\theta)\}$ indexed by parameter $\theta \in \Theta$ has monotone likelihood ratio Then to test $$ H_0: \theta = \theta_0 \quad \text{vs} \quad H_1: \theta > \theta_0$$
         the test function
         \[
           \phi(\mathbf{X}) =
           \begin{cases}
                1 & if T(\mathbf{X}) > k  \\
                \gamma & if T(\mathbf{X}) = k\\
                0 & if T(\mathbf{X}) < k
           \end{cases}
         \]
         Where $k$ and $\gamma$ are chosen so that $E_{\theta_0}(\phi(\mathbf{X})) = \alpha$ gives a uniformly most powerful (UMP) level $\alpha$ test.

         (note if we have a one sided lower alternative, we flip the direction of the inequalities )
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 330
    \end{field}
    \begin{field}
        Is there a UMP two sided test for $X_1, \ldots , X_n$ iid Exp($\lambda$) where $H_0: \lambda = 2 $ vs $H_1 \lambda \neq 2$?
    \end{field}
    \begin{field}
        No:
        For $\lambda_1 > \lambda_0 = 2$,the UMP test would have the form
        \[
          \psi(\mathbf{X}) =
          \begin{cases}
              1 &  \text{if } T(\mathbf{x}) = \sum_{i=1}^n x_i > k_1\\
              0  & \text{if } T(\mathbf{x}) = \sum_{i=1}^n x_i <k_1
          \end{cases}
        \]

        and for $\lambda_1 < \lambda_0 = 2$, the UMP test would have the form
        \[
          \psi(\mathbf{X}) =
          \begin{cases}
              1 &  \text{if } T(\mathbf{x}) = \sum_{i=1}^n x_i < k_2\\
              0  & \text{if } T(\mathbf{x}) = \sum_{i=1}^n x_i > k_2
          \end{cases}
        \]


        Since these forms are not the same, there is no UMP test.
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 331
    \end{field}
    \begin{field}
        Let $X\sim$ Unif($0,\theta$). Is there a UMP test for testing two sided $H_0: \theta = 1$ vs $H_1: \theta \neq 1$
    \end{field}
    \begin{field}
        Yes:  \[
          \psi(\mathbf{x}) =
          \begin{cases}
               1 & x < \alpha \text{ or } x > 1 \\
               0 & \alpha < x < 1
          \end{cases}
        \]

    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 332
    \end{field}
    \begin{field}
        Unbiased test (definition)
    \end{field}
    \begin{field}
        A test of $H_0: \theta \in \Theta_0$ vs $H_1: \theta \in \Theta_1$ is called unbiased if $\beta(\theta_1) \geq \beta(\theta_0)$ for all $\theta_1 \in \Theta_1$ and  all $\theta_0 \in \Theta_0$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 333
    \end{field}
    \begin{field}
        Uniformly most powerful unbiased (UMPU) level $\alpha$ test (definition )
    \end{field}
    \begin{field}
        A level $\alpha$ test of $H_0: \theta \in \Theta_0$ vs $H_1: \theta \in \Theta_1$ with critical function $\psi(\mathbf{x})$ is called uniformly most powerful unbiased (UMPU) if it is unbiased level $\alpha$ and for any other unbiased test with critical function $\psi^*(\mathbf{x})$, we have $$ E_\theta(\psi(\mathbf{x})) \geq E_\theta(\psi^*(\mathbf{x})) \quad \text{for all } \theta \in \Theta_1$$
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 334
    \end{field}
    \begin{field}
        Likelihood Ratio Test (definition)
    \end{field}
    \begin{field}
        Suppose we have the data $\mathbf{X} = X_1, \ldots , X_n$, with joint density $f(x;\theta)$ for some parameter $\theta \in \Theta$, and we wish to perform a level $\alpha$ test of $H_0: \theta \in \Theta_0$ vs $H_1: \theta \in \Theta_1$, where $\Theta_1 \cup \Theta_0 = \Theta$. The likelihood ratio test statistic is given by $$\lambda(\mathbf{x}) = \frac{sup_{\theta\in \Theta_0}L(\theta|x)}{sup_{\theta\in \Theta}L(\theta|x)} = \frac{L(\hat{\theta}_{0,MLE};x)}{L(\hat{\theta}_{MLE};x)}$$
        and the null hypothesis is rejected for small values of $\lambda$ (indicating that the null hypothesis is relatively 'unlikely')

        We maximize by finding $\theta = \hat{\theta}_{MLE}$ and $\hat{\theta}_{0,MLE}$


    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 335
    \end{field}
    \begin{field}
        If $T(\mathbf{X})$ is a sufficient statistic for $\theta$, then $\lambda(\mathbf{x})$ (the LRT statistic)...
    \end{field}
    \begin{field}
        If $T(\mathbf{X})$ is a sufficient statistic for $\theta$, then $\lambda(\mathbf{x})$ will be a function of $T(\mathbf{x})$. In particular $\lambda(\mathbf{x})$ will be a function of the minimal sufficient statistic
    \end{field}
\end{note}

%Lecture 7:

\begin{note}
    \begin{field}
        \tiny 336
    \end{field}
    \begin{field}
        Frequentist Probability vs Bayesian probability  (definition)
    \end{field}
    \begin{field}
      \begin{itemize}
        \item Frequentist: For an event $E$, in our outcome space, $P(E)$ is the long run proportion of experiments that have outcome $E$, the relative frequency with which an event happens is its probability
        \item Bayesian: For an event $E$ in the outcome space, $P(E)$ is any number between zero and one that you want to assign it, as long as you are coherent about the rules of additivity etc.
      \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 337
    \end{field}
    \begin{field}
        Treatment of population parameters, frequentist vs bayesian
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Frequentist: A population parameter $\theta$ is some fixed (though generally unknown value) that belongs to some set of possible values $\Theta$
          \item Bayesian: A population parameter $\theta$ is a random quantity that has a prior distribution
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 338
    \end{field}
    \begin{field}
        Likelihood function (bayesian )
    \end{field}
    \begin{field}
        Given some value of the parameter $\theta$, the distribution of the data $\mathbf{x}$ is $f(\mathbf{x};\theta)$ is the likelihood (a function of both the value $\theta$ and the data $\mathbf{x}$).
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 339
    \end{field}
    \begin{field}
        Posterior Distribution (definition)
    \end{field}
    \begin{field}
        The posterior distribution of theta given the observed data $\mathbf{x}$ is $$k(\theta;\mathbf{x}) = \frac{f(\mathbf{x};\theta)h(\theta)}{\int_\theta f(\mathbf{x};\theta)h(\theta)d\theta} $$

        Posterior probability $\propto$ Likelihood $\times$ prior probability

        Note that the posterior distribution is proportional to the numerator.
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 340
    \end{field}
    \begin{field}
        Conjugate Priors
    \end{field}
    \begin{field}
        If the prior $h(\theta)$ belongs to some (parametric) family of distributions $\mathscr{P}$ and the likelihood $L(\theta;\mathbf{x})$ (the joint density of the data for any particular value of $\theta$) is such that the posterior $k(\theta;\mathbf{x})$ belongs to the same family $\mathscr{P}$, then this family of priors is said to be conjugate for the likelihood $L(\theta;\mathbf{x})$ (ie the posterior family is the prior family if we choose a conjugate prior. )
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 341
    \end{field}
    \begin{field}
        Non-informative Priors
    \end{field}
    \begin{field}
        A non-informative prior is intended to give as little information as possible about the value of the parameter of interest $\theta$.
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 342
    \end{field}
    \begin{field}
        Improper prior
    \end{field}
    \begin{field}
        an improper prior is a prior that does not integrate to one
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 343
    \end{field}©
    \begin{field}
        Bayes Estimator
    \end{field}
    \begin{field}
        A Bayes estimator (with respect to the particular prior/likelihood) is the estimator that minimizes the Bayesian Risk
        $$\delta^* = arg inf_{\delta \in D} \int_\Theta R(\theta,\delta)h(\theta)d\theta $$
        Where D is the set of all possible estimators for $\theta$

        The Bayes Estimator equivalently minimizes the posterior risk, given the observed data.

        For squared-error loss, the Bayes estimate is the mean of the posterior distribution $k\theta(\mathbf{x})$:

        $$\delta^* = \int_\Theta \theta k(\theta|\mathbf{x})d\theta $$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 344
    \end{field}
    \begin{field}
        Maximum A Posteriori (MAP)
    \end{field}
    \begin{field}
        A MAP test selects the hypothesis $H_0$ or $H_1$ that has the highest posterior probability. (Bayesian. )
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 345
    \end{field}
    \begin{field}
        Definition of a p-value
    \end{field}
    \begin{field}
        \begin{itemize}
          \item For testing null hypothesis $H_0$ vs alternative hypothesis $H_1$, the p-value $p(\mathbf{x})$ corresponding to the observed data, is the smallest value $\alpha$ for which $H_0$ would be rejected by a size $\alpha$ test
          \item Let $W(\mathbf{X})$ be a test statistic such that large values of $W$ are evidence that $H_1$ is true, and therefore the null hypothesis $H_0$ is rejected for large $W(\mathbf{X})$ then a p-value can be defined as $$p(\mathbf{x}) = sup_{\theta \in \Theta}P_\theta(W(\mathbf{X})\geq W(\mathbf{x}))$$ This says that the p-value is the (largest in the null space) probability of obtaining a test statistic at least as extreme as the observed test statistic value.
          \item A p-value is just a function of the observed data; a test statistic
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 346
    \end{field}
    \begin{field}
        Validity of p-value
    \end{field}
    \begin{field}
        A p-value is valid (exact) if for every $\theta \in \Theta_0$ and every value of $\alpha \in [0,1]$, we have
        $$P_\theta(p(\mathbf{X})\leq \alpha )\leq \alpha  $$
    \end{field}
\end{note}



\begin{note}
    \begin{field}
        \tiny 347
    \end{field}
    \begin{field}
        Confidence interval (definition)
    \end{field}
    \begin{field}
        Suppose we have data $\mathbf{X}$ such that the (joint) density of our data give information about an unknown parameter $\theta$. Then a $(1-\alpha)100$ confidence interval for $\theta$ is a random interval $[L(\mathbf{X}),U(\mathbf{X})]$ such that $$ inf_{\theta\in\Theta}P_\theta(L(\mathbf{X})\leq \theta \leq U(\mathbf{X})) = 1 - \alpha $$

        It is important to note that it is the limits of the interval $L(\mathbf{X}), U(\mathbf{X})$ that are the random quantities here.
    \end{field}
\end{note}




\begin{note}
    \begin{field}
        \tiny 348
    \end{field}
    \begin{field}
        Construct a CI using a hypothesis test
    \end{field}
    \begin{field}
        A level $(1-\alpha)100$ confidence interval can be constructed by inverting a level $\alpha$ hypothesis test. This fact is known as the duality of confidence intervals and hypothesis testing.
        The confidence region $\mathscr{C}$ $\mathscr{C} = \{\theta_0: H_0: \theta = \theta_0 \text{ would not be rejected at level } \alpha \}$

        (ie solve for $\theta_0$ to be in the center)
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 349
    \end{field}
    \begin{field}
        Pivot
    \end{field}
    \begin{field}
        Suppose $X$ comes from some parametric family $F(\mathbf{x}:\theta)$ indexed by parameter $\theta$. A pivot, or pivotal quantity is a random variable $U = g(\mathbf{X},\theta)$ that depends upon both the sample $\mathbf{X}$ and the unknown parameter $\theta$ for which the distribution of $U$ does not depend on $\theta$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 350
    \end{field}
    \begin{field}
        Finding a confidence interval for $\theta$ using the pivotal method
    \end{field}
    \begin{field}
        \begin{enumerate}
          \item Identify a pivotal quantity $U$ and its distribution $F_U(u)$
          \item Find $a$ and $b$ such that $$ P(a < U < b)= 1 - \alpha$$
          Let $F_U(u)$ denote the cdf of the pivot $U$, so then we can set \begin{align*}
            a &= F_U^{-1}(c \alpha)\\
            b &= F_U^{-1}(1 - (1 - c)\alpha)
        \end{align*}
        For any $c \in [0,1]$ (usually .5 to split up area on the tails evenly)
        \item Solve the inequality $a < U < b$ for $\theta$ in the middle.
        \end{enumerate}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 351
    \end{field}
    \begin{field}
        Pivotal CI example: Let $Y \sim exp(\theta)$.
    \end{field}
    \begin{field}
        \begin{enumerate}
          \item Let $U = Y/\theta$, so $U \sim Exp(1)$. which doesn't depend on $\theta$, so $U$ is a pivotal quantity.
          \item We must find $a$,$b$, such that $P(a \leq U \leq b) = 1- \alpha$.
          We then solve $P(U \leq a) = \alpha/2$ and $P(b \leq U) = \alpha/2$
          Solve for $\theta$

          $$ P(a \leq U \leq b) = P(Y/b \leq \theta \leq Y/a) $$
        \end{enumerate}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 352
    \end{field}
    \begin{field}
        Finding a pivotal quantity
    \end{field}
    \begin{field}
        \begin{itemize}
          \item If $\theta$ is a location parameter, a possible pivot has the form $U = T(\mathbf{X}) - a (\theta)$
          \item If $\theta$ is a scale parameter, $U = T(\mathbf{X})/b(\theta)$ is a possible pivot
          \item If $\theta$ is a location-scale parameter, $U = (T(\mathbf{X}) - a(\theta))/b(\theta)$ is a possible pivot
          \item If neither, use how parameter is related to $X$, ie if $F_Y(y) = y^N$, use $y^N$ as the pivot.
        \end{itemize}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 353
    \end{field}
    \begin{field}
        Confidence Interval optimality criteria
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Length
          \begin{itemize}
            \item Length can be a function of sample size and critical value choice (Normal mean with known variance), other cases a random quantity (Normal mean with unknown variance)
            \item When length is random, we typically want confidence intervals with shortest mean length.
            \item Ie for normal variance with known mean, the expected length of the confidence interval is $E(L) = c \sigma^2 n$, so a shortest interval would be not using equal tails, but requires numerical computation.
          \end{itemize}
          \item Convexity
          \begin{itemize}
            \item In the case of a one-dimensional parameter, this means that the region should be a connected interval
          \end{itemize}
          \item Agreement with a reasonable estimate and with a reasonable hypothesis test.
          \begin{itemize}
            \item Example score test and wald CI
          \end{itemize}
          \item Equal coverage probability for all $\theta$
        \end{itemize}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 354
    \end{field}
    \begin{field}
        $1 - \alpha $ credible interval
    \end{field}
    \begin{field}
        (Bayesian) A $1 - \alpha $ credible interval is an interval $[a,b]$ such that the credible probability of that interval is $1 - \alpha$, that is $$\int_{\theta=a}^b k(\theta|\mathbf{x})d\theta = 1 - \alpha $$

        Where the posterior distribution $k(\theta|\mathbf{x}) = \frac{f(\mathbf{x};\theta)h(\theta)}{\int_\theta f(\mathbf{x};\theta)h(\theta)} = \frac{p(\mathbf{x}|\theta)p(\theta)}{p(\mathbf{x})} $, where the posterior is proportional to the likelihood times the prior

        In many cases, we can choose $c=.5$ and let $a = $ the $c \times \alpha$ quantile of the distribution, and $b = $ the $1 - (1-c)\times \alpha$ quantile of the posterior distribution, but in cases when the posterior distribution is unimodal, we can obtain shorter intervals.

    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 355
    \end{field}
    \begin{field}
        Credible probability
    \end{field}
    \begin{field}
        (Bayesian ) For any region $\mathscr{A} \subset \Theta$, the credible probability of the set $\mathscr{A }$ is
        $$P(\theta \in \mathscr{A }| \mathbf{x}) = \int_{\theta \in \mathscr{A }} k(\theta| \mathbf{x})d\theta $$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 356
    \end{field}
    \begin{field}
        Highest posterior density $1 - \alpha$ credible interval
    \end{field}
    \begin{field}
        For a unimodal posterior distribution $k(\theta|\mathbf{x})$ Highest posterior density $1 - \alpha$ credible interval is the interval $[a,b]$ such that
        \begin{enumerate}
          \item $\int_{\theta = a}^b k(\theta|\mathbf{x})d\theta = 1 - \alpha$
          \item $k(a|\theta) = k(b|\theta)$
          \item $a < \theta^* < b$ where $\theta^*$ is the mode of the posterior distribution.
        \end{enumerate}
        This credible interval will be the shortest. Note it might not contain the Bayes estimator for $\theta$ if the posterior distribution is very highly skewed and has a heavy tail.
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 357
    \end{field}
    \begin{field}
        Unimodal pdf
    \end{field}
    \begin{field}
        A pdf is unimodal if there exists some value $x^*$ such that
        \begin{enumerate}
          \item For all $x \leq x^*, f(x)$ is non decreasing
          \item For all $x \geq x^*, f(x)$ is non-increasing.
        \end{enumerate}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 358
    \end{field}
    \begin{field}
        Confidence Intervals for functions of parameters: Obtain a confidence interval for $\tau = g(\theta)$, given a confidence interval for $\theta$
    \end{field}
    \begin{field}
        \begin{enumerate}
          \item Invert a test of $H_0: \tau = \tau_0$ vs $H_1: \tau \neq \tau_0$
          \item Create a pivot $U = h(\mathbf{X},\tau)$, and construct a pivotal interval for $\tau$
          \item Transform the CI for $\theta$ into an interval for $\tau$
          \begin{itemize}
            \item Coverage of this interval: $C_\tau = \{\tau_0 : \tau_0 = g(\theta_0)$ for $\theta_0 \in C_\theta\}$
            \item NOT the same as $(g(L_\theta(\mathbf{X})), g(U_\theta(\mathbf{X})))$ (unless strictly monotone, otherwise coverage not the same. )
          \end{itemize}
        \end{enumerate}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 359
    \end{field}
    \begin{field}
        Joint Confidence Intervals for Multivariate parameters
    \end{field}
    \begin{field}
        \begin{enumerate}
          \item Construct $(1 - \alpha^*)$ CIs for each component $\theta_j$ separately
          \item Define $\mathscr{C } = \{\theta_0: \theta_{0,j} \in \mathscr{C}_j$ for all $j = 1, \ldots p\}$
          \item Chose the univariate coverage levels $(1 - \alpha^*)$ to ensure that the coverage level of the joint confidence region is at least the desired level $(1 - \alpha)$
          \item Often choose $\alpha^* = \alpha/p$, although this often gives larger CIs - over-coverage
          \item If parameters independent, if we set $1 - \alpha = (1 - \alpha^{**})^p$
        \end{enumerate}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 360
    \end{field}
    \begin{field}
        Example: Construct a CI for $$ X_1, \ldots , X_n \sim iid Exp(\mu_x,\sigma)$$ $$Y_1, \ldots , Y_n \sim iid Exp(\mu_x,\sigma)$$


    \end{field}
    \begin{field}
        $\kappa(\theta) = \mu_x - \mu_y$
        \begin{itemize}
          \item Sufficient statistic for parameter $(\mu_x,\mu_y,\sigma)$ is
          $$(X_{(1)}, Y_{(1)}, W = \sum_{i=1}^n (X_i - X_{(1)}) + \sum_{i=1}^n (Y_i - Y_{(1)}) $$
          \item $X_{(1)} \sim exp(\mu_x, \sigma/n)$
          \item $Y_{(1)} \sim exp(\mu_y, \sigma/n)$
          \item $\frac{2W}{\sigma} \sim \chi^2_{4n-4}$
          \item Construct pivot: $U = \frac{|n(X_{(1)} - Y_{(1)} - \mu_x - \mu_y)}{W/(2n-2)} \sim F_{2,4n-4}$
        \end{itemize}
    \end{field}
\end{note}

% Lecture 13
\begin{note}
    \begin{field}
        \tiny 361
    \end{field}
    \begin{field}
        Determinant of a square matrix
    \end{field}
    \begin{field}
        The determinant of a square matrix $\mathbf{A}$ is denoted by $|\mathbf{A}|$ and is defined recursively
        \begin{itemize}
          \item For a $(1 \times 1 )$ matrix $|\mathbf{A}| = a_{11}$
          \item For a $(p\times p)$ matrix $|\mathbf{A}| = \sum_{j=1}^p a_{ij}(-1)^{i+j}|\mathbf{A}_{-i,-j}|$
          \item Where $\mathbf{A}_{-i,-j}$ is the matrix obtained by removing the $i$th column and $j$th row from $\mathbf{A}$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 362
    \end{field}
    \begin{field}
        Properties of the determinant
        \begin{itemize}
          \item det$(\mathbf{I_p})$
          \item det$(\mathbf{A}^{-1})$
          \item $\mathbf{A}$ is invertible iff
          \item det($AB$)
          \item det$(\mathbf{A}^t)$
          \item det$(\mathbf{XAX}^{-1}) = $
          \item Relationship to eigenvalues
        \end{itemize}
    \end{field}
    \begin{field}
      \begin{itemize}
        \item det$(\mathbf{I_p}) = 1$
        \item det$(\mathbf{A}^{-1}) = $ (det$(\mathbf{A}))^{-1}$
        \item $\mathbf{A}$ is invertible iff det($\mathbf{A} \neq 0$
        \item det($\mathbf{AB}$) = det(\textbf{A})det(\textbf{B})
        \item det$(\mathbf{A}^t) =$ det(\textbf{A})
        \item det$(\mathbf{XAX}^{-1}) = $ det(\textbf{A})
        \item The determinant of $\mathbf{A}$ is equal to the product of all eigen values of $\mathbf{A}$
      \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 363
    \end{field}
    \begin{field}
        Properties of the Multivariate Normal Distribution
    \end{field}
    \begin{field}
        \begin{itemize}
          \item If $\mathbf{X}$ has a multivariate normal distribution, then each element has a marginal normal distribution
          \item Random variables $X_1, \ldots X_p$ with marginal normal distributions DO NOT necessarily have a multivariate normal joint distribution
          \item All subsets of elements of $\mathbf{X}$ have a multivariate normal distribution
          \item All linear combinations of the components of $X$ are normally distributed
          \item $\mathbf{X} + c \sim MVN(\mu + c, \Sigma)$
          \item $\mathbf{AX} \sim MVN(\mathbf{A}\mu, \mathbf{A} \Sigma A^t)$, where each element of $\mathbf{AX}$ is a linear combination of the random vector \textbf{X}
          \item $Cov(X_j,X_k) = \sigma_{jk} = 0$ iff and only if $X_j,X_k$ independent
          \item $Z = \Sigma^{-1/2}(\vec{X} - \vec{\mu}) \sim MVN(\vec{0},I_p)$
          \item $Z^tZ \sim \chi^2_p$

        \end{itemize}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 364
    \end{field}
    \begin{field}
        Distribution of sample mean and variance from Multivariate normal
    \end{field}
    \begin{field}
        \begin{itemize}
          \item $\mathbf{\bar{X}} \sim MVN(\vec{\mu}, \frac{1}{n}\Sigma)$
          \item $\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n(\mathbf{X}_i - \mathbf{\bar{X}})(\mathbf{X}_i - \mathbf{\bar{X}})^t$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 365
    \end{field}
    \begin{field}
        $X \sim \chi^2_{v_1}, Y \sim \chi^2_{v_2}, X + Y \sim $
    \end{field}
    \begin{field}
        $\chi^2_{v_1 + v_2}$
    \end{field}
\end{note}

% Lecture 14

\begin{note}
    \begin{field}
        \tiny 366
    \end{field}
    \begin{field}
        Definition of consistent sequence of estimators
    \end{field}
    \begin{field}
        Suppose we have random variables $X_1, X_2, \ldots$, such that the collection $\{X_1, \ldots X_n\}$ gives information about a parameter $\theta$. A sequence of estimators $W_n = W_n(X_1, \ldots, X_n)$ is a consistent sequence of estimators of the parameter $\theta$ if FOR EVERY $\theta \in \Theta$,
        $$W_n \overset{p}{\to} \theta $$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 367
    \end{field}
    \begin{field}
        Mean square error
    \end{field}
    \begin{field}
        $$MSE(\hat{\theta},\theta) = E[(\hat{\theta} - \theta)^2] = Bias^2 + V(\hat{\theta})$$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 368
    \end{field}
    \begin{field}
        If a sequence of estimators $W_n$ for a parameter $\theta$ satisfies
        \begin{itemize}
          \item $\lim_{n\to\infty} V_\theta(W_n) \to 0$
          \item $\lim_{n\to\infty} E_{\theta}(W_n) \to \theta$
          \item Equivalently, if $MSE_\theta(W_n;\theta) \to 0$
        \end{itemize}
        for all $\theta \in \Theta$, then
    \end{field}
    \begin{field}
        $W_n$ is a consistent sequence of estimators for the parameter $\theta$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 369
    \end{field}
    \begin{field}
        How to prove consistency:
    \end{field}
    \begin{field}
      If a sequence of estimators $W_n$ for a parameter $\theta$ satisfies
      \begin{itemize}
        \item $\lim_{n\to\infty} V_\theta(W_n) \to 0$
        \item $\lim_{n\to\infty} E_{\theta}(W_n) \to \theta$
        \item Equivalently, if $MSE_\theta(W_n;\theta) \to 0$
      \end{itemize}
      for all $\theta \in \Theta$, then $W_n$ is consistent.
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 370x
    \end{field}
    \begin{field}
        Definition of asymptotic distribution of Estimators
    \end{field}
    \begin{field}
        Suppose that a sequence of random variables $W_n$ satisfies
        $$k_n(W_n - \theta) \overset{d}{\to} F $$
        \begin{itemize}
          \item $k_n$ is the stabilizing constant,
          \item $F$ is the asymptotic distribution
          \item $\sigma^2(\theta)$ the asymptotic variance (which can't depend on $n$)
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 371x
    \end{field}
    \begin{field}
        $X_i \sim Exp(\sigma)$, $\sum X_i \sim $
    \end{field}
    \begin{field}
        $\sum X_i \sim Gamma(n,\sigma)$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 372
    \end{field}
    \begin{field}
        Relationship between asymptotic variance and limiting variance
    \end{field}
    \begin{field}
        limiting variance $\geq$ asymptotic variance, where the limiting variance is defined as $\lim_{n \to \infty} V(Y_n)$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 373
    \end{field}
    \begin{field}
        Score function
    \end{field}
    \begin{field}
        $$U(\theta) = \frac{\partial}{\partial \theta} l(\theta;\mathbf{x}) = \sum_{i=1}^n \frac{\frac{\partial}{\partial \theta}f(x_i;\theta)}{f(x_i;\theta)} $$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 374
    \end{field}
    \begin{field}
        Notation and test construction $X_1, \ldots, X_n$ iid
        \begin{itemize}
          \item Density/Mass Function
          \item Likelihood
          \item Log likelihood
          \item Maximum likelihood estimator
          \item Score
          \item Information
        \end{itemize}
    \end{field}
    \begin{field}
      \begin{itemize}
        \item Density/Mass Function : $f(x;\theta)$
        \item Likelihood : $L(\theta;\mathbf{x}) = \prod_{i=1}^n f(x_i;\theta)$
        \item Log likelihood $l(\theta;\mathbf{x}) = \log L(\theta;\mathbf{x}) = \sum_{i=1}^n \log f(x_i;\theta)$
        \item Maximum likelihood estimator $\hat{\theta}_n : \frac{\partial}{\partial \theta} l(\theta;\mathbf{x})\bigg|_{\theta = \hat{\theta_n}} = 0$ (except if support depends on parameter of interest)
        \item Score: $U(\theta) = \frac{\partial}{\partial \theta} l(\theta;\mathbf{x}) = \sum_{i=1}^n \frac{\frac{\partial}{\partial \theta}f(x_i;\theta)}{f(x_i;\theta)} $
        \item Information: $I_1(\theta) = E\bigg(- \frac{\partial^2}{\partial \theta^2} \log f(x; \theta)\bigg)$
      \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 375
    \end{field}
    \begin{field}
        Important Assumptions of Maximum Likelihood Estimators
    \end{field}
    \begin{field}
        Note these assumptions hold in exponential families, Binomial, Poisson, etc
        \begin{itemize}
          \item $X_i$ iid with common density function
          \item Identifiability: for $\theta_1 \neq \theta_2$, $f(x;\theta_1) \neq f(x;\theta_2)$
          \item Common support (the one we need to verify ) The set of possible values for $X$ does not depend on the value of the parameter $\theta$
          \item Open Parameter Space (ie cant include 0 or 1 in bernoulli case)
        \end{itemize}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 376
    \end{field}
    \begin{field}
        Asymptotic Normality of MLE
    \end{field}
    \begin{field}
        Under the common assumptions, The MLE $\hat{\theta}_n$ for $\theta$ satisfies
        $$ \frac{\hat{\theta})n -\theta}{\sqrt{\frac{1}{n I_1(\theta)}}} \overset{d}{\to} N(0,1)$$
        Equivalently,
        $\hat{\theta}_n \dot\sim N(\theta, \frac{1}{nI_1(\theta)})$
        This implies that the MLE is asymptotically the UMVUE (as $E(\hat{\theta}_n \approx \theta)$ and $Var(\hat{\theta}_n) \approx \frac{1}{I_n(\theta)}$), the CRLB var of unbiased estimate of $\theta$
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 377
    \end{field}
    \begin{field}
        Mann-Wald Theorem (for MLE)
    \end{field}
    \begin{field}
        For any differentiable $g(\cdot)$ with non-zero first derivative:
        if $\sqrt{n}(\hat{\theta}_n - \theta) \overset{d}{\to} N(0, \frac{1}{I_1(\theta)})$, then
        $$\sqrt{n}(g(\hat{\theta}_n) - g(\theta)) \overset{d}{\to} N(0,\frac{[g'(\theta)]^2}{I_1(\theta)}) $$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 378
    \end{field}
    \begin{field}
        Wald Hypothesis Tests and Confidence intervals
    \end{field}
    \begin{field}
        Since $\theta$ is not known, we cannot find the value of $I(\theta)$ exactly, but we can use $\hat{I}(\theta) = I(\hat{\theta}_n) \overset{p}{\to} I(\theta)$, where $\hat{\theta}_n$ is the MLE
        \begin{itemize}
          \item Test statistic: $$W(\theta_0) = \frac{\hat{\theta}_n - \theta_0}{\sqrt{\frac{1}{nI_1(\hat{\theta}_n)}}} $$
          \item Two sided alternative: Reject $H_0: \theta = \theta_0$ vs $\theta \neq \theta_0$ if
          $$|W(\theta_0)| > z_{\alpha/2} $$
          \item Confidence interval:
          $$\bigg( \hat{\theta}_n - z_{\alpha/2} \sqrt{\frac{1}{nI_1(\hat{\theta_n})}}, \hat{\theta}_n + z_{\alpha/2} \sqrt{\frac{1}{nI_1(\hat{\theta_n})}} \bigg) $$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 379
    \end{field}
    \begin{field}
        Asymptotic Likelihood-Based Test Statistics
    \end{field}
    \begin{field}
        \begin{tabular}{c c c }
          Test & Test Statistic & Asymptotic Null Distribution \\
          \hline \\
          Likelihood Ratio & $G(\theta_0) = 2\big(l(\hat{\theta}_n; \mathbf{x}) - l(\theta_0; \mathbf{x})\big)$ & $\chi^2_1$\\
          \hline \\
          Wald & $W(\theta_0) = \frac{(\hat{\theta}_n - \theta_0)^2}{\frac{1}{n I(\hat{\theta}_n)}}$ & $\chi^2_1$\\
          & $W^*(\theta_0) = \frac{\hat{\theta}_n - \theta_0}{\sqrt{\frac{1}{n I(\hat{\theta}_n)}}}$ \\
          \hline \\
          Score & $S(\theta_0) = \frac{U(\theta_0)^2}{nI(\theta_0)}$ (no need to find MLE) & $\chi^2_1$ \\
          & $S^*(\theta_0) = \frac{U(\theta_0)}{\sqrt{nI(\theta_0)}}$ & $N(0,1)$
        \end{tabular}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 380
    \end{field}
    \begin{field}
        Comparison between Likelihood Ratio, Wald and Score tests and intervals
    \end{field}
    \begin{field}
        \begin{enumerate}
          \item Likelihood Ratio:
          \begin{itemize}
            \item Requires computation of $sup_{\theta \in \Theta} L(\hat{\theta};\mathbf{x})$ (likelihood of MLE) and $L(\theta_0; \mathbf{x})$ (likelihood of null hypothesis )
            \item Generally hardest to invert to form confidence intervals
          \end{itemize}
          \item Wald:
          \begin{itemize}
            \item Only requires computation of MLE and Information under MLE
            \item Tends to differ from the Score and LR tests more than they do from each other
            \item Easiest to invert to form confidence intervals
          \end{itemize}
          \item Score:
          \begin{itemize}
            \item Only requires computation of Information under Null
            \item Has best power of the three tests for alternatives close to null
            \item Typically similar in form to Wald, except variance is estimated under null instead of using MLE
          \end{itemize}
        \end{enumerate}
        All three tests are asymptotically equivalent under the null hypothesis when $n$ is large. If the alternative is true, they may differ substantially, no matter the value of $n$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 381
    \end{field}
    \begin{field}
        Variance Stabilizing Transformation
    \end{field}
    \begin{field}
        To potentially improve confidence interval coverage in settings where there is a mean-variance relationship, we can try to find a function $g(\cdot)$ such that $$(g'(\theta))^2V(\hat{\theta}) = 1 $$ That is the asymptotic variance of $g(\hat{\theta})$ does not depend on the value of $\theta$. This function $g(\cdot)$ is called the variance stabilizing transformation for the estimate $\hat{\theta}$
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 382
    \end{field}
    \begin{field}
        Find the information in the sample $X_1, \ldots, X_m, Y_1, \ldots, Y_n$ where $X_i$ iid and $Y_j$ iid and $X_i$ and $Y_i$ are independent
    \end{field}
    \begin{field}
        $I_{X,Y}(\theta) = mI_{X_1}(\theta) + nI_{Y_1}(\theta)$
    \end{field}
\end{note}

%%end_tag
\end{document}
