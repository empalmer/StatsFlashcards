\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\newenvironment{note}{\paragraph{NOTE:}}{}
\newenvironment{field}{\paragraph{field:}}{}
%\newenvironment{tags}{\paragraph{tags:}}{}
\newcommand*{\tags}[1]{\paragraph{tags: }#1}

\begin{document}

%%start_tag Undergrad_Textbook

\tags{UndergradTextbok}
%Chapter 2

\begin{note}
  \begin{field}
    Law of total probability $k=2$ (using conditional probability)
  \end{field}
  \begin{field}
    $P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$

  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Bayes formula in terms of law of total probability,
  \end{field}
  \begin{field}
    $P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^c)P(B^c)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    $P(A \text{ and }B)$
  \end{field}
  \begin{field}
    $P(A \text{ and }B) = P(A|B)P(B) = P(B|A)P(A)$
  \end{field}
\end{note}


% Chapter 3

\begin{note}
  \begin{field}
    Events $A$ and $B$ are independent if
  \end{field}
  \begin{field}
    $P(A|B) = P(A)$ equivalently $P(A \text{ and } B)  = P(A)P(B)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Poisson setting
  \end{field}
  \begin{field}
    The Poisson setting arises in the context of discrete counts of events that occur over space or time with the small probability and where successive events are independent

    Eg: 2 on average calls a minute, $X$ is number of calls a minute, $X \sim Pois $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Poisson approximation of binomial distribution
  \end{field}
  \begin{field}
    Suppose $X \sim Binom(n,p)$, $Y \sim Pois (\lambda)$. If $n\to \infty$, and $p \to 0$, in such a way that $np \to \lambda > 0$, then for all $k$, $P(X = k) \to P(Y = k)$. The Poisson distribution with parameter $\lambda = np$ serves as a good approiximation for the binomial distribution when $n$ is large and $p$ is small.
  \end{field}
\end{note}

% Chapter 4


\begin{note}
  \begin{field}
    $E(f(X,Y))$ when $X,Y$ are discrete
  \end{field}
  \begin{field}
    $E(f(X,Y)) = \sum_x \sum_y f(x,y)P(X=x,Y=y)$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    If $X,Y$ are independent, then $f(X),g(Y)$
  \end{field}
  \begin{field}
    are also independent
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    If $X,Y$ independent, $E(XY) = , E(f(X)g(Y)) = $
  \end{field}
  \begin{field}
    If $X,Y$ independent, $E(XY) = E(X)E(Y), E(f(X)g(Y)) = E(f(X))E(g(Y))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Sum of independent discrete random variables $X,Y$: $P(X+Y = k)$
  \end{field}
  \begin{field}
    $P(X+Y = k) = \sum_i P(X=i)P(Y=k-i)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    $V(X) = 0$
  \end{field}
  \begin{field}
    If and only if $X $ is a constant
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    $E(I_A) = , V(I_A)$ Where $I_A$ is an indicator function
  \end{field}
  \begin{field}
    $E(I_A) = P(A), V(I_A) = P(A)P(A^c)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = $$
  \end{field}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = \frac{P(X=x,Y=y)}{P(X=x)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = $
  \end{field}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = \sum_y y P(Y = y | X = x)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Problem solving strategy for expected value of counting
  \end{field}
  \begin{field}
    Use indicator functions for each trial , where $X = \sum I$ and use linearity of expectation
  \end{field}
\end{note}

 % Chapter 5

\begin{note}
  \begin{field}
    $P(X > s + t|X > t)$ for geometric, exponential
  \end{field}
  \begin{field}
    $P(X > s + t|X > t) = P(X > s)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Distribution for: A bag of $N$ balls which conatins $r$ red balls and $N-r$ blue balls, $X$ is number of red balls in a sample of size $n$ taken without replacement.
  \end{field}
  \begin{field}
    Hypergeometric.
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Distribution for modeling arrival time
  \end{field}
  \begin{field}
    Exponential
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    $E(g(X,Y)) = $ (continuous )
  \end{field}
  \begin{field}
    $E(g(X,Y)) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty g(x,y)f(x,y)dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    $Cov(X,Y) = $ (integration )
  \end{field}
  \begin{field}
    $Cov(X,Y) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty (x - E(X))(y - E(Y))dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Problem solving strategies for functions of random variables
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Methods of cdf: $Y = g(X)$, find cdf $P(Y \leq y) = P(g(X) \leq y) = P(X \leq g^{-1}(y))$
      \item For finding $P(X < Y)$, set up integrals that cover
      \item For finding probabilities of independent uniform random variables, use geometric (area) properties
    \end{itemize}
  \end{field}
\end{note}

% Chapter 7

\begin{note}
  \begin{field}
    Quantile
  \end{field}
  \begin{field}
    If $X$ is a continuous random variable, then the $p$th quantile is is the number $q$ that satisfies $P(X \leq q) = p/100 $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Poisson process
  \end{field}
  \begin{field}
    Times between arrivals are modeled as iid exponential random variables with parameter $\lambda = 1/\beta$. Let $N_t$ be the number of arrivals up to time $t$. Then $N_t \sim Pois(\lambda t)$
% go back and do properties of poisson process
  \end{field}
\end{note}

% Chapter 8

\begin{note}
  \begin{field}
    Conditional density function $f_{Y|X}(y|x) = $
  \end{field}
  \begin{field}
    $f_{Y|X}(y|x) = \frac{f(x,y)}{f_x(x)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Continuous bayes formula
  \end{field}
  \begin{field}
    $f_{X|Y}(x|y)  = \frac{f_{Y|X}(y|x)f_x(x)}{\int_{t = -\infty}^\infty f_{Y|X}(y|t)f_x(t)dt}$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    Conditional expectation for continuous random variables
    $E(Y|X = x)$
  \end{field}
  \begin{field}
    $E(Y|X = x) = \int_y y f_{Y|X}(y|x)dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Law of total expectation
  \end{field}
  \begin{field}
    $E(Y) = E(E(Y|X))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = $
      \item $E(g(Y)|X=x) = $
      \item If $X,Y$ independent, $E(Y|X) = $
      \item If $Y = g(X)$, then $E(Y|X) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = a E(Y|X) + bE(Z|X)$
      \item $E(g(Y)|X=x) = \int_y g(y) f_{Y|X}(y|x)dy$
      \item If $X,Y$ independent, $E(Y|X) = E(Y)$
      \item If $Y = g(X)$, then $E(Y|X) = Y$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Law of total probability, continuous
  \end{field}
  \begin{field}
    $P(A) = \int_{-\infty}^\infty P(A|X=x)f_x(x)dx$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Conditional variance $V(Y|X = x)$
  \end{field}
  \begin{field}
    $$V(Y|X = x) = \sum_y (y - E(Y|X=x))^2P(Y=y|X=x)$$ discrete
    $$V(Y|X = x) = \int_y (y - E(Y|X=x))^2 f_{Y|X}(y|x)dy$$ continuous
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = $
      \item $V(aY + b | X = x) = $
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = E(Y^2 | X=x) - (E(Y|X=x))^2$
      \item $V(aY + b | X = x) = a^2 V(Y|X=x)$
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = V(Y|X=x) + V(Z|X=x)$
    \end{itemize}
  \end{field}
\end{note}

% Chapter 9

\begin{note}
  \begin{field}
    $P(X \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(X \geq \epsilon) \leq E(X) / \epsilon$ (Markov's Inequality )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    $P(|X-\mu| \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(|X-\mu| \geq \epsilon) \leq \sigma^2/\epsilon^2$ (Chebyshev's inequality, if mean and variance finite )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    $P(\lim_{n\to \infty} S_n/n = \mu) = $
  \end{field}
  \begin{field}
  $P(\lim_{n\to \infty} S_n/n = \mu) = 1$ (Strong law of large numbers )
\end{field}
\end{note}

%%end_tag

\end{document}
