\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,mathrsfs}
\newenvironment{note}{\paragraph{NOTE:}}{}
\newenvironment{field}{\paragraph{field:}}{}
\newenvironment{tags}{\paragraph{tags:}}{}

\begin{document}
%%start_tag Theory1
\tags{Theory1}
%%start_tag Casella Ch1
\begin{note} \begin{field} \tiny 130 \end{field}
  \begin{field}
    \begin{tabular}{|c| c c |}
    \hline \\
    & replace & no replacement \\
    number of trials &  &  \\
    Draw till nth success & & \\
    \hline
    \end{tabular}
  \end{field}
  \begin{field}
    \begin{tabular}{|c| c c |}
    \hline \\
    & replace & no replacement \\
    number of trials & Binom & Hypergeometric \\
    Draw till nth success & Nbinom & Negative hypergeometric \\
    \hline
    \end{tabular}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 131 \end{field}
  \begin{field}
    Plug uniform into inverse CDF
  \end{field}
  \begin{field}
    Get cdf
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 132 \end{field}
  \begin{field}
    Sample Space
  \end{field}
  \begin{field}
    The set, $S$, of all possible outcomes of a particular experiment is called the \textit{sample space} for the experiment.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 133 \end{field}
  \begin{field}
    Event
  \end{field}
  \begin{field}
    An \textit{event} is any collection of possible outcomes of an experiment, that is, any subset of $S$ (including $S$ itself).
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 134 \end{field}
  \begin{field}
    Union
  \end{field}
  \begin{field}
    $ A \cup B = \{x:x \in A \text{ or } x \in B\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 135 \end{field}
  \begin{field}
    Intersection
  \end{field}
  \begin{field}
    $A \cap B = \{x:x \in A \text{ and } x \in B\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 136 \end{field}
  \begin{field}
    Complementation
  \end{field}
  \begin{field}
    $A^c = \{x:x\notin A\}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 137 \end{field}
  \begin{field}
    Commutativity
    $$ A \cup B = $$
    $$ A \cap B = $$
  \end{field}
  \begin{field}
    Commutativity
    $$ A \cup B = B \cup A$$
    $$ A \cap B = B \cap A$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 138 \end{field}
  \begin{field}
    Associativity
    $$ A \cup (B \cup C) = $$
    $$ A \cap (B \cap C) = $$
  \end{field}
    \begin{field}
      Associativity
        $$ A \cup (B \cup C) = (A \cup B) \cup C$$
        $$ A \cap (B \cap C) = (A \cap B) \cap C$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 139 \end{field}
  \begin{field}
    Distributive Laws
    $$A \cap ( B \cup C) = $$
    $$ A \cup ( B \cap C) = $$
  \end{field}
    \begin{field}
      Distributive Laws
        $$A \cap ( B \cup C) = (A \cap B) \cup (A \cap C)$$
        $$ A \cup ( B \cap C) = (A \cup B) \cap ( A \cup C)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 140 \end{field}
  \begin{field}
    DeMorgan's Laws
    $$(A \cup B)^c = $$
    $$(A \cap B)^c = $$
  \end{field}
    \begin{field}
      DeMorgan's Laws
        $$(A \cup B)^c = A^c \cap B^c$$
        $$(A \cap B)^c = A^c \cup B^c$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 141 \end{field}
  \begin{field}
    Disjoint
  \end{field}
    \begin{field}
        Disjoint: Two events $A$ and $B$ are disjoint ( or mutually exclusive) if $ A \cap B = \emptyset $
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 142 \end{field}
  \begin{field}
    $$P(A_1 \cap A_2 \cap \cdots \cap A_n)  = $$
  \end{field}
  \begin{field}
    $$P(A_1)P(A_2|A_1)P(A_3|A_1A_2) \ldots P(A_n|A_1\cdots A_{n-1}) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 143 \end{field}
  \begin{field}
    $$ P(A,B,C) = $$
  \end{field}
  \begin{field}
    $$ P(A,B,C) = P(A)P(B|A)P(C|A,B)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 144 \end{field}
  \begin{field}
    $$P(A \cup B \cup C) =  $$
  \end{field}
  \begin{field}
    $$P(A \cup B \cup C) =  P(A) + P(B) + P(C) - P(A \cap B) - P(B\cap C) - P(A \cap C) + P(A \cap B \cap C)$$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 145 \end{field}
  \begin{field}
    Pairwise disjoint
  \end{field}
    \begin{field}
        Two Events $A_1, A_2$ are pairwise disjoint ( or mutually exclusive) if $A_i \cap A_j = \emptyset $ for all $i \neq j$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 146 \end{field}
    \begin{field}
      Partition
    \end{field}
    \begin{field}
        If $A_1, A_2, \ldots$ are pairwise disjoint and $\cup_{i=1}^\infty A_i = S$, then the collection $A_1, A_2, \ldots$ forms a partition of $S$.
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 147 \end{field}
    \begin{field}
        Sigma Algebra
    \end{field}
    \begin{field}
        A collection of subsets of $S$ is called a sigma algebra (or Borel field), denoted by $\mathcal{B}$, if it satisfies the following three properties:
            \begin{enumerate}
              \item $\emptyset \in \mathcal{B}$ (the empty set is an element of $\mathcal{B}$)
              \item If $A \in \mathcal{B}$, then $A^c \in \mathcal{B}$ ($\mathcal{B}$ is closed under complementation)
              \item If $A_1, A_2, \ldots \in \mathcal{B}$, then $\cup_{i-1}^\infty A_i \in \mathcal{B} \mathcal{B}$ is closed under countable unions)
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 148 \end{field}
    \begin{field}
        Probability Function / Kolmogorov Axioms
    \end{field}
    \begin{field}
        Given a sample space $S$ and an associated sigma algebra $\mathcal{B}$, a probability function is a function $P$ with domain $\mathcal{B}$ that satisfies:
            \begin{enumerate}
              \item $P(A) \geq 0$ for all $A \in \mathcal{B}$
              \item $P(S) = 1$
              \item If $A_1, A_2, \ldots \mathcal{B}$ are pairwise disjoint, then $P(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)$ (Axiom of Countable Additivity)
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 149 \end{field}
    \begin{field}
      If $A\in \mathcal{B}$ and $B \in \mathcal{B}$ are disjoint, then
          $$P(A\cup B) = P(A) + P(B) $$
        Axiom of Finite Additivity
    \end{field}
    \begin{field}
        If $A\in \mathcal{B}$ and $B \in \mathcal{B}$ are disjoint, then
            $$P(A\cup B) = P(A) + P(B) $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 150 \end{field}
    \begin{field}
        Properties of probability functions
        \begin{enumerate}
              \item $P(\emptyset) = $
              \item $P(A) $
              \item $P(A^c) = $
            \end{enumerate}
    \end{field}
    \begin{field}
      Properties of probability functions
        \begin{enumerate}
              \item $P(\emptyset) = 0$
              \item $P(A) \leq 1$
              \item $P(A^c) = 1 - P(A)$
            \end{enumerate}
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 151 \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(B\cap A^c) = $$
    \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(B\cap A^c) = P(B) - P(A \cap B)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 152 \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(A\cup B) = $$
    \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then $$P(A\cup B) = P(A) + P(B) - P(A \cap B)$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 153 \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then if $A \subset B$ then
    \end{field}
    \begin{field}
        If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal{B}$, then if $A \subset B$ then $P(A) \leq P(B)$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 154 \end{field}
    \begin{field}
        Bonferroni's Inequality
        $$P(A\cap B)  $$
    \end{field}
    \begin{field}
      Bonferroni's Inequality:
        $$P(A\cap B) \geq P(A) + P(B) -1 $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 155 \end{field}
    \begin{field}
        If $P$ is a probability function, then for any partition $C_1, C_2, \ldots P(A) = $
    \end{field}
    \begin{field}
        If $P$ is a probability function, then for any partition $C_1, C_2, \ldots P(A) = \sum_{i=1}^\infty P(A \cap C_i)$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 156 \end{field}
    \begin{field}
        Boole's Inequality
        $$P(\cup_{i=1}^\infty A_i)$$
    \end{field}
    \begin{field}
        If $P$ is a probability function,
            $$P(\cup_{i=1}^\infty A_i) \leq \sum_{i=1}^\infty P(A_i) \text{ for any sets } A_1, A_2, \ldots $$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 157 \end{field}
    \begin{field}
        Fundamental Theorem of Counting
    \end{field}
    \begin{field}
        Ifa job consists of $k$ separate tasks, the $i$th of which can be done in $n_i$ ways, $i = 1, \ldots, k$, then the entire job can be done in $n_1 \times n_2 \times \cdots, \times n_k$ ways.
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 158 \end{field}
    \begin{field}
        Ordered without replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
        $$\frac{n!}{(n-r)!}$$

        eg lottery with $n=44$ choices for $r=6$ values, cant use same number twice, order matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 159 \end{field}
    \begin{field}
        Unordered without replacement: number of arrangements of size $r$ from $n$ objects
      \end{field}
    \begin{field}
        $$\binom{n}{r} = \frac{n!}{r!(n-r!)}$$

        eg lottery with $n=44$ choices for $r=6$ values, cant use same number twice, order does not matter (Use ordered without replacement and divide by redundant orderings )
    \end{field}
\end{note}


\begin{note} \begin{field} \tiny 160 \end{field}
    \begin{field}
        Ordered with replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
      Ordered with replacement: number of arrangements of size $r$ from $n$ objects
        $$n^r$$
        eg lottery with $n=44$ choices for $r=6$ values, can use same number twice, order matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 161 \end{field}
    \begin{field}
        Unordered with replacement: number of arrangements of size $r$ from $n$ objects
    \end{field}
    \begin{field}
      Unordered with replacement: number of arrangements of size $r$ from $n$ objects
        $$\binom{n+r-1}{r} = \frac{(n+r-1)!}{r!(n-1)!}$$

        eg lottery with $n=44$ choices for $r=6$ values, can use same number twice, order does not matters
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 162 \end{field}
  \begin{field}
    Number of arrangements of size $r$ from $n$ objects

    \begin{tabular}{|c| c c|}
      \hline \\
       & Without Replacement & With replacement\\
       \hline \\
       Ordered & & \\
       Unordered & & \\
       \hline
    \end{tabular}
  \end{field}
  \begin{field}
    Number of arrangements of size $r$ from $n$ objects

    \begin{tabular}{|c| c c|}
      \hline \\
       & Without Replacement & With replacement\\
       \hline \\
       Ordered & $\frac{n!}{(n-r)!}$ & $n^r$ \\
       Unordered & $\binom{n}{r}$& $\binom{n+r-1}{r}$ \\
       \hline
    \end{tabular}
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 163 \end{field}
    \begin{field}
        Binomial Coefficient $\binom{n}{r}$
    \end{field}
    \begin{field}
      Binomial Coefficient
        $$\binom{n}{r} = \frac{n!}{r!(n-r)!}$$
    \end{field}
\end{note}

\begin{note} \begin{field} \tiny 164 \end{field}
  \begin{field}
    $$P(A|B)=$$
  \end{field}
  \begin{field}
    $$ P(A|B) = \frac{P(A \cap B)}{P(B)} $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 165 \end{field}
  \begin{field}
    Statistically independent
    $P(A \cap B) = $
  \end{field}
  \begin{field}
    Statistically independent
    $P(A \cap B) = P(A)P(B)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 166 \end{field}
  \begin{field}
    If $A$ and $B$ are independent events, what else is independent?
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $A $ and $B^c$
      \item $A^c$ and $B$
      \item $A^c$ and $B^c$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 167 \end{field}
  \begin{field}
    Mutually independent
  \end{field}
  \begin{field}
    A collection of events $A_1, \ldots , A_n$ are mutually independent for any subcollection $A_{i1}, \ldots , A_{ik}$, we have
    $$ P((\cap_{j=1}^k A_{ij})) = \prod_{j=1}^k P(A_{ij}) $$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 168 \end{field}
  \begin{field}
    Random variable
  \end{field}
  \begin{field}
    A random variable is a function from a sample space $S$ into the real numbers
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 169 \end{field}
  \begin{field}
    Definition of a pdf
  \end{field}
  \begin{field}
    A function $f_X(x)$ is a pdf (or pmf) of a random variable $X$ if and only if
    \begin{enumerate}
      \item $f_x(x) \geq 0$ for all $x$
      \item $\sum_x f_x(x) = 1$ or $\int_{-\infty}^\infty f_x(x) dx = 1$
    \end{enumerate}
  \end{field}
\end{note}


%%end_tag
%%start_tag Casella Ch2
\begin{note} \begin{field} \tiny 170 \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{enumerate}
      \item If $g$ is an increasing function on $X$, $F_Y(y) = $ for $y \in Y$
      \item If $g$ is a decreasing function on $X$ and $X$ is a continuous random variable, $F_y(y) = $ for $y \in Y$
    \end{enumerate}
  \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{enumerate}
      \item If $g$ is an increasing function on $X$, $F_Y(y) = F_X(g^{-1}(y))$ for $y \in Y$
      \item If $g$ is a decreasing function on $X$ and $X$ is a continuous random variable, $F_y(y) = 1 - F_X(g^{-1}(y))$ for $y \in Y$
    \end{enumerate}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 171 \end{field}
  \begin{field}
    Method of pdf
  \end{field}
  \begin{field}
    Conditions:
    \begin{enumerate}
      \item $g$ is a monotone function
      \item $f_X(x)$ is continuous on $X$
      \item $g^{-1}(y)$ has a continuous derivative
    \end{enumerate}
    Let $X$ have pdf $f_x(x)$ and let $Y = g(Y)$

    $$f_Y(y) = f_x(g^{-1}(y))\big|\frac{d}{dy}g^{-1}(y)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 172 \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{itemize}
      \item If $g$ is an increasing function, $F_Y(y) = $
      \item If $g$ is a decreasing function, and $X$ is a continuous random variable, $F_Y(y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    (Theorem) Let $X$ have cdf $F_X(x)$, let $Y = g(X)$
    \begin{itemize}
      \item If $g$ is an increasing function, $F_Y(y) = F_x(g^{-1}(y))$
      \item If $g$ is a decreasing function, and $X$ is a continuous random variable, $F_Y(y) = 1 - F_X(g^{-1}(y))$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 173 \end{field}
  \begin{field}
    eg: $X \sim Unif(0,1)$, $Y = -log(X)$
    $F_Y(y) = $
  \end{field}
  \begin{field}
    $F_Y(y) = 1 - F_x(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 173  \end{field}
  \begin{field}
    $X$ is a continuous random variable. For $y > 0$,
    $F_Y(y) = $
  \end{field}
  \begin{field}
    \begin{align*}
      F_Y(y) &= P(Y \leq y)\\
      &= P(X^2 \leq y)\\
      &= P(- \sqrt{y} \leq X \leq \sqrt{y})\\
      &= P(X \leq \sqrt{y}) - P(X \leq -\sqrt{y})\\
      &= F_X(\sqrt{y}) - F_X( - \sqrt{y})
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 174 \end{field}
  \begin{field}
    Pdf of $F_X(g(X))$, where $Y = g(X)$
  \end{field}
  \begin{field}
    Chain rule:
    $f_Y(y) = g'(y)f(g(y))$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 175 \end{field}
  \begin{field}
    Method of pdf if $g$ is not monotone all entire domain
  \end{field}
  \begin{field}
    $f_Y = \sum f_x(g_i^{-1}(y))|\frac{d}{dy}g_i^{-1}(y)|$ $y \in Y$, 0 otherwise

    eg: $Y = X^2$,
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 176 \end{field}
  \begin{field}
    $P(Y  \leq y)$ when $Y = F_X(x)$
  \end{field}
  \begin{field}
    \begin{align*}
      P(Y \leq y) &= P(X \leq F_x^{-1}(y))\\
      &= F_X(F_X^{-1}(y))\\
      &= y
    \end{align*}
    $Y$ is uniformly distributed
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 177 \end{field}
  \begin{field}
    $M_x(t) = $ (discrete )
  \end{field}
  \begin{field}
    $M_x(t) = E(e^{tX}) = \sum_x e^{tX}P(X)$ (discrete )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 178 \end{field}
  \begin{field}
    $M_x(t) = $ (continuous)
  \end{field}
  \begin{field}
    $M_x(t) = E(e^{tX}) = \int_{-\infty}^\infty e^{tX}f_x(x)dx$ (continuous)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 179 \end{field}
  \begin{field}
    $E(X^n) = $
  \end{field}
  \begin{field}
    $E(X^n) = M_x^n(0) = \frac{d^n}{dt^n}M_x(t)|_{t=0}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 180 \end{field}
  \begin{field}
    $M(aX + b)(t) = $
  \end{field}
  \begin{field}
    $M(aX + b)(t) = e^{bt}M_x(at)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 181 \end{field}
  \begin{field}
    If $E(X^n)$ exists then...
  \end{field}
  \begin{field}
    If $E(X^n)$ exists then $E(X^m)$ exists for $m \leq n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 182 \end{field}
  \begin{field}
    If $X_i$ are independent and $Y = a_1X_1 + \cdots + a_nX_n + b$, then $M_Y(t) = $
  \end{field}
  \begin{field}
    If $X_i$ are independent and $Y = a_1X_1 + \cdots + a_nX_n$, then $M_Y(t) = e^{bt}\prod_{i=1}^n M_{X_i}(a_i t) $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 183 \end{field}
  \begin{field}
    Example of using MGF for finding expected value:
    MGF gamma: $(\frac{1}{1 - \beta t})^\alpha$: $E(X) = $
  \end{field}
  \begin{field}
    $E(X) = \frac{\alpha\beta}{(1 - \beta t)^{\alpha+1}}|_{t=0} = \alpha \beta$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 184 \end{field}
  \begin{field}
    Using MGF to relate distributions: MGF exp = $( 1 - \beta t)^{-1}$
  \end{field}
  \begin{field}
    $Y = \sum X_i$ is gamma as MGF gamma is $(1 - \beta t)^{-\alpha}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 185 \end{field}
  \begin{field}
    First step in transforming a RV
  \end{field}
  \begin{field}
    Determine support
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 186 \end{field}
  \begin{field}
    $nth$ Moment of X
  \end{field}
  \begin{field}
    $E(X^n)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 187 \end{field}
  \begin{field}
    $n$th central moment of $X$
  \end{field}
  \begin{field}
    $E(X - \mu)^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 188 \end{field}
  \begin{field}
    $(a + b)^n = $
  \end{field}
  \begin{field}
    $(a + b)^n = \sum_{x = 0}^n \binom{n}{x}a^xb^{n-x}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 189 \end{field}
  \begin{field}
    $\sum_{x = 0}^n \binom{n}{x}a^xb^{n-x} = $
  \end{field}
  \begin{field}
    $(a + b)^n$
  \end{field}
\end{note}

%%end_tag
%%start_tag Casella Ch3
\begin{note} \begin{field} \tiny 190 \end{field}
  \begin{field}
    $N$ balls $r$ red $N - r$ green. Select $n$ balls. Probability that $y$ are red?
  \end{field}
  \begin{field}
    Hypergeometric distribution($N,r,n$)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 191 \end{field}
  \begin{field}
    Hypergeometric distribution description $(N,r,n)$
  \end{field}
  \begin{field}
    $N$ is total balls, $r$ is number red balls, $n$ is number balls selected.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 192 \end{field}
  \begin{field}
    Negative binomial description
  \end{field}
  \begin{field}
    Number of Bernoulli trials required to get a fixed number of successes. $r$ being the $r$th success
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 193 \end{field}
  \begin{field}
    Geometric description
  \end{field}
  \begin{field}
    Modeling waiting time. $X$ is the trial at which the first success occurs.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 194 \end{field}
  \begin{field}
    Location-scale family for $f(x)$
  \end{field}
  \begin{field}
    $1/\sigma f((x - \mu)/\sigma)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 195 \end{field}
  \begin{field}
    Given $X$ give the mean and variance for the location-scale random $Y= 1/\sigma f((y - \mu)/\sigma)$ variable
  \end{field}
  \begin{field}
    $E(Y) = \sigma E(X) + \mu$, $V(Y) = \sigma^2 V(X)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 196 \end{field}
  \begin{field}
    $X \sim Pois(\lambda)$ $P(X = x+1) = $
  \end{field}
  \begin{field}
    $X \sim Pois(\lambda)$ $P(X = x+1) = \frac{\lambda}{x + 1}P(X = x)$
  \end{field}
\end{note}

%%end_tag
%%start_tag Casella Ch4

\begin{note} \begin{field} \tiny 197 \end{field}
  \begin{field}
    $f(y|x) = $
  \end{field}
  \begin{field}
    $f(y|x) = \frac{f(x,y)}{f_x(x)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 198 \end{field}
  \begin{field}
    $E(g(Y)|x) = $
  \end{field}
  \begin{field}
    $E(g(Y)|x) = \int_{-\infty}^\infty g(y)f(y|x) dy $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 199 \end{field}
  \begin{field}
    Example of calculating donditional pdfs
$f(x,y) = e^{-y}, 0 < x < y < \infty$. $f(y|x) = $

  \end{field}
  \begin{field}
    \begin{align*}
      f_x(x) = \int_{-\infty}^\infty f(x,y) dy \\
      &= e^{-x}\\
      \\
      f(y|x) &= \frac{f(x,y)}{f_x(x)}\\
      &= \frac{e^{-y}}{e^{-x}} \text{ if } y > x\\
      &= \frac{0}{e^{-x}} \text{ if } y \leq x
    \end{align*}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 200 \end{field}
  \begin{field}
    Let ($X,Y$) be given as $f(x,y)$. Then $X$ and $Y$ are independent if
  \end{field}
  \begin{field}
    Let ($X,Y$) be given as $f(x,y)$. Then $X$ and $Y$ are independent if there exist functions $g(x), h(y)$ such that $f(x,y) = g(x)h(y)$ (factorization - don't need to compute marginals )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 201 \end{field}
  \begin{field}
    Let $X,Y$ be independent. Then $E(g(X)h(Y)) = $
  \end{field}
  \begin{field}
    $E(g(X)h(Y)) = (E(g(X)))(E(h(Y)))$

    example: $E(X^2Y) = E(X^2)E(Y)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 202 \end{field}
  \begin{field}
    $X,Y$ independent\\ $Z = X + Y$  \\ $M_Z(t) = $
  \end{field}
  \begin{field}
    $M_Z(t) = M_x(t)M_Y(t)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 203 \end{field}
  \begin{field}
    Method of pdf bivariate
  \end{field}
  \begin{field}
    $f_{u,v}(u,v) = f_{x,y}(h_1(u,v),h_2(u,v))|J|$\\
    Where $|J| = \begin{vmatrix}
      \frac{\partial  x}{\partial  u} & \frac{\partial  x }{\partial v }\\ \frac{\partial  y }{\partial u } & \frac{\partial  y }{\partial v }
    \end{vmatrix} = \frac{\partial  x }{\partial u } \frac{\partial  y}{\partial v } - \frac{\partial  y }{\partial u } \frac{\partial  x }{\partial v }$

    and $u = g_1(x,y), v = g_2(x,y)$ and $x = h_1(x,y), y = h_2(x,y)$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 204 \end{field}
  \begin{field}
    $X,Y$ independent, $g(X)$ a function only of $X$ and $h(Y)$ a function only of $Y$. Then
  \end{field}
  \begin{field}
    $g(X)$ and $g(Y)$ are independent.
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 205 \end{field}
  \begin{field}
    Correlation
  \end{field}
  \begin{field}
    $\rho_{XY} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 206 \end{field}
  \begin{field}
    $m$ independent trials, each trial resulting in one of $n$ outcomes, with probabilities $p_1, \ldots, p_n$. $X_i$ is the count of the number of times the $i$th outcome occured in the $m$ trials.
  \end{field}
  \begin{field}
    Multinomial distribution
    $f(x_1, \ldots, x_n) = \frac{m!}{x_1! \cdots x_n!}p_1^{x_i} \cdots p_n^{x_n}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 207 \end{field}
  \begin{field}
    $|E(XY)| \leq $ (Cauchy-Schwartz)
  \end{field}
  \begin{field}
    $|E(XY)| \leq E(|XY|) \leq (E(|X|^2))^{1/2}(E(|Y|^2))^{1/2}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 208 \end{field}
  \begin{field}
    $E(g(X)) \geq $ where $g$ is a convex function
  \end{field}
  \begin{field}
    $E(g(X)) \geq g(E(X))$ where $g$ is a convex function (Jensen's inequlity)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 209 \end{field}
  \begin{field}
    Ranking of types of means
  \end{field}
  \begin{field}
    $\mu_{\text{harmonic}} \leq \mu_{\text{geometric}} \leq \mu_{\text{arithmetic}}$ By Jensens inequality (using logs )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 210 \end{field}
  \begin{field}
    Linear transformations of multivariate normal $X \sim N(\vec{\mu},\Sigma) $\\
    $A \vec{X} + \vec{b}$
  \end{field}
  \begin{field}
    $A \vec{X} + \vec{b} \sim N(A \vec{\mu} + \vec{v}, A \Sigma A^t)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 211 \end{field}
  \begin{field}
    $X \sim N(\vec{\mu},\Sigma) $\\

    $\vec{X}_a | \vec{X}_b \sim $
  \end{field}
  \begin{field}
    $\vec{X}_a | \vec{X}_b \sim N\big(\vec{\mu_a} + \Sigma_{ab} \Sigma^{-1}_{bb}(\vec{x}_b - \vec{\mu}_b), \Sigma_{ba} - \Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}$\\

ex: $(X_1,X_2,X_3), \vec{\mu} = (1,2,3)^t, \Sigma = \begin{pmatrix}
  3 & 1 & 0 \\ 1 & 3 & 1 \\ 0 & 1 & 3
\end{pmatrix}$
$X1,X_3 | X_2 = 1$\\
$a = \{1,3\}, b = \{2\}$\\
$\mu_a = (1,3)^t, \mu_b = 1$\\
$\Sigma_{aa} = \begin{pmatrix}
  3 & 0 \\ 0 & 3
\end{pmatrix}, \Sigma_{ab} = (1,1)^t$

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 212 \end{field}
  \begin{field}
    $(X,Y)$ multinomial \\
    $aX + bY \sim $
  \end{field}
  \begin{field}
    $aX + bY \sim N(a \mu_x + b\mu_y, a^2 \sigma_x^2 + b^2 \sigma_y^2 + 2 ab \rho \sigma_x \sigma_y)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 213 \end{field}
  \begin{field}
    $(X,Y)$ multinomial \\
    $Y|X\sim $
  \end{field}
  \begin{field}
    $Y|X\sim  N(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x - \mu_x), \sigma_Y^2(1 - \rho^2))$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 214 \end{field}
  \begin{field}
    CDF for Max order statistic
  \end{field}
  \begin{field}
    $(F(x))^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 215 \end{field}
  \begin{field}
    PDF for Max order statistic
  \end{field}
  \begin{field}
    $n(F(x))^{n-1}f(x)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 216 \end{field}
  \begin{field}
    CDF for Min order statistic
  \end{field}
  \begin{field}
    $1 - (1 - F(x))^n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 217 \end{field}
  \begin{field}
    PDF for Min order statistic
  \end{field}
  \begin{field}
    $n(1 - F(x))^{n-1}f(x)$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 218 \end{field}
  \begin{field}
    CDF for $k$th order statistic
  \end{field}
  \begin{field}
    $F_{(k)}(x) = \sum_{j = k}^n \binom{n}{j}(F(x))^j(1 -F(x))^{n-j}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 219 \end{field}
  \begin{field}
    PDF for $k$th order statistic
  \end{field}
  \begin{field}
    $f_{(k)}(x) = k \binom{n}{k}f(x)F(x)^{k-1}(1 - F(x))^{n-k}$
  \end{field}
\end{note}

%%end_tag
%%start_tag From Stat Cheatsheet

\begin{note}
  \begin{field}
    \tiny 1
  \end{field}
  \begin{field}
    CDF of Geometric ($p$)
  \end{field}
  \begin{field}
    $1 - (1-p)^x$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 2
  \end{field}
  \begin{field}
    CDF of Exponential($\beta$)
  \end{field}
  \begin{field}
    $1 - e^{-\frac{x}{\beta}}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 3
  \end{field}
  \begin{field}
    \begin{itemize}
  \item $P(\varnothing) = $
  \item $B = \Omega \cap B = (A \cup A^c) \cap B
    = (A \cap B) \cup (A^c \cap B)$
  \item $P(A^c) = $
  \item $P(B) = $
  \item $P(\Omega) =  \qquad P(\varnothing) = $
  \item $\left(\bigcup_n A_n\right) =
    \quad
    \left(\bigcap_n A_n\right) =
    \qquad$
    \textsc{DeMorgan}
\end{itemize}
  \end{field}
  \begin{field}
    \begin{itemize}
  \item $P(\varnothing) = 0$
  \item $B = \Omega \cap B = (A \cup A^c) \cap B
    = (A \cap B) \cup (A^c \cap B)$
  \item $P(A^c) = 1 - P(A)$
  \item $P(B) = P(A \cap B) + P(A^c \cap B)$
  \item $P(\Omega) = 1 \qquad P(\varnothing) = 0$
  \item $\left(\bigcup_n A_n\right) = \bigcap_n A_n
    \quad
    \left(\bigcap_n A_n\right) = \bigcup_n A_n
    \qquad$
    \textsc{DeMorgan}
\end{itemize}
  \end{field}
\end{note}




\begin{note}
  \begin{field}
    \tiny 4
  \end{field}
  \begin{field}
    Probability Set intersection
    \begin{itemize}
      \item $P(\bigcup_n A_n)
        = 1 - P(\bigcap_n A_n^c)$
      \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)
        \implies P(A \cup B) \leq P(A) + P(B)$
      \item $P(A \cup B)
        = $
      \item $P(A \cap B^c) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Probability Set intersection
    \begin{itemize}
      \item $P(\bigcup_n A_n)
        = 1 - P(\bigcap_n \mathsf{A_n})$
      \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)\\[1ex]
        \implies P(A \cup B) \le P(A) + P(B)$
      \item $P(A \cup B)
        = P(A \cap B^c) + P(A^c \cap B) + P(A \cap B)$
      \item $P(A \cap B^c) = P(A) - P(A \cap B)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 5
  \end{field}
  \begin{field}
    $P(A \cap B) =  \text{ when }A \text{ and } B \text{independent}$
  \end{field}
  \begin{field}
    $P(A \cap B) = P(A)P(B) \text{ when }A \text{ and } B \text{independent}$
  \end{field}
  \end{note}

\begin{note}
  \begin{field}
    \tiny 6
  \end{field}
  \begin{field}
    $$P(A|B) = $$
  \end{field}
  \begin{field}
    $$P(A|B) = \frac{P(A\cap B)}{P(B)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 7
  \end{field}
  \begin{field}
    Law of total probability
  \end{field}
  \begin{field}
    Law of total probability
    $$P(B) = \sum_{i=1}^n P(B|A_i)P(A_i) \quad \Omega = \cup_{i=1}^n A_i$$

    $$P(B) = P(A \cup B) + P(A^c \cup B) $$
   \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 8
  \end{field}
  \begin{field}
    Bayes Theorem
  \end{field}
  \begin{field}
    Bayes Theorem
    $$P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^n P(B|A_j)P(A_j)} \quad \Omega = \cup_{i=1}^n A_i$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 9
  \end{field}
  \begin{field}
    CDF Laws
  \end{field}
  \begin{field}
    CDF Laws
    \begin{enumerate}
      \item Nondecreasing: $x_1 < x_2 \implies F(x_1) \leq F(x_2)$
      \item Limits: $\lim_{x \to -\infty}=0$ and $\lim_{x\to \infty} = 1$
      \item Right-Continuous $\lim_{y \to x^+}F(y) = F(x)$
    \end{enumerate}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 10
  \end{field}
  \begin{field}
    $$f_{y|x}(y|x) = $$
  \end{field}
  \begin{field}
    $$f_{y|x}(y|x) = \frac{f(x,y)}{f_x(x)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 11
  \end{field}
  \begin{field}
    $X,Y \text{independent}$
    \begin{itemize}
      \item $P(X \leq x, Y \leq y) = $
      \item $f_{x,y}(x,y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    $X,Y \text{independent}$
    \begin{itemize}
      \item $P(X \leq x, Y \leq y) = P(X \leq x)P(Y \leq y)$
      \item $f_{x,y}(x,y) = f_x(x)f_y(y)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 12
  \end{field}
  \begin{field}
     Transformations $Z = \phi(X)$

    \begin{itemize}
      \item Discrete: $f_Z(z) = $
      \item Continuous: $F_Z(z)=$
      \item Cont, $\phi$ strictly monotone:
      $f_z(z)$
    \end{itemize}
  \end{field}
  \begin{field}
    Transformations $Z = \phi(X)$
  \begin{itemize}
    \item Discrete: $$f_Z(z) = P(\phi(X) = z) = P(X \in \phi^{-1}(z)) = \sum_{x \in \phi^{-1}(z)}f_x(x) $$
    \item Continuous (Method of CDF): $$F_Z(z)= P(\phi(X)\leq z) = \int_{x:\phi(x)\leq z} f(x) dx$$
    \item Cont, $\phi$ strictly monotone: (Method of PDF)
    $f_z(z) = f_x(\phi^{-1}(z))|\frac{d}{dz} \phi^{-1}(z)|$
  \end{itemize}
\end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 13
  \end{field}
  \begin{field}
     Rule of the Lazy Statistician: $ E[g(x)] = $
  \end{field}
  \begin{field}
    Rule of the Lazy Statistician:  $E[g(x)] = \int g(x)f_x(x)dx$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 14
  \end{field}
  \begin{field}
    Expectation rules
    \begin{itemize}
      \item $E(c) = $
      \item $E(cX) = $
      \item $E(X + Y) = $
      \item $E(\phi(X)) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Expectation rules
    \begin{itemize}
      \item $E(c) = c$
      \item $E(cX) = cE(X)$
      \item $E(X + Y) = E(X) + E(Y)$
      \item $E(\phi(X)) \neq \phi(E(X))$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 15
  \end{field}
  \begin{field}
    Conditional expectation
      \begin{itemize}
        \item $E(Y|X = x) = $
        \item $E(X) = $
        \item $E(Y+Z|X) = $
        \item $E(Y|X) = c \implies $
      \end{itemize}
  \end{field}
  \begin{field}
    Conditional expectation
      \begin{itemize}
        \item $E(Y|X = x) = \int y f(y|x) dy$
        \item $E(X) = E(E(X|Y))$
        \item $E(Y+Z|X) = E(Y|X) + E(Z|X)$
        \item $E(Y|X) = c \implies Cov(X,Y) = 0$
      \end{itemize}

  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 16
  \end{field}
  \begin{field}
    Variance
    \begin{itemize}
      \item $V(X) = \sigma_x^2 = $
      \item $V(X+Y) =$
      \item $V\bigg[\sum_{i=1}^n X_i\bigg] = $
    \end{itemize}
  \end{field}
  \begin{field}
    Variance
    \begin{itemize}
      \item $V(X) = \sigma_x^2 = E[(X - E(X))^2] = E(X^2) - E(X)^2$
      \item $V(X+Y) = V(X) + V(Y) + Cov(X,Y)$
      \item $V\bigg[\sum_{i=1}^n X_i\bigg] = \sum_{i=1}^n V(X_i) + \sum_{i\neq j} Cov(X_i,X_j)$
    \end{itemize}
  \end{field}
\end{note}








\begin{note}
  \begin{field}
    \tiny 17
  \end{field}
  \begin{field}
    Covariance
    \begin{itemize}
      \item $Cov(X,Y) = $
      \item $Cov(X,c) = $
      \item $Cov(Y,X) = $
      \item $Cov(aX,bY) =$
      \item $Cov(X + a, Y + b) =$
      \item $Cov\bigg(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\bigg) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Covariance
    \begin{itemize}
      \item $Cov(X,Y) = E[(X - E(X)(Y - E(Y)))] = E(XY) - E(X)E(Y)$
      \item $Cov(X,c) = 0$
      \item $Cov(Y,X) = Cov(X,Y)$
      \item $Cov(aX,bY) = abCov(X,Y)$
      \item $Cov(X + a, Y + b) = Cov(X,Y)$
      \item $Cov\bigg(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\bigg) = \sum_{i=1}^n \sum_{j=1}^m Cov(X_i,Y_j)$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 18
  \end{field}
  \begin{field}
    Correlation: $\rho(X,Y)$
  \end{field}
  \begin{field}
    Correlation: $\rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{V(X)V(Y)}}$
  \end{field}
\end{note}



\begin{note}
  \begin{field}
    \tiny 19
  \end{field}
  \begin{field}
    Conditional Variance
    \begin{itemize}
      \item $V(Y|X) = $
      \item $V(Y) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Conditional Variance
    \begin{itemize}
      \item $V(Y|X) = E\big[(Y - E(Y|X))^2|X\big] = E(Y^2|X) - E(Y|X)^2$
      \item $V(Y) = E(V(Y|X)) + V(E(Y|X))$
    \end{itemize}
  \end{field}
\end{note}

%%end_tag
%%start_tag Undergrad_Textbook
%Chapter 2

\begin{note}
  \begin{field}
    \tiny 20
  \end{field}
  \begin{field}
    Law of total probability $k=2$ (using conditional probability)
  \end{field}
  \begin{field}
    $P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$

  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 21
  \end{field}
  \begin{field}
    Bayes formula in terms of law of total probability,
  \end{field}
  \begin{field}
    $P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^c)P(B^c)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 22
  \end{field}
  \begin{field}
    $P(A \text{ and }B)$
  \end{field}
  \begin{field}
    $P(A \text{ and }B) = P(A|B)P(B) = P(B|A)P(A)$
  \end{field}
\end{note}


% Chapter 3

\begin{note}
  \begin{field}
    \tiny 23
  \end{field}
  \begin{field}
    Events $A$ and $B$ are independent if
  \end{field}
  \begin{field}
    $P(A|B) = P(A)$ equivalently $P(A \text{ and } B)  = P(A)P(B)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 24
  \end{field}
  \begin{field}
    Poisson setting
  \end{field}
  \begin{field}
    The Poisson setting arises in the context of discrete counts of events that occur over space or time with the small probability and where successive events are independent

    Eg: 2 on average calls a minute, $X$ is number of calls a minute, $X \sim Pois $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 25
  \end{field}
  \begin{field}
    Poisson approximation of binomial distribution
  \end{field}
  \begin{field}
    Suppose $X \sim Binom(n,p)$, $Y \sim Pois (\lambda)$. If $n\to \infty$, and $p \to 0$, in such a way that $np \to \lambda > 0$, then for all $k$, $P(X = k) \to P(Y = k)$. The Poisson distribution with parameter $\lambda = np$ serves as a good approiximation for the binomial distribution when $n$ is large and $p$ is small.
  \end{field}
\end{note}

% Chapter 4


\begin{note}
  \begin{field}
    \tiny 26
  \end{field}
  \begin{field}
    $E(f(X,Y))$ when $X,Y$ are discrete
  \end{field}
  \begin{field}
    $E(f(X,Y)) = \sum_x \sum_y f(x,y)P(X=x,Y=y)$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 27
  \end{field}
  \begin{field}
    If $X,Y$ are independent, then $f(X),g(Y)$
  \end{field}
  \begin{field}
    are also independent
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 28
  \end{field}
  \begin{field}
    If $X,Y$ independent, $E(XY) = , E(f(X)g(Y)) = $
  \end{field}
  \begin{field}
    If $X,Y$ independent, $E(XY) = E(X)E(Y), E(f(X)g(Y)) = E(f(X))E(g(Y))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 29
  \end{field}
  \begin{field}
    Sum of independent discrete random variables $X,Y$: $P(X+Y = k)$
  \end{field}
  \begin{field}
    $P(X+Y = k) = \sum_i P(X=i)P(Y=k-i)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 30
  \end{field}
  \begin{field}
    $V(X) = 0$
  \end{field}
  \begin{field}
    If and only if $X $ is a constant
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 31
  \end{field}
  \begin{field}
    $E(I_A) = , V(I_A)$ Where $I_A$ is an indicator function
  \end{field}
  \begin{field}
    $E(I_A) = P(A), V(I_A) = P(A)P(A^c)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 32
  \end{field}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = $$
  \end{field}
  \begin{field}
    For discrete jointly distributed random variables,
    $$ P(X = y | X = x) = \frac{P(X=x,Y=y)}{P(X=x)}$$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 33
  \end{field}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = $
  \end{field}
  \begin{field}
    For discrete random variables
    $E(Y|X=x) = \sum_y y P(Y = y | X = x)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 34
  \end{field}
  \begin{field}
    Problem solving strategy for expected value of counting
  \end{field}
  \begin{field}
    Use indicator functions for each trial , where $X = \sum I$ and use linearity of expectation
  \end{field}
\end{note}

 % Chapter 5

\begin{note}
  \begin{field}
    \tiny 35
  \end{field}
  \begin{field}
    $P(X > s + t|X > t)$ for geometric, exponential
  \end{field}
  \begin{field}
    $P(X > s + t|X > t) = P(X > s)$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 36
  \end{field}
  \begin{field}
    Distribution for: A bag of $N$ balls which conatins $r$ red balls and $N-r$ blue balls, $X$ is number of red balls in a sample of size $n$ taken without replacement.
  \end{field}
  \begin{field}
    Hypergeometric.
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 37
  \end{field}
  \begin{field}
    Distribution for modeling arrival time
  \end{field}
  \begin{field}
    Exponential
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 38
  \end{field}
  \begin{field}
    $E(g(X,Y)) = $ (continuous )
  \end{field}
  \begin{field}
    $E(g(X,Y)) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty g(x,y)f(x,y)dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 39
  \end{field}
  \begin{field}
    $Cov(X,Y) = $ (integration )
  \end{field}
  \begin{field}
    $Cov(X,Y) = \int_{y = - \infty}^\infty \int_{x = -\infty}^\infty (x - E(X))(y - E(Y))dx dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 40
  \end{field}
  \begin{field}
    Problem solving strategies for functions of random variables
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Methods of cdf: $Y = g(X)$, find cdf $P(Y \leq y) = P(g(X) \leq y) = P(X \leq g^{-1}(y))$
      \item For finding $P(X < Y)$, set up integrals that cover
      \item For finding probabilities of independent uniform random variables, use geometric (area) properties
    \end{itemize}
  \end{field}
\end{note}

% Chapter 7

\begin{note}
  \begin{field}
    \tiny 41
  \end{field}
  \begin{field}
    Quantile
  \end{field}
  \begin{field}
    If $X$ is a continuous random variable, then the $p$th quantile is is the number $q$ that satisfies $P(X \leq q) = p/100 $
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 42
  \end{field}
  \begin{field}
    Poisson process
  \end{field}
  \begin{field}
    Times between arrivals are modeled as iid exponential random variables with parameter $\lambda = 1/\beta$. Let $N_t$ be the number of arrivals up to time $t$. Then $N_t \sim Pois(\lambda t)$
% go back and do properties of poisson process
  \end{field}
\end{note}

% Chapter 8

\begin{note}
  \begin{field}
    \tiny 43
  \end{field}
  \begin{field}
    Conditional density function $f_{Y|X}(y|x) = $
  \end{field}
  \begin{field}
    $f_{Y|X}(y|x) = \frac{f(x,y)}{f_x(x)}$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 44
  \end{field}
  \begin{field}
    Continuous bayes formula
  \end{field}
  \begin{field}
    $f_{X|Y}(x|y)  = \frac{f_{Y|X}(y|x)f_x(x)}{\int_{t = -\infty}^\infty f_{Y|X}(y|t)f_x(t)dt}$
  \end{field}
\end{note}


\begin{note}
  \begin{field}
    \tiny 45
  \end{field}
  \begin{field}
    Conditional expectation for continuous random variables
    $E(Y|X = x)$
  \end{field}
  \begin{field}
    $E(Y|X = x) = \int_y y f_{Y|X}(y|x)dy$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 46
  \end{field}
  \begin{field}
    Law of total expectation
  \end{field}
  \begin{field}
    $E(Y) = E(E(Y|X))$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 47
  \end{field}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = $
      \item $E(g(Y)|X=x) = $
      \item If $X,Y$ independent, $E(Y|X) = $
      \item If $Y = g(X)$, then $E(Y|X) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional expectation
    \begin{itemize}
      \item $E(aY + bZ | X) = a E(Y|X) + bE(Z|X)$
      \item $E(g(Y)|X=x) = \int_y g(y) f_{Y|X}(y|x)dy$
      \item If $X,Y$ independent, $E(Y|X) = E(Y)$
      \item If $Y = g(X)$, then $E(Y|X) = Y$
    \end{itemize}
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 48
  \end{field}
  \begin{field}
    Law of total probability, continuous
  \end{field}
  \begin{field}
    $P(A) = \int_{-\infty}^\infty P(A|X=x)f_x(x)dx$
  \end{field}
\end{note}

\begin{note}
  \begin{field}
  \tiny 49
\end{field}
  \begin{field}
    Conditional variance $V(Y|X = x)$
  \end{field}
  \begin{field}
    $$V(Y|X = x) = \sum_y (y - E(Y|X=x))^2P(Y=y|X=x)$$ discrete
    $$V(Y|X = x) = \int_y (y - E(Y|X=x))^2 f_{Y|X}(y|x)dy$$ continuous
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 50
  \end{field}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = $
      \item $V(aY + b | X = x) = $
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = $
    \end{itemize}
  \end{field}
  \begin{field}
    Properties of conditional variance
    \begin{itemize}
      \item $V(Y|X=x) = E(Y^2 | X=x) - (E(Y|X=x))^2$
      \item $V(aY + b | X = x) = a^2 V(Y|X=x)$
      \item If $Y,Z$ independent, $V(Y+Z | X=x) = V(Y|X=x) + V(Z|X=x)$
    \end{itemize}
  \end{field}
\end{note}

% Chapter 9

\begin{note}
  \begin{field}
    \tiny 51
  \end{field}
  \begin{field}
    $P(X \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(X \geq \epsilon) \leq E(X) / \epsilon$ (Markov's Inequality )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 52
  \end{field}
  \begin{field}
    $P(|X-\mu| \geq \epsilon)$
  \end{field}
  \begin{field}
    $P(|X-\mu| \geq \epsilon) \leq \sigma^2/\epsilon^2$ (Chebyshev's inequality, if mean and variance finite )
  \end{field}
\end{note}

\begin{note}
  \begin{field}
    \tiny 53
  \end{field}
  \begin{field}
    $P(\lim_{n\to \infty} S_n/n = \mu) = $
  \end{field}
  \begin{field}
  $P(\lim_{n\to \infty} S_n/n = \mu) = 1$ (Strong law of large numbers )
\end{field}
\end{note}

%%end_tag
%%end_tag


\end{document}
