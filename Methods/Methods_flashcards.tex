% -*- coding: utf-8 -*-
\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}
\usepackage{mathrsfs}
\pagestyle{empty}
\setlength{\parindent}{0in}

\newenvironment{note}{\paragraph{NOTE:}}{}
\newenvironment{field}{\paragraph{field:}}{}
\newenvironment{tags}{\paragraph{tags:}}{}

\begin{document}

%%start_tag Methods 1
\tags{Methods1}
\begin{note} \begin{field} \tiny 100000 \end{field}
 \begin{field}
  Epidemiology Definition of Causation
 \end{field}
 \begin{field}
  Factor/variable $X$ \textbf{causes} result $Y$ if some cases of $Y$ would not have occurred if X had been absent.
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100001 \end{field}
 \begin{field}
  Sample variance
 \end{field}
 \begin{field}
  $s^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i - \bar{x})^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100002 \end{field}
 \begin{field}
  Population(s) of interest
 \end{field}
 \begin{field}
  The group to which you would like your answer to apply
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100003 \end{field}
 \begin{field}
  Variable of Interest
 \end{field}
 \begin{field}
  A measurement that can be made on each individual/member of the population
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100004 \end{field}
 \begin{field}
  Facts about Normal Distributions
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If $Z$ has a Normal(0,1) distribution then $X = \sigma Z + \mu$ has a Normal$(\mu,\sigma^2)$ distribution
   \item If $X$ has a Normal($\mu,\sigma^2$) distribution, then $Z = \frac{X - \mu}{\sigma}$ has a Normal(0,1) distribution.
   \item If $X$ has a Normal($\mu_x,\sigma^2_x)$ distribution, and $Y$ has a Normal($\mu_y,\sigma_y^2$) distribution, and $X$ and $Y$ are independent of each other, then $X + Y \sim $ Normal($\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100005 \end{field}
 \begin{field}
  Sample mean
 \end{field}
 \begin{field}
  $\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100006 \end{field}
 \begin{field}
  Sampling distribution for population $Y \sim $ Normal($\mu,\sigma$)
 \end{field}
 \begin{field}
  $N(\mu,\sigma^2/n)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100007 \end{field}
 \begin{field}
  Variance (Expected value)
 \end{field}
 \begin{field}
  $V(Y) = E[(X - E(X))^2] = E(X^2) - E[(X)]^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100008 \end{field}
 \begin{field}
  Covariance
 \end{field}
 \begin{field}
  $Cov(X,Y) = E[(X - E(X))(Y - E(Y))]$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100009 \end{field}
 \begin{field}
  If $X$ and $Y$ are independent (covariance)
 \end{field}
 \begin{field}
  The covariance is 0
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100010 \end{field}
 \begin{field}
  If $Cov(X,Y) = 0$, (independence)
 \end{field}
 \begin{field}
  Cannot say that $X$ and $Y$ are independent
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100011 \end{field}
 \begin{field}
  $Cov(X,X) =$
 \end{field}
 \begin{field}
  $Var(X)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100012 \end{field}
 \begin{field}
  $X \sim N(\mu,\sigma^2)$
  \begin{itemize}
   \item $E(\bar{X}) = $
   \item $V(\bar{X}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\bar{X}) = \mu$
   \item $V(\bar{X}) = \sigma^2/n$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100013 \end{field}
 \begin{field}
  Central Limit Theorem (in words)
 \end{field}
 \begin{field}
  If the population distribution of a variable $X$ has population mean $\mu$ and finite population variance $\sigma^2$, then the sampling distribution of the sample mean becomes closer and closer to a Normal distribution as the sample size $n$ increases: $\bar{X} \sim N(\mu,\sigma^2/n)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100014 \end{field}
 \begin{field}
  Central Limit Theorem (theoretical)
 \end{field}
 \begin{field}
  Let $X_1, X_2, \ldots X_n$ be an iid sample from some poupation distribution $F$ with mean $\mu$ and variance $\sigma^2 < \infty$. Then as the sample size $n \to \infty$, we have $$\frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} \to N(0,1)$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100015 \end{field}
 \begin{field}
  $X \sim (\mu,\sigma^2)$
  \begin{itemize}
   \item $E(\bar{X}) = $
   \item $V(\bar{X}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\bar{X}) = \mu$
   \item $V(\bar{X}) = \sigma^2/n$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100016 \end{field}
 \begin{field}
  Reject $H_0$ when $H_0$ True
 \end{field}
 \begin{field}
  Type I error (false positive)
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100017 \end{field}
 \begin{field}
  Type I error (false positive)
 \end{field}
 \begin{field}
  Reject $H_0$ when $H_0$ True
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100018 \end{field}
 \begin{field}
  Fail to Reject $H_0$ when $H_0$ false
 \end{field}
 \begin{field}
  Type II error
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100019 \end{field}
 \begin{field}
  Type II error
 \end{field}
 \begin{field}
  Fail to Reject $H_0$ when $H_0$ false
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100020 \end{field}
 \begin{field}
  Significance level
 \end{field}
 \begin{field}
  $\alpha$ the probability of a Type I error
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100021 \end{field}
 \begin{field}
  Power (at $\theta_1$)
 \end{field}
 \begin{field}
  Probability of rejecting the null hypothesis when $\theta_1$ is the truth
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100022 \end{field}
 \begin{field}
  Test for data setting: $X_1, X_2, \ldots X_n$ iid with sample mean $\bar{X}$, and known population variance $\sigma^2$, Null hypothesis $mu = \mu_0$

  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value
         \begin{itemize}
          \item Lower
          \item Upper
          \item Two sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  z-test
  \begin{itemize}
   \item Test statistic: $Z(\mu_0) = \frac{\bar{X} - \mu_0}{\sqrt{\sigma^2/n}}$
   \item Reference Distribution: Under $H_0, Z(\mu_0) \sim N(0,1)$
         \begin{itemize}
          \item Lower: Reject when $Z(\mu_0) < z_{\alpha}$ = qnorm($\alpha$)
          \item Upper: Reject when $Z(\mu_0) > z_{1 - \alpha} = $qnorm(1-$\alpha$)
          \item Two sided:  Reject when $|Z(\mu_0)| > z_{1 - \alpha/2}$ = qnorm(1 - $\alpha/2$)
         \end{itemize}
   \item Confidence interval: $ \bar{X} \pm z_{1 - \alpha/2}\sqrt{\frac{\sigma^2}{n}}$
   \item pvalue:
         \begin{itemize}
          \item upper: 1 - $\Phi(z)$ = 1 - pnorm(z)
          \item lower: $\Phi(z)$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item Consistent: Yes /Finite Sample Exact: Yes if $X_i \sim N$/ Asymptotically Exact: Yes
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100023 \end{field}
 \begin{field}
  Exactness (finite/asymptotic)
 \end{field}
 \begin{field}
  Under any setting for which the null hypothesis is true, is the actual rejection probability equal to the desired level $\alpha$?
  \begin{itemize}
   \item Finite Sample Exact: for sample size $n$ is $P(Reject H_0) = \alpha$ when $H_0$ is true?
   \item Asymptotic Exactness: As $n \to \infty$ does $P(Reject H_0) \to \alpha$ when $H_0$ is true?
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100024 \end{field}
 \begin{field}
  When is a test exact?
 \end{field}
 \begin{field}
  \begin{itemize}
   \item A test is FSE if the reference distribution is the true distribution of the test statistic $T$ when $H_0$ is true
   \item A test is AE if the reference distribution is the asymptotic distribution of the test statistic when $H_0$ is true.
   \item (Distribution of p-values should be Unif(0.1))
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100025 \end{field}
 \begin{field}
  Consistency
 \end{field}
 \begin{field}
  When $H_0$ is false (the alternative hypothesis is true), does the rejection probability (probability reject the null) tend to 1 as $n \to \infty$?
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100026 \end{field}
 \begin{field}
  Interpretation of Confidence intervals
 \end{field}
 \begin{field}
  $(1 - \alpha)100$\% of the time, intervals constructed in this manner will include $\mu$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100027 \end{field}
 \begin{field}
  Test for data setting: $X_1, X_2, \ldots X_n$ iid with sample mean $\bar{X}$, and unknown population variance , Null hypothesis $\mu = \mu_0$
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: t-test
   \item Test Statistic: $t(\mu_0) = \frac{\bar{X} - \mu_0}{\sqrt{s^2/n}}$
   \item Test Reference Distribution: $t_{n-1}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: Reject if $t(\mu_0) > t_{(n-1),1-\alpha}$ = qt(1 - $\alpha$,n-1)
          \item lower: Reject if $t(\mu_0)  < t_{n-1,\alpha}$
          \item two sided: Reject if $|t(\mu_0)| > t_{n-1, 1 - \alpha/2}$
         \end{itemize}
   \item Confidence interval: $\bar{X} \pm t_{n-1,1-\alpha/2}\sqrt{\frac{s^2}{n}}$
   \item pvalue, with $t(\mu_0)  = t$, and pt representing the cdf of a t distribution
         \begin{itemize}
          \item upper: 1 - pt($t,n-1$)
          \item lower: pt($t$,n-1)
          \item two-sided: 2*(1 - pt(abs(t)),n-1)
         \end{itemize}
   \item Consistent Yes/Finite Sample Exact Yes if normal/ Asymptotically Exact Yes
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100028 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$ iid Bernoulli(p) (option 1), parameter of interest $p$
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$ iid Bernoulli(p), parameter of interest $p$
  \begin{itemize}
   \item Test name: Exact Binomial Test (uses the distribution of the sum of Bern(p) RVs)
   \item Test Statistic: $X = \sum_{i=1}^n Y_i = n\bar{Y}$
   \item Test Reference Distribution: Under $H_0$ Binomial$(n,p_0)$
   \item Critical Value/ Rejection region: Sometimes use randomized test
         \begin{itemize}
          \item upper: Reject $H_0$ for $X \geq c$ for c such that $P(X \geq c)\leq \alpha$
          \item lower: Reject $H_0$ for $X \leq c$ for c such that $P(X \leq c)\leq \alpha$
          \item two-sided: Reject $H_0$ for $p_0(X)\leq c$ for $c$ such that$P_{H_0}(p_0(X) \leq c)\leq \alpha$, where $p_0(X)$ is $P(X = x)$ under $H_0$
         \end{itemize}
   \item Confidence interval: Values that are not rejected
   \item pvalue: Sum of the probabilities that are less than or equal to the observed value (under the null hypothesis)
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100029 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$, parameter of interest: $p$ iid Bernoulli(p) (option 2)
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$, parameter of interest: $p$ iid Bernoulli(p) (option 2)
  \begin{itemize}
   \item Test name: Binomial $z$-test (Use when $np_0 > 5$ and $n(1-p_0) > 5$)
   \item Test Statistic: $X  = \sum_{i=1}^n = n\bar{Y}$, $\hat{p} = X/n$, $z(p_0) = \frac{\hat{p}- p_0}{\sqrt{p_0(1-p_0)/n}}$ (score)
   \item Test Reference Distribution: Under $H_0$, Approximately $X \sim N(np_0,np_0(1-p_0))$ and $z(p_0) \sim N(0,1)$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: $z(p_0) > z_{1-\alpha}$
          \item lower: $z(p_0) < z_\alpha$
          \item two-sided: $|z(p_0)| > z_{1-\alpha/2}$
         \end{itemize}
   \item Confidence interval: Uses wald interval (derived from t-test) (with $z_w(p_0) = \frac{\hat{p} - p_0}{\sqrt{\hat{p}(1 - \hat{p})/n}}$) $\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}$
   \item pvalue
         \begin{itemize}
          \item upper: 1 - $\Phi(z(p_0))$ = 1 - pnorm($z(p_0)$)
          \item lower: $\Phi(z(p_0))$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z(p_0)|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item Consistent: Yes/Finite Sample Exact: No/ Asymptotically Exact: Yes
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100030 \end{field}
 \begin{field}
  Continuity correction for Binomial z-test
 \end{field}
 \begin{field}
  With $X \sim Binom(n,p)$, instead of $P(X \leq x)$, use $P(W \leq x + 1/2)$ where $W \sim N(np,np(1-p))$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100031 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$ (option 1)
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: Sign Test
   \item Test Statistic: $Y_i = I(X_i < M_0)$ , $\hat{p}_{M_0} = \frac{\sum Y_i}{n}$ (proportion of observations less than or equal to hypothesized median)
   \item Test Reference Distribution: Normal distribution: with $p_0 = .5$
   \item Critical Value/ Rejection region: $z = \frac{\hat{p}_{M_0} - p_0}{\sqrt{p_0(1-p_0)/n}}$
         \begin{itemize}
          \item upper: $z > z_{1-\alpha}$
          \item lower: $z < z_\alpha$
          \item two-sided: $|z| > z_{1-\alpha/2}$
         \end{itemize}
   \item Confidence interval: cant use the binomial proportion CI
         Set of values of $M_0$ that wouldn't be rejected at level $\alpha$

         $$ \bigg( \frac{n - z_{1 - \alpha/2 \sqrt{n}}}{2}\bigg)^{th} \text{ Smallest Observation}, \bigg( \frac{n - z_{1 - \alpha/2 \sqrt{n}}}{2}\bigg)^{th}\text{ Smallest Observation} $$
   \item pvalue (binomial test on proportion)
         \begin{itemize}
          \item upper: 1 - $\Phi(z(p_0))$ = 1 - pnorm($z(p_0)$)
          \item lower: $\Phi(z(p_0))$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z(p_0)|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item If there are ties: remove all observations equal to $M_0$, then test prop of observations $< M_0$ given not equal to $M_0$ is .5
   \item Consistent: yes/Finite Sample Exact: No / Asymptotically Exact: yes
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100032 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$ (option 2)
  \begin{itemize}
   \item Test name:
   \item Procedure:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$ (option 1)
  \begin{itemize}
   \item Test name: Wilcoxon signed-rank test (require symmetry assumption) - equivalently a test of the mean - Tests the pseudo-median
   \item Procedure: testing $c_0$ is the center (median)
         \begin{itemize}
          \item Calculate distance of each observation from $c_0$
          \item Rank observations by the distance (abs value) from $c_0$
         \end{itemize}
   \item Test Statistic: $S$ sum of the ranks that correspond to observations larger than $c_0$, $Z = \frac{S - \frac{n(n+1)}{4}}{\sqrt{\frac{n(n+1)(2n+1)}{24}}} \sim N(0,1)$
   \item Test Reference Distribution:
         \begin{itemize}
          \item Exact p-value - assume each rank has the same chance of being assigned to observations above or below $c_0$ - all possible ways to assign the ranks
          \item Normal approximation to the null distribution $S \sim N\big(\frac{n(n+1)}{4}, \frac{n(n+1)(2n+1)}{24}\big)$
         \end{itemize}
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue - Same as for Normal
   \item Consistent Yes under symmetry assumption /Finite Sample Exact No/ Asymptotically Exact Yes (under symmetry assumption)
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100033 \end{field}
 \begin{field}
  Pseudomedian
 \end{field}
 \begin{field}
  Median of the distribution of sample means from samples of size 2
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100034 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid N($\mu,\sigma^2$) parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid N($\mu,\sigma^2$) parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name: $\chi^2$ for Population Variance
   \item Test Statistic $X(\sigma_0) = \frac{(n-1)s^2}{\sigma_0^2}$
   \item Test Reference Distribution: Under $H_0: X(\sigma_0) = \frac{(n-1)s^2}{\sigma_0^2} \sim \chi_{n-1}^2$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item  $\sigma^2 > \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) > \chi^2_{n-1}(1-\alpha)$
          \item $\sigma^2 < \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) < \chi^2_{n-1}(\alpha)$
          \item $\sigma^2 \neq \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) > \chi^2_{n-1}(1 - \alpha/2)$ or $X(\sigma_0) < \chi^2_{n-1}(\alpha/2)$
         \end{itemize}
   \item Confidence interval  $$ \bigg( \frac{(n-1)s^2}{\chi^2_{n-1}(1 - \alpha/2)}, \frac{(n-1)s^2}{\chi^2_{n-1}(\alpha/2)}\bigg)$$
   \item pvalue
         \begin{itemize}
          \item $\sigma^2 > \sigma_0^2$: $p = 1 - pchisq(X(\sigma_0)^2,n-1)$
          \item $\sigma^2 < \sigma_0^2: p = pchisq(X(\sigma_0^2),n-1)$
          \item $\sigma^2 \neq \sigma_0^2: p = 2\min(1 - pchisq(X(\sigma_0^2), n-1), pchisq(X(\sigma_0^2)),n-1)$
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100035 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid \\
  Parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, \\
  $H_0: \sigma^2 = \sigma_0^2$ (asymptotic)
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item pvalue
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name: Asymptotic $t$-test for population variance
   \item Test Statistic: $Y = (X_i - \bar{X})^2$, $$ t(\sigma_0^2)  = \frac{\bar{Y - \frac{n-1}{n}\sigma_0^2}}{\sqrt{s_y^2/n}} \to N(0,1)$$
   Note $\bar{Y} = \frac{n-1}{n}s^2$
   \item Tests that the population mean of the $Y_i$ is $\frac{n-1}{n}\sigma_0^2$
   \item Test Reference Distribution $ \frac{\frac{n-1}{n}s^2 - \frac{n-1}{n}\sigma^2}{\sqrt{Var(\frac{n-1}{n}s^2)}} = \frac{\bar{Y}- \frac{n-1}{n}\sigma^2}{\sqrt{Var(\bar{Y})}} \to N(0,1)$, so we can use t-test
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: Reject if $t(\sigma_0^2) > t_{(n-1),1-\alpha}$ = qt(1 - $\alpha$,n-1)
          \item lower: Reject if $t(\sigma_0^2)  < t_{n-1,\alpha}$
          \item two sided: Reject if $|t(\sigma_0^2)| > t_{n-1, 1 - \alpha/2}$
         \end{itemize}
   \item Confidence interval: $\bar{X} \pm t_{n-1,1-\alpha/2}\sqrt{\frac{s^2}{n}}$
   \item pvalue, with $t(\mu_0)  = t$, and pt representing the cdf of a t distribution
         \begin{itemize}
          \item upper: 1 - pt($t,n-1$)
          \item lower: pt($t$,n-1)
          \item two-sided: 2*(1 - pt(abs(t)),n-1)
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100036 \end{field}
 \begin{field}
  Test for data setting $X_1, \ldots X_n$ iid from population distribution $F$. Test $H_0: F = F_0$
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $X_1, \ldots X_n$ iid from population distribution $F$. Test $H_0: F = F_0$
  \begin{itemize}
   \item Test name: Kolmogorov-Smirnov Test
   \item Process
   \item Test Statistic: $D(F_0) = sup_x|\hat{F}(x) - F_0(x)$, where $\hat{F}(x) = \frac{1}{n}\sum_{i=1}^n 1(X_i \leq x)$ is the empirical cdf and $F_0(x)$ is the null hypothesis cdf (maximum values of difference between emperical and null)
   \item Test Reference Distribution: Kolmogorov distribution
   \item Critical Value/ Rejection region: Reject for large values of $\sqrt{n}D(F_0)$
   \item Note the one sided version does not have an easy interpretation
  \end{itemize}
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100037 \end{field}
 \begin{field}
  Data setting: $X_1, \ldots , X_n$ iid from discrete distribution. Test fit of distribution
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: $X_1, \ldots , X_n$ iid from discrete distribution. Test fit of distribution
  \begin{itemize}
   \item Test name: $\chi^2$ goodness of fit test, test for discrete distributions
   \item Process: Test the underlying population distribution is $P(X = x) = p_0(x)$, where $\hat{p}(x) = \frac{1}{n} \sum_{i=1}^n 1(X_i = x)$
         \begin{itemize}
          \item Let $j = 1, \ldots, k$ the different categories that $X_i$ can take
          \item Let $O_j$ be the observed number of observations that belong to category $j$
          \item Let $E_j = np_0(j)$ be the expected number of observations that would belong to category $j$ if the null hypothesis were true
         \end{itemize}
   \item Test Statistic: $X(p_0) = \sum_x\frac{n(\hat{p}(x) - p_0(x))^2}{p_0(x)} = \sum_{j=1}^k \frac{(O_j - E_j)^2}{E_j}$
   \item Test Reference Distribution: Under $H_0$, $X(p_0) \to \chi_{k-1}^2$
   \item Critical Value/ Rejection region: Reject for large values of $X(p_0)$ - Reject $H_0$ for $X(p_0) > \chi_{k-1}^2(1-\alpha)$
   \item Note: Null hypothesis doesnt completely specify the distribution, just the family of distributions with perhaps unknown parameters
         \begin{itemize}
          \item Estimate the parameters
          \item Use null distribution with estimated parameter values for $E_j$
          \item Compute $\chi^2$ test statistic
          \item Compare to $\chi_{k-d-1}^2$ distribution where $k$ = number of categories, $d = $ number of estimated parameters
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100038 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with known $\sigma_x, \sigma_y$. Estimate $d$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item p-value
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with known $\sigma_x, \sigma_y$. Estimate $d$,
  \begin{itemize}
   \item Test name: 2 sample $z$ test
   \item Test Statistic: $z(d_0) = \frac{(\bar{X}- \bar{Y}) - d_0}{\sqrt{\frac{\sigma_x^2}{m} - \frac{\sigma_y^2}{n}}}$
   \item Test Reference Distribution: Under $H_0$, $z(d_0) \sim N(0,1)$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item Lower: $d \leq d_0$ Reject when $z(d_0) < z_{\alpha}$ = qnorm($\alpha$)
          \item Upper: $d \geq d_0$ Reject when $z(d_0) > z_{1 - \alpha} = $qnorm(1-$\alpha$)
          \item Two sided: $d \neq d_0$ Reject when $|z(d_0)| > z_{1 - \alpha/2}$ = qnorm(1 - $\alpha/2$)
         \end{itemize}
   \item Confidence interval: $$ (\bar{X} - \bar{Y}) \pm z(1 - \frac{\alpha}{2})\sqrt{\frac{\sigma_x^2}{m} + \frac{\sigma_y^2}{n}} $$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100039 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with unknown but equal  $\sigma_x, \sigma_y$ Estimate $d$
  \begin{itemize}
   \item Test name:
   \item Estimate of $\sigma_x^2 = \sigma_y^2$
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item When not equal
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with unknown $\sigma_x, \sigma_y$. Estimate $d$
  \begin{itemize}
   \item Test name: Equal variance 2-sample t-test
   \item Note: Estimate of $\sigma_x^2 = \sigma_y^2 = s_p^2 = \frac{\sum_{i=1}^m (X_i - \bar{X})^2 + \sum_{i=1}^n (Y_i - \bar{Y})}{(m-1) + (n-1)} = \frac{(m-1)s_x^2 + (n-1)s_y^2}{(m+n-2)}$ (weighted average of the two sample variances )
   \item Test Statistic: $t(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{s_p^2(\frac{1}{m} + \frac{1}{n})}}$
   \item Test Reference Distribution: For Normal populations, under $H_0$: $t(d_0) \sim t_{m+n-2}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item $d > d_0$ Reject $H_0$ for $t_e(d_0) > t_{m+n-2}(1 - \alpha)$
          \item $d < d_0$ Reject $H_0$ for $t_e(d_0) < t_{m+n-2}(\alpha)$
          \item $d \neq d_0$ Reject $H_0$ for $|t_e(d_0)| > t_{m+n-2}(1 - \alpha/2)$
         \end{itemize}
   \item Confidence interval $ (\bar{X} - \bar{Y}) \pm t_{m+n-2}(1 - \frac{\alpha}{2})\sqrt{s_p^2(\frac{1}{m} + \frac{1}{n})}$
   \item When not equal:
         \begin{itemize}
          \item Expected value of Estimated variance is larger than it should be when the smaller sample comes from the population with smaller variance - the test statistic will be closer to zero than it should be, and rejection rates will be smaller - Less power - more conservative
          \item Expected value of Estimated variance is smaller than it shoudl be when smaller sample comes from the population with the larger variance - test statistic will have a larger absolute value than it should an rejection rates will be larger  - more power - anti conservative
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100040 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with unknown not equal equal  $\sigma_x, \sigma_y$ Estimate $d$
  \begin{itemize}
   \item Test name:
   \item Estimate of $Var(\bar{X} - \bar{Y})$
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item Compare to equal variance
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with unknown not equal equal  $\sigma_x, \sigma_y$ Estimate $d$
  \begin{itemize}
   \item Test name: Unequal variance 2 sample t-test
   \item Estimate of $Var(\bar{X} - \bar{Y}) = \frac{s_x^2}{m} + \frac{s_y^2}{n}$
   \item Test Statistic: $t_U(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{s_x^2}{m} + \frac{s_y^2}{n}}}$
   \item Test Reference Distribution: If the two distributions are Normal, there is not an exact distribution for the test statistic -  Use Welch-Satterthwaite approximation: Estimate degrees of freedom
         $$ v = \frac{\big(\frac{s_x^2}{m} + \frac{s_y^2}{n}\big)^2}{\frac{s_x^4}{m^2(m-1)} + \frac{s_Y^4}{n^2(n-1)}}$$
         $\min(m-1,n-1) \leq v \leq m+n-2$
         Under $H_0$ $t_u(d_0) $ approx $\sim t_{v}$
   \item Critical Value/ Rejection region: same as t-test
   \item Confidence interval: $(\bar{X} - \bar{Y}) \pm t_v(1 - \frac{\alpha}{2})\sqrt{\frac{s_x^2}{m} + \frac{s_Y^2}{n}}$
   \item Compare to equal variance:
         \begin{itemize}
          \item For unequal sample sizes with unequal population variances, equal variance t-test does not have correct calibration
          \item When samples sizes are equal both test statistics are the same, but degrees of freedom differ
          \item When equal variance assumption is true, equal variance has slightly better power, and very slightly better calibration (more exact )
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100041 \end{field}
 \begin{field}
   Data setting $X_1, \ldots , X_n$ iid $F_x$,\\
    $Y_1, \ldots, Y_n$ iid $F_y$,\\
     $X_i$ not independent $Y_i$,\\
      $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$. \\
      Estimate $d = \mu_x - \mu_y$, when $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ known
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$,\\
   $Y_1, \ldots, Y_n$ iid $F_y$,\\
    $X_i$ not independent $Y_i$,\\
     $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$. \\
     Estimate $d = \mu_x - \mu_y$, when $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ known
  \begin{itemize}
   \item Test name: Paired z-test
   \item Test Statistic: $z(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{\sigma_x^2}{n} + \frac{\sigma_Y^2}{n} - 2 \frac{\sigma_{XY}}{n}}} = \frac{\bar{D} - d_0}{\sqrt{\frac{\sigma_D^2}{n}}}$
   \item Test Reference Distribution: Under $H_0$, $z(d_0) $ aprox $\sim N(0,1)$
   \item Critical Value/ Rejection region: Same as normal
   \item Confidence interval : $$(\bar{X} - \bar{Y}) \pm z(1 - \frac{\alpha}{2})\sqrt{\frac{\sigma_x^2}{n} + \frac{\sigma_Y^2}{n} - 2 \frac{\sigma_{XY}}{n}} = \bar{D} \pm z(1 - \alpha/2) \sqrt{\frac{\sigma_D^2}{n}}$$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100042 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$, $X_i$ not independent $Y_i$, $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$ Estimate $d = \mu_x - \mu_y$, $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ unknown
  \begin{itemize}
   \item Test name:
   \item Estimate of $\sigma_{XY}$
   \item Estimate of $Var(\bar{X} - \bar{Y})$
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$, $X_i$ not independent $Y_i$, $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$ Estimate $d = \mu_x - \mu_y$, $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ unknown
  \begin{itemize}
   \item Test name: Paired Data t-test
   \item Estimate of $\sigma_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})$
   \item Estimate of $Var(\bar{X} - \bar{Y}) = \frac{s_d^2}{n} = \frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}$
   \item Test Statistic: $t(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}}} = \frac{\bar{D} - d_0}{\sqrt{\frac{s_D^2}{n}}}$
   \item Test Reference Distribution: If differences are Normal (note X,Y Normal does not imply Differences are normal unless X,Y are jointly multivariate-normal) Under $H_0$, $t(d_0) \sim t_{n-1}$ (exact distribution)
   \item Critical Value/ Rejection region Same as t
   \item Confidence interval
         $$ (\bar{X} - \bar{Y}) = t_{n-1}(1 - \frac{\alpha}{2})\sqrt{\frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}} = \bar{D} \pm  t_{n-1}(1 - \frac{\alpha}{2}) \sqrt{\frac{s_d^2}{n}}$$
   \item Equivalent to a one sample - t-test on the differences
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100043 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$
  \begin{itemize}
   \item Test name: Binomial proportions two-sample z-test
   \item Test Statistic: $$ z = \frac{\hat{p}_x - \hat{p}_y}{\sqrt{\hat{p}_c(1 - \hat{p}_c(\frac{1}{m} + \frac{1}{n}))}} $$ Where $\hat{p_c} = \frac{m\hat{p}_x + n\hat{p}_y}{m+n} = \frac{b + d}{N}$
   \item Test Reference Distribution: Under $H_0: z $ approx $\sim N(0,1)$
   \item Critical Value/ Rejection region: Same as regular 2-sample
   \item Confidence interval: $$  \hat{p}_x - \hat{p}_y \pm z_{1 - \alpha/2} \sqrt{\big(\frac{\hat{p}_x(1 - \hat{p}_x)}{m} + \frac{\hat{p}_y(1 - \hat{p}_y)}{n}\big)}$$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100044 \end{field}
 \begin{field}
  Multinomial sampling
 \end{field}
 \begin{field}
  Collection of random samples, recording what group they are in: Can estimate $P(X = x | G = g)$, where $G$ is the group
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100045 \end{field}
 \begin{field}
  Two-Sample Binomial sampling
 \end{field}
 \begin{field}
  Sample $m$ units from group 1 and $n $ units from group 2
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100046 \end{field}
 \begin{field}
  Can we estimate $P(X = x | G = g)$ with binomial sampling
 \end{field}
 \begin{field}
  Cannot estimate
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100047 \end{field}
 \begin{field}
  $P(X = x | G = g)$ with multinomial sampling
 \end{field}
 \begin{field}
  Can estimate
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100048 \end{field}
 \begin{field}
  $E(g(T)) = $
 \end{field}
 \begin{field}
  $E(g(T)) \neq g(E(T))$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100049 \end{field}
 \begin{field}
  Reason for performing transformations on data
 \end{field}
 \begin{field}
  Some tests are FSE only when population distribution is Normal (otherwise the methods are asymptotically exact), requiring a large $n$. Transformations that improve approximation of normality make Normal-based methods perform more exactly
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100050 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$ (Association/independent/relationship)
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$ (Association/independent/relationship)
  \begin{itemize}
   \item Test name: Pearson's Chi-squared Test
   \item Test Statistic: $X = \sum_{i,j \in \{1,2\}} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ Where $O_{ij} = n_{ij}$ and $E_{ij} = \frac{R_iC_j}{N}$
   \item Test Reference Distribution: Under $H_0$ $X \sim \chi^2_1$
   \item Critical Value/ Rejection region: Reject for $X > \chi_1^2(1 - \alpha)$
   \item Note: Equal to to sided z-test for binomial proportions: $X = z^2$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100051 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ (No association between response variable $X$ and grouping variable $G$)
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item pvalue
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ (No association between response variable $X$ and grouping variable $G$)
  \begin{itemize}
   \item Test name: Fisher's Exact Test (of homogeneity of proportions)
   \item Test Statistic: Probability of observed table conditioning on margins: Compute all tables with the same margin totals: $\frac{\binom{C_2}{O_{12}}\binom{C_1}{O_{11}}}{\binom{N}{R_1}}$
   \item pvalue: Sum of probability of all tables more extreme than observed table
         More Extreme:
         \begin{itemize}
          \item $p_x > p_y$ More extreme = larger $O_{12}$
          \item $p_x < p_y$ More extreme = smaller $O_{12}$
          \item $p_x \neq p_y$ More extreme = less likely table
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100052 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ Binomial sampling scheme
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ Binomial sampling scheme
  \begin{itemize}
   \item Test name: Log Odds - test $H_0: \omega = 1$
   \item Test Statistic: $\hat{\omega} = \frac{ad}{bc}$, $z = \frac{\log(\hat{omega})}{\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}$
   \item Test Reference Distribution $ \log(\hat{\omega}) $ approx $\sim N(\log(\omega), \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d})$, $z $ approx $\sim N(0,1)$
   \item Critical Value/ Rejection region
   \item Confidence interval $(\hat{omega}e^{-z(1 - \frac{\alpha}{2})\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}, \hat{omega}e^{z(1 - \frac{\alpha}{2})\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}})$
   \item: $\omega > 1, p_1 > p_2$, $\omega = 1, p_1 = p_2$, small $p_1, p_2$, $\omega = p_1/p_2$ = relative risk
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100053 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, $X,Y$ not independent (paired) test proportions equal in groups (equally likely/probability)
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, $X,Y$ not independent (paired), test proportions equal in groups (equally likely/probability)
  \begin{itemize}

   \item Note, requires a table that keeps track of the pairs
   \begin{tabular}{|c|c c|c|}
     & Measurement 1 & & \\
     Measurement 2 & No & Yes & Total \\
     \hline\\
     No & a & b & $R_1$\\
     Yes & c & d & $R_2$\\
     \hline \\
     Total & $C_1$ & $C_2$ & n\\
     \hline
  \end{tabular}
   \item Test name: McNemar's Test
   \item Test Statistic: $z = \frac{b-c}{\sqrt{b+c}}$
   \item Test Reference Distribution: $z \sim N(0,1)$, $z^2 \sim \chi_1^2$
   \item Critical Value/ Rejection region: Two sided rejecct $|z| > z(1 - \alpha/2)$
   \item Note equivalent to performing a paired t-test on the differences:
         $$ t = \frac{b-c}{\sqrt{\frac{n}{n-1}(b + c - \frac{(b-c)^2}{40})}} $$ compare to $t_{n-1}$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100054 \end{field}
 \begin{field}
  Data setting: $n$ observations, record Group 1 and Group 2, where each group takes on $> 2$ values, Test if there is an association between the groups
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: $n$ observations, record Group 1 ($r$ values) and Group 2$(c)$ values, Test if there is an association between the groups
  \begin{itemize}
   \item Test name: Pearsons $\chi^2$
   \item Test Statistic: $X = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$, where $E_{ij} = \frac{n_in_j}{N}$
   \item Test Reference Distribution: Under $H_0$, $X$ approx $\sim \chi^2_{(r-1)(c-1)}$
   \item Note not FSE, but performance is good if $E_{ij} > 5$
  \end{itemize}
 \end{field}
\end{note}




\begin{note} \begin{field} \tiny 100055 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name:
   \item Assumptions
   \item Process
   \item pvalue
   \item Test Reference Distribution
   \item Test Statistic:
   \item Consistency
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name: Wilcoxon Rank-Sum (Mann-Whitney U-test)
   \item Note this is only a test of medians only if just additive effect - G1 is just a shift from G2 (shame and scale must be same ) (but then just the same as a test of mean, 10th percentile, min, $F_x = F_y$ etc )
   \item If No additive assumption - test of $H_0: P(X > Y) = .5$
   \item Process:
         \begin{itemize}
          \item Combine samples
          \item Rank the observations in combined sample from smalles to largest (1 to $n + m$)
          \item Add ranks of the smaller group
         \end{itemize}
   \item pvalue: Calculate using permutations: Count number of permutations that lead to a R value more extreme than observed out of total permutations ($\binom{n+m}{m}$)
   \item Test Statistic: $R$ sum of the ranks, or $z = \frac{R - \frac{m(m+n+1)}{2}}{\sqrt{\frac{mn(m+n+1)}{12}}}$
   \item Test Reference Distribution: If there was no difference between two populations, then each rank has equal chance of being assigned to group 1 (belongs to $X$: $p = \frac{m}{n+m}$)
         Normal approximation: $R \dot\sim N( \frac{m(m+n+1)}{2}, \frac{mn(m+n+1)}{12})$, $z \dot\sim N(0,1)$
   \item Notes: If ties, assign ranks, and then average ranks of tied values
   \item Continuity correction to normal distribution: add .5 to R if lower probability, subtract .5 from R if upper probability (ie 1 - pnorm())
   \item Not consistent test unless under additive assumption. IS consistent test of $H_0: P(X > Y) = .5$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100056 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test statistic:
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name: Mood's Test for Equality of Population Medians
   \item Process:
         \begin{itemize}
          \item Find combined sample median $\hat{m}$
          \item Calculate $\hat{p}_x = $ proportion of $X$s greater than $\hat{m}$, $\hat{p}_y$, proportion of $Ys$ greater than $\hat{m}$
          \item Conduct two sample binomial z-test( Pearsons chi-squared test) or Fisher's exact test
          \item  Test statistic: $$ z = \frac{\hat{p}_x - \hat{p}_y}{\sqrt{\hat{p}_c(1 - \hat{p}_c(\frac{1}{m} + \frac{1}{n}))}} $$ Where $\hat{p_c} = \frac{m\hat{p}_x + n\hat{p}_y}{m+n} = \frac{b + d}{N}$
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100057 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test some statistic $W$
  \begin{itemize}
   \item Test name:
   \item Process
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test some statistic $W$
  \begin{itemize}
   \item Test name: Permutation test
   \item Process: Permute group labels across observations and recalculate statistic for each permutation to create permutation distribution - calculate p-values using the permutation distribution
   \item Performance: Many settings (like medians equal), will not reject correctly (even in large samples) if the medians are equal, but the distributions differ
   \item Permutation hypothesis is that the observations from the two pouplations are exchangable (ie same population distributions, not just equal medians )
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100058 \end{field}
 \begin{field}
  Data setting: Estimate value of nuisance parameter
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: Bootstrap
   \item Process: Since the empirical distribution function converges to the true distribution function, we can use samples from the empirical distribution to approximate how samples from the true distribution would behave.
   \item Confidence interval: $100 (\alpha/2)$ largest resampled statistic $100(1 - (\alpha/2))$ largest resampled statistic
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100059 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $N$, $Y_1, \ldots, Y_n$ iid $N$. $H_0: \sigma_x^2 = \sigma_y^2$ or $H_0 \sigma_x^2/\sigma_y^2 = r$
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$ or $H_0 \sigma_x^2/\sigma_y^2 = r$
  \begin{itemize}
   \item Test name: F
   \item Recall $s_x^2 = \frac{1}{n-1}\sum_{i=1}^m(X_i - \bar{X})^2$
   \item Note that $\frac{(m-1)s_x^2}{\sigma_x^2} \sim \chi^2_{m-1}, \frac{(n-1)s_y^2}{\sigma_y^2} \sim \chi^2_{n-1},$
   \item Test Statistic: $F(r) = \frac{s_x^2/\sigma_x^2}{s_y^2/\sigma_y^2} = \frac{s_x^2}{s_y^2} \frac{1}{r}$
   \item Test Reference Distribution: Under $H_0: F(r) \sim F_{m-1,n-1}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item $\sigma_x^2/\sigma_y^2 > r$ Reject for $F(r) > F_{m-1,n-1}(1-\alpha)$
          \item $\sigma_x^2/\sigma_y^2 > r$ Reject for $F(r) > F_{m-1,n-1}(\alpha)$
          \item $\sigma_x^2/\sigma_y^2 \neq r$ Reject for $F(r) > F_{m-1,n-1}(1-\alpha/2)$ or $F(r) < F_{m-1,n-1}(\alpha/2)$
         \end{itemize}
   \item Performance: Not Well if underlying population is not normal: Not FSE or AE (but is consistent ) - don't use if population is not normal
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100060 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$
  \begin{itemize}
   \item Test name:
   \item Process:
   \item Interpretation
   \item Assumptions
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$
  \begin{itemize}
   \item Test name: Levene's Test
   \item Process:
         \begin{itemize}
          \item Construct new variables:
                \begin{itemize}
                 \item $U_i = |X_i - med(X)|$ or $(X_i - med(X))^2$ or $|X_i - \bar{X}|$ or $(X_i - \bar{X})^2$
                 \item $V_i = |Y_i - med(Y)|$ or $(Y_i - med(Y))^2$ or $|Y_i - \bar{Y}|$ or $(Y_i - \bar{Y})^2$
                \end{itemize}
          \item Perform two-sample $t$ test on $U_i$ and $V_i$ (use Welch)
         \end{itemize}
   \item Interpretation: If last option used, can be a test in difference in population variances
   \item Assumptions:
         \begin{itemize}
          \item Independence
          \item Large sample sizes, so t-test assumptions are met
         \end{itemize}
   \item Note: dont use as a test to determine which t-test version to use
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100061 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $H_0: F_x = F_y$
  \begin{itemize}
   \item Test name
   \item Test statistic

  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $H_0: F_x = F_y$
  \begin{itemize}
   \item Test name: Two-sample Kolmogorov-Smirnov Test
   \item Test statistic: $D = sup_x|\hat{F}_x(x) - \hat{F}_y(y)|$ ie the largest distance between the empirical CDF for $X$ and $Y$
   \item Reject for large values of $\sqrt{\frac{mn}{m+n}}$
   \item Only for continuous distributions, for discrete distributions, use Pearsons $\chi^2$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100062 \end{field}
 \begin{field}
  Multiple 2x2 tables under $k$ different conditions $p_{xj} = P(X = 1 \text{ in Table }j), p_{yj} = P(Y = 1 \text{ in Table }j)$ $H_0: p_{xj} = p_{yj}$ for all $j$
 \end{field}

 \begin{field}
  \begin{itemize}
   \item Test name: Mantel-Haenszel Test
   \item Test statistic: $\omega_j = \frac{p_{xj}(1 - p_{xj})}{p_{yj}(1 - p_{yj})}$, $H_0: \omega_j = 1$ for all $j$
         $$E(n_{X1j}) = \mu_{X1j} = \frac{n_{X\cdot j}n_{\cdot 1 j}}{n_{\cdot j}}, V(n_{X1j}) = \sigma^2_{X1j} = \frac{n_{X\cdot j}n_{Y\cdot j}n_{\cdot 1j} n_{\cdot 0j}}{n^2_{\cdot \cdot j}(n_{\cdot \cdot j} -1)} $$
         $$ C = \frac{[\sum_{j}(n_{X1j} - \mu_{X1j})]^2}{\sum_j \sigma^2_{X1j}}$$
   \item Under $H_0$ $C \dot\sim \chi^2(1)$
   \item Assumes the odds-ratios are the same in all $k$ tables
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100063 \end{field}
 \begin{field}
  Test for data setting: Sample 1: $X_{1,1}, \ldots , X_{1n_1}$ from population 1 with mean $\mu_1$,\\
   Sample 2: $X_{2,1}, \ldots , X_{2n_2}$ from population 2 with mean $\mu_2$,\\ $\ldots $ Sample M: $X_{M,1}, \ldots , X_{Mn_M}$ from population M with mean $\mu_M$
  \begin{itemize}
   \item Independence within and between groups
   \item Populations (approximately ) normal
   \item Equal variances
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: ANOVA
   \item Estimate of common variance $s_p = \frac{(n_1-1)s_1^2 + \cdots + (n_M -1)s_M^2}{(n_1-1) + \cdots + (n_M-1)}$
   \item Could use two-sample-t test on two population means
   \item Could test are population means 1 through M equal to each other?
   \item Compare the variability between groups to the variability withing groups
   \item Sum of squares within groups:
         $$ SSW = (n-M)s_p^2  = \sum_{i=1}^{n_1}(X_{1i} - \bar{X}_1)^2 + \cdots +  \sum_{i=1}^{n_M}(X_{Mi} - \bar{X}_M)^2$$
         degrees of freedom: $n-M$
   \item Sum of squares total
         $$ SST  = \sum_{i=1}^{n_1}(X_{1,i} - \bar{X})^2 + \cdots + \sum_{i=1}^{n_M} (X_{M,i} - \bar{X})^2$$
         degrees of freedom: $n-1$
   \item Sum of squares between groups: $ SSB = SST - SSW = \sum_{j=1}^Mn_j(\bar{X}_j - \bar{X})^2$ df: $(n-1) - (n-M) = M-1$
   \item Test statistic: $$ F = \frac{MSB}{MSW} = \frac{SSB/(M-1)}{SSW/(n-M)} $$
   \item Reference distribution: Under $H_0, F \sim F_{M-1, n-M}$
  \end{itemize}
 \end{field}
\end{note}

%%end_tag

%%start_tag Methods 2
\tags{Methods2}

\begin{note} \begin{field} \tiny 100064 \end{field}
  \begin{field}
    Vectors $\mathbf{x}$ and $\mathbf{y}$ orthogonal
  \end{field}
  \begin{field}
    Vectors $\mathbf{x}$ and $\mathbf{y}$ orthogonal (perpendicular) if $(x,y) = \mathbf{x}^t \mathbf{y} = 0$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100065 \end{field}
  \begin{field}
    A matrix $\mathbf{A}$ is orthogonal if:
  \end{field}
  \begin{field}
    A matrix $\mathbf{A}$ is orthogonal if $\mathbf{A}^t \mathbf{A} = \mathbf{A} \mathbf{A}^t = \mathbf{I}_n$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100066 \end{field}
  \begin{field}
    A set of $n$ vectors are linearly dependent
  \end{field}
  \begin{field}
    A set of $n$ vectors are linearly dependent if there exist constants $c_1, \ldots c_n$ not all 0 such that $\sum_{j=1}^n c_j \mathbf{x})j = 0$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100067 \end{field}
  \begin{field}
    Inverse of a square matrix: $\mathbf{A}_{n\times n}$
  \end{field}
  \begin{field}

  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100068 \end{field}
  \begin{field}
    Inverse of $\mathbf{A}$, $\mathbf{A}^{-1}$ where $\mathbf{A}$ is $2 \times 2$
  \end{field}
  \begin{field}
    $\mathbf{A}^{-1} = \begin{pmatrix}
      a & b \\ c & d
    \end{pmatrix}^{-1} = \frac{1}{ad - bc}\begin{pmatrix}
      d & -b \\ -c & a
    \end{pmatrix}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100069 \end{field}
  \begin{field}
    A square matrix is invertible if:
  \end{field}
  \begin{field}
    A square matrix is invertible if the columns (rows) are linearly independent. (If the columns are not independent, the matrix is called singular)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100071 \end{field}
  \begin{field}
    Square of matrix $\mathbf{A}$
  \end{field}
  \begin{field}
    $\mathbf{AA}^t$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100072 \end{field}
  \begin{field}
    Norm of a vector $|\mathbf{x}|$
  \end{field}
  \begin{field}
    $|\mathbf{x}| = \sqrt{\sum_{j=1}^p x_j^2}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100073 \end{field}
  \begin{field}
    Determinant of a $2\times 2 $ matrix
  \end{field}
  \begin{field}
    $\bigg| \begin{pmatrix}
      a & b \\ c & d
    \end{pmatrix}\bigg| = ad - bc $
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100074 \end{field}
  \begin{field}
    Trace of a square matrix
  \end{field}
  \begin{field}
    Sum of the diagonal elements
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100075 \end{field}
  \begin{field}
    Rank of a matrix
  \end{field}
  \begin{field}
    Number of linearly independent columns
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100076 \end{field}
  \begin{field}
    Eigenvalue and eigenvector
  \end{field}
    \begin{field}
      $\lambda $ is an eigen value and  $\mathbf{u}_{n \times 2}$ is the eigen vector of $\mathbf{A}_{n \times n}$ if $\mathbf{Au} = \lambda \mathbf{u}$
      \begin{itemize}
        \item A real symmetric matrix has $n$ eigen values  and $n$ eigen vectors, and each are orthogonal to each other
        \item Roots of $det(\mathbf{A} - \lambda  \mathbf{I})$ determine the eigenvalues of $A$
      \end{itemize}
    \end{field}
\end{note}

% Come back to lecture 1 for more linear algebra notes

\begin{note} \begin{field} \tiny 100077 \end{field}
  \begin{field}
    Matrix properties
    \begin{itemize}
      \item $(AB)^{t} = $
      \item $(A+B)^t = $
      \item For invertible matrices $(AB)^{-1} = $
      \item For invertible matrices $(\mathbf{A}^{-1})^t = $
    \end{itemize}
  \end{field}
  \begin{field}
    Matrix properties
    \begin{itemize}
      \item $(AB)^{t} = B^tA^t$
      \item $(A+B)^t = A^t + B^t$
      \item For invertible matrices $(AB)^{-1} = B^{-1}A^{-1}$
      \item For invertible matrices $(\mathbf{A}^{-1})^t = (\mathbf{A}^t)^{-1}$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100078 \end{field}
  \begin{field}

  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100079 \end{field}
  \begin{field}
    $$ E(Y_i | X_{i1}, \ldots , X_{ip}) = $$
    Where $Y_i$ is the $i$th response and $X_{ij}$ is the $i$th value of the $j$th predictor
  \end{field}
  \begin{field}
    Since the error terms $\epsilon_i$ are independent and normally distributed with mean 0,
    $$ E(Y_i | X_{i1}, \ldots , X_{ip}) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_pX_{ip}$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100080 \end{field}
  \begin{field}
    Matrix form of linear Model and data
  \end{field}
  \begin{field}
    $$ \begin{pmatrix}
      Y_1 \\ Y_2 \\ \vdots \\ Y_n
    \end{pmatrix}  = \begin{pmatrix}
      1 & X_{11} & X_{12} & \cdots & X_{1p}\\
      1 & X_{21} & X_{22} & \cdots & X_{2p}\\
      \vdots & \vdots & \vdots & & \vdots \\
      1 & X_{n1} & X_{n2} & \cdots & X_{np}
    \end{pmatrix} \begin{pmatrix}
      \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
    \end{pmatrix} + \begin{pmatrix}
      \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n
    \end{pmatrix}$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100081 \end{field}
  \begin{field}
    Assumptions of a linear model
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Linearity: $E(\epsilon_i) = 0$ or $E(\epsilon) = \mathbf{0}$ or $E(\mathbf{Y}) = \mathbf{X}\beta$
      \item Constant variance $V(Y_i) = \sigma^2 = Var(\epsilon_i)$ or $V(\epsilon) = \sigma^2 \mathbf{I}_n$
      \item Normality $Y_i$ follows normal distribution, equivalently, $\epsilon_i$ follows normal distribution
      \item Independence $Y_i$ are indepednent equivalently under normality $Cov(\epsilon_i, \epsilon_j) = 0$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100082 \end{field}
  \begin{field}
    Interpretation of intercept of linear model
  \end{field}
  \begin{field}
    Mean response when all explanatory variables are 0
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100083 \end{field}
  \begin{field}
    Interpretation of slopes of linear model
  \end{field}
  \begin{field}
    Change in mean response for 1 unit change in the value of the explanatory, keeping all other variables constant. When $p = 2$
    $$ E(Y|X_1 + 1, X_2) - E(Y|X_1,X_2) = \beta_1$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100084 \end{field}
  \begin{field}
    Reason for $g-1$ indicator variables for a variable with $g$ values
  \end{field}
  \begin{field}
    The model matrix $X_{n \times(p+1)}$ needs to be full column rank - $\mathbf{X}^t \mathbf{X}$ needs to be non-singular
    If there is no intercept, we can include all groups, but interpretation will be different
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100085 \end{field}
  \begin{field}
    Interpretation of slope coefficient for indicator variable $\beta$
  \end{field}
  \begin{field}
    Difference in expected value of $Y$ between group value $a$ and $b$ where $a$ is the associated value for $\beta_j$ and $b$ is the base category
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100086 \end{field}
  \begin{field}
    \begin{itemize}
      \item $E(\textbf{AU} + \textbf{b}) = $
      \item $V(\mathbf{AU + \mathbf{B}}) = $
    \end{itemize}
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $E(\textbf{AU} + \textbf{b}) = \mathbf{A} E(\mathbf{U}) + \mathbf{b}$
      \item $V(\mathbf{AU + \mathbf{B}}) = \mathbf{A}V(\mathbf{U}+ \mathbf{A}^t$
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100087 \end{field}
  \begin{field}
    Least squares estimate of $\beta$ (process to find )
  \end{field}
  \begin{field}
    Minimize the squared error loss ($L(\beta)$) with respect to $\beta$

      $$ L(\beta ) = \sum_{i=1}^n Y_i - (\beta_0 + \beta_1 X_{i1} + \cdots + \beta_p X_{ip}))^2 = (\mathbf{Y} - \mathbf{X}\beta)^t(\mathbf{Y} - \mathbf{X}\beta)$$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100088 \end{field}
  \begin{field}
    $$  \frac{\partial}{\partial \beta} L(\beta) = $$
  \end{field}
  \begin{field}
    \begin{align*}
       \frac{\partial}{\partial \beta} L(\beta)  &= \frac{\partial}{\partial \beta} (\mathbf{Y} - \mathbf{X}\beta)^t(\mathbf{Y} - \mathbf{X}\beta)\\
       &= \frac{\partial}{\partial \beta}  \mathbf{Y}^t \mathbf{Y} - \beta^t \mathbf{X}^t
\textbf{Y} - \mathbf{Y}^t \mathbf{X}\beta - \beta^t \mathbf{X}^t \mathbf{X} \beta \\
&= 0 - \mathbf{X}^t \mathbf{Y} - \mathbf{X}^t \mathbf{Y} + 2 \mathbf{X}^t \mathbf{X} \beta  \\
\mathbf{X^t}\mathbf{X}\beta = \mathbf{X}^t \mathbf{Y}
    \end{align*}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100089 \end{field}
  \begin{field}
    Least squares estimate of $\hat{\beta}$
  \end{field}
  \begin{field}
    $$ \hat{\beta} = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y} $$ (if $\mathbf{X}^t \mathbf{X}$ is invertible )
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100090 \end{field}
  \begin{field}
    Residual
  \end{field}
  \begin{field}
    $e_i = Y_i - \hat{Y}_i$, $\mathbf{e}_{n\times 1} = \mathbf{Y} - \hat{\mathbf{Y}}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100091 \end{field}
  \begin{field}
    Vector of fitted values
  \end{field}
  \begin{field}
    $\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta} = \mathbf{X} (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100092 \end{field}
  \begin{field}
    Projection matrix
  \end{field}
  \begin{field}
    Hat matrix
    $$\mathbf{H_{n \times n}} = \mathbf{X} (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100093 \end{field}
  \begin{field}
    Properties of projection matrix
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $H$ and $\mathbf{I} - \mathbf{H}$ are symmetric matrices
      \item $\mathbf{HX} = X$
      item $(\mathbf{I} - \mathbf{X})\mathbf{X} = \mathbf{0}$
      \item $\mathbf{H}^2 = \mathbf{H}$
      \item $(\mathbf{I} - \mathbf{H})\mathbf{H} = 0$
      \item $\mathbf{X}^t \mathbf{e} = 0$
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100094 \end{field}
  \begin{field}
    Unbiased estimate of $\sigma^2$
  \end{field}
  \begin{field}
    $\hat{\sigma}^2 = \frac{1}{n- (p+1)} \sum_{i=1}^n e_i^2 = \frac{1}{n- (p+1)} \mathbf{e}^t \mathbf{e}$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100095 \end{field}
  \begin{field}
    $\mathbf{e}^t \mathbf{e} = $
  \end{field}
  \begin{field}
    $\mathbf{e}^t \mathbf{e} = \mathbf{Y}^t \mathbf{Y} - \mathbf{Y}^t \mathbf{HY}$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100096 \end{field}
  \begin{field}
    $E(\hat{\beta}) = $
  \end{field}
  \begin{field}
    $E(\hat{\beta}) = E((\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}) = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t E(\mathbf{Y}) = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{X}\beta  = \beta$

    So $\hat{\beta}$ is an unbiased estimate
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100097 \end{field}
  \begin{field}
    Gauss - Markov Theorem
  \end{field}
  \begin{field}
    If $E(\mathbf{Y}) = \mathbf{X}\beta$ and $V(\mathbf{Y}) = \sigma^2 \mathbf{I}$, then the least squares estimate $\hat{\beta}$ has the least variance among all linear unbiased estimators of $\beta$. (BLUE)
  \end{field}
\end{note}

% add proof of gauss markov


\begin{note} \begin{field} \tiny 100098 \end{field}
  \begin{field}
    $V(\hat{\beta}) = $
  \end{field}
  \begin{field}
    $V(\hat{\beta}) = \sigma^2 (\mathbf{X}^t \mathbf{X})^{-1}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100099 \end{field}
  \begin{field}
    $E(\hat{\sigma}^2) = $
  \end{field}
  \begin{field}
    $E(\hat{\sigma}^2) = \sigma^2$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100100 \end{field}
  \begin{field}
    If $\mathbf{X}_{p \times 1}$ has a multivariate normal distribution $N(\mu_{p\times 1}, \Sigma_{p \times p})$, then $\mathbf{AX} + b \sim $
  \end{field}
  \begin{field}
    If $\mathbf{X}_{p \times 1}$ has a multivariate normal distribution $N(\mu_{p\times 1}, \Sigma_{p \times p})$, then $\mathbf{AX} + \mathbf{b} \sim N(\mathbf{A}\mu + \mathbf{b}, \mathbf{A}\Sigma \mathbf{A}^t)$



  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100101 \end{field}
  \begin{field}
    Multivariate normal properties for $\mathbf{X}_{p \times 1} \sim N(\mu_{p\times 1}, \Sigma_{p \times p})$
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $Cov(X_j,X_k) = 0$ if and only if $X_j,X_k$ are independent (two way due to multivariate normal )
      \item All subsets of elements of $\mathbf{X}$ have a multivarite normal distribution
      \item All linear combinations of the components of $X$ are normally distributed
      \item $\mathbf{a}^t \mathbf{X} \sim N(\mathbf{a}^t, \mathbf{a}^t \Sigma \mathbf{a})$ for a vector $a$
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100102 \end{field}
  \begin{field}
    Linear Hypothesis testing single parameter $H_0: \mathbf{c}^t \beta = d$
  \end{field}
  \begin{field}
    For a vector $\mathbf{c}_{(p+1)\times 1}$, we have that
    \begin{itemize}
      \item $E(\textbf{c}^t\hat{\beta}) = \mathbf{c}^t \beta$, $V(\mathbf{c}^t \hat{\beta}) = \sigma^2 \mathbf{c}^t (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}$
      \item Thus $$ \frac{\mathbf{c}^t \hat{\beta} - \mathbf{c}^t \beta }{\sigma \sqrt{\mathbf{c}^t(\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}}} \sim N(0,1)$$ and under $H_0$
      $$ T = \frac{\mathbf{c}^t \hat{\beta} - d}{ \sqrt{\hat{\sigma}^2\mathbf{c}^t(\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}}} \sim t_{n - (p+1)}$$
      \item Example: testing $H_0: \beta_1 = \beta_2, \mathbf{c} = (0,1,-1)^t, d = 0$
      \item Reject $H_a: c^t \beta \neq d:  |T| > t_{n - (p+1)}(1-\alpha/2)$, $c^t \beta > d, T > t_{n-(p+1)}(\alpha), c^t\beta < d: T < t(1 - \alpha)$
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100103 \end{field}
  \begin{field}
    Confidence interval for a single parameter
  \end{field}
  \begin{field}
    $$ \hat{\beta}_j \pm t_{n-(p-1)}(1 - \alpha/2)\sqrt{\hat{\sigma}^2((\mathbf{X}^t \mathbf{X})^{-1})_{j+1,j+1}} $$
    $$ \mathbf{c}^t \beta \pm t_{n-(p-1)}(1 - \alpha/2)\sqrt{\hat{\sigma}^2 \mathbf{c}^t((\mathbf{X}^t \mathbf{X})^{-1})\mathbf{c}}  $$
    eg if we were testing $\beta_1 - \beta_2, c = (0,1,-1)$
  \end{field}
\end{note}





\begin{note} \begin{field} \tiny 100104 \end{field}
  \begin{field}
    F statistic in matrix form
  \end{field}
  \begin{field}
  \begin{itemize}
    \item $\mathbf{K}$  is $p\times k$, $\mathbf{m}$ is $k\times 1$
    \item Testing $H_0: \mathbf{K}^t\beta = \mathbf{m}$
    \item $F = \frac{\big((\mathbf{K}\hat{\beta} - \mathbf{m})^t (\mathbf{K}(\mathbf{X}^t \mathbf{X})^{-1}\mathbf{K}^{-1})(\mathbf{K}\hat{\beta} - \mathbf{m})\big)}{k\hat{\sigma}^2} \sim F_{k,n-p}$
    \item Eg$K = \begin{pmatrix}
      0 \\ 1 \\ \vdots \\ 0
    \end{pmatrix}$, $m = 0$
    \item Tests $\beta_1 = 0$
    \item Note the $\mathbf{K}^t$ matrix is the coefficients of the system of linear equations for the the null hypothesis, and $m$ is what they are equal to
  \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100105 \end{field}
  \begin{field}
    Overall regression F-test
  \end{field}
  \begin{field}
    Tests if any predictors are related to the response
    \begin{itemize}
      \item Full model: $\mathbf{Y} = \mathbf{X}\beta + \epsilon$
      \item Reduced model a nested model with $q$ estimated parameters
      \item eg: Reduced model: $\mathbf{Y} = \beta_0 + \epsilon$, $q = 1$
      \item $H_0: \beta_1 =  \ldots = \beta_p = 0$
      \item $F = \frac{(RSS_\omega - RSS_\Omega)/(p-q)}{RSS_\Omega/(n-p)}$
      \item
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100106 \end{field}
  \begin{field}
    Analysis of Variance Table and calculated F stat
  \end{field}
  \begin{field}
\begin{tabular}{|c|c|c|c|}
  \hline \\
  Type & df & Sum of Squares & Mean SS\\
  \hline \\
  Regression & $p$ & SS(Reg) & SS(Reg)/p\\
  Residual & $n-p+1$ & SS(Res) & $\hat{\sigma}^2 =$ SS(Res)/$n-p-1$\\
  Total & $n-1$ & SS(Total) = SS(Reg) + SS(Res) & $\frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2$\\
  \hline
\end{tabular}
and $F = \frac{Mean(SSREG)}{Mean(SSRES)}$
\end{field}
\end{note}

\begin{note} \begin{field} \tiny 100107 \end{field}
  \begin{field}
    Distribution of $\hat{\beta}$
  \end{field}
  \begin{field}
    $\hat{\beta} \sim N(\beta, \sigma^2(\mathbf{X}^t \mathbf{X})^{-1})$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100108 \end{field}
  \begin{field}
    RSS (in terms of $\Omega$ and $\omega$)
  \end{field}
  \begin{field}
    $$RSS_\Omega = \sum_{i=1}^n e_i^2$$
    $$RSS_\omega = \sum_{i=1}^n (Y_i - \bar{Y})^2$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100109 \end{field}
  \begin{field}
    $R^2$
  \end{field}
  \begin{field}
    $R^2 = \frac{SS(Reg)}{SS(Tot)} = 1 - \frac{SS(Res)}{SS(Tot)}$
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100110 \end{field}
  \begin{field}
    Properties of the estimate of $\sigma^2$
  \end{field}
  \begin{field}
    \begin{itemize}
      \item   $\hat{\sigma}^2 = \frac{|\mathbf{e}|^2}{n - (p+1)}$
      \item Under normality: $\frac{(n - (p+1))\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n - (p+1)}$
      \item $\hat{\sigma}^2$ is independent from $\hat{\beta}$
    \end{itemize}
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100111 \end{field}
  \begin{field}
    Prediction Interval
  \end{field}
  \begin{field}
    Predicting a future response
    $\mathbf{x}_0^t \hat{\beta} \pm t_{n-p}(\alpha/2)\hat{\sigma^2} \sqrt{1 + x_0^t(X^tX)^{-1}x_0}$

    A 95\% prediction interval for a response with (list values) is between and
  \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100112 \end{field}
  \begin{field}
    Confidence interval
  \end{field}
  \begin{field}
    Confidence in mean response $\mathbf{x}_0^t \hat{\beta} \pm t_{n-p}(\alpha/2)\hat{\sigma^2} \sqrt{x_0^t(X^tX)^{-1}x_0}$
    With 95\% confidence, the expected mean response
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100113 \end{field}
  \begin{field}
    Residual Plot
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Plot residuals against fitted values (so there is only 1 plot vs against explanatory variables)
      \item Verifies linearity and constant variance
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100114 \end{field}
  \begin{field}
    Leverege
  \end{field}
  \begin{field}
    \begin{itemize}
      \item An obervarion has high leverage if the explanatory variable values of the observation are different from general pattern
      \item $h_i = H_{ii} = (X(X^tX)^{-1}X^t)_{ii}$
      \item High leverage $h_i > \frac{2(p+1)}{n}$
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100115 \end{field}
  \begin{field}
    Standardized Residual
  \end{field}
  \begin{field}
    $r_i = \frac{e_i}{\hat{\sigma}\sqrt{1 - h_i}}$
    Large if $|r_i| > 2$ - indicates outlier
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100116 \end{field}
  \begin{field}
    Influential - if fitted model depends highly on the value
  \end{field}
  \begin{field}
    Measure using cook's distance
    $$ D_i = \frac{(\hat{Y} - \hat{Y}_{(i)})^t(\hat{Y} - \hat{Y}_{(i)})}{(p+1)\hat{\sigma}^2} = \frac{1}{p+1}r_i^2 \frac{h_i}{(1 - h_i)}$$
    Where $Y_{i}$ is the vector of fitted values when the model is fitted to the data without the $i$ ths observationl Moderate if $>1 $ Large if $>6$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100117 \end{field}
  \begin{field}
    Multicollinearity
  \end{field}
  \begin{field}
    \begin{itemize}
      \item $X^tX$ is close to singular
      \item Some columns are highly correlated
      \item there is a relationship between predictors \item leads to large standard errors
      \item Not a violation of assumptions, but leads to issues in interpretations
      \item Calculate using Condition number if $>30$ than large , or Variance inflation factors $VIF_j = \frac{1}{1 - R^2_j}$ where $R^2_j$ is $R^2$ from regression of the $j$th explanatory variable on all the other explanatory variables
      \item Not a problem for prediction
      \item Fix using selection of explanatory variables, generalized inverse, ridge regression
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100118 \end{field}
  \begin{field}
    Ridge Regression
  \end{field}
  \begin{field}
    $\hat{\beta} = (X^tX + \lambda I)^{-1} X^t Y$, where $\lambda$ is chosen. Note these are biased estimators
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100119 \end{field}
  \begin{field}
    Fix non-constant spread/variance
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Transform response (box-cox)
      \item Use more complicated model (glm)
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100120 \end{field}
  \begin{field}
    Fix non-linearity
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Transform response
      \item Transform predictor
      \item allow for curvature: predictor squared, splines, gam
      \item use a non linear model
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100121 \end{field}
  \begin{field}
    Fix Non-normality
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Transform response
      \item more complicated models : glm
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100122 \end{field}
  \begin{field}
    Missing data completely at random (MCAR)
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Throwing out cases with missing data does not bias inferences
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100123 \end{field}
  \begin{field}
    Missing at random (MAR)
  \end{field}
  \begin{field}
    Probability of missingness depends only on available information, like the explanatory variables and the response variables present in the regression - impute missing data
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100124 \end{field}
  \begin{field}
    Model Selection methods
  \end{field}
  \begin{field}
    \begin{itemize}
      \item Sequential Methods: Backward/Forward (eliminate untill all values have p-value below critical value) Elimination
      \item Penalized Regression: Ridge and Lasso
    \end{itemize}
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100125 \end{field}
  \begin{field}
    AIC
  \end{field}
  \begin{field}
    Estimate the distance of a candidate model from the true model (small good)
    $$ n \log (RSS/n) + 2(p+1)$$
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100126 \end{field}
  \begin{field}
    BIC
  \end{field}
  \begin{field}
    Estimate the best parsimonious model, using a prior distribution on the parameters (small good)
    $$  n \log (RSS/n) + \log(n)(p+1)$$

    Where $n$ is the number of observations, $p$ is the number of predictors (not including intercept), and $RSS = \sum (Y_i - \hat{Y})^2 = \sum e_i^2 $
  \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100127 \end{field}
  \begin{field}
    Adjusted $R^2$
  \end{field}
  \begin{field}
    $$ 1 - \frac{n-1}{n-p}(1 - R^2)$$ (large is good)
    $\frac{MS(Reg)}{MS(Total)} = 1 - \frac{SS(Reg)/(n - p - 1)}{SS(Tot)/(n-1)}$
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100128 \end{field}
  \begin{field}
    Mallow's Cp
  \end{field}
  \begin{field}
    $$ RSS/\hat{\sigma^2} + 2p - n $$ (small good)
  \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100129 \end{field}
  \begin{field}
    Box-Cox Transformation
  \end{field}
  \begin{field}
    Transform so model is $g(Y) = X\beta + \epsilon$
    where $g(y) = \frac{y^\lambda -1}{\lambda} if \lambda \neq 0, 0 $ otherwise
  \end{field}
\end{note}


%%end_tag
\end{document}
