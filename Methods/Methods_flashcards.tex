% -*- coding: utf-8 -*-
\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}
\usepackage{mathrsfs}
\pagestyle{empty}
\setlength{\parindent}{0in}

\newenvironment{note}{\paragraph{NOTE:}}{}
\newenvironment{field}{\paragraph{field:}}{}
\newenvironment{tags}{\paragraph{tags:}}{}

\begin{document}

%%start_tag Methods 1 100000 - 1000063
\tags{Methods1}
\begin{note} \begin{field} \tiny 100000 \end{field}
 \begin{field}
  Epidemiology Definition of Causation
 \end{field}
 \begin{field}
  Factor/variable $X$ \textbf{causes} result $Y$ if some cases of $Y$ would not have occurred if X had been absent.
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100001 \end{field}
 \begin{field}
  Sample variance
 \end{field}
 \begin{field}
  $s^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i - \bar{x})^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100002 \end{field}
 \begin{field}
  Population(s) of interest
 \end{field}
 \begin{field}
  The group to which you would like your answer to apply
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100003 \end{field}
 \begin{field}
  Variable of Interest
 \end{field}
 \begin{field}
  A measurement that can be made on each individual/member of the population
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100004 \end{field}
 \begin{field}
  Facts about Normal Distributions
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If $Z$ has a Normal(0,1) distribution then $X = \sigma Z + \mu$ has a Normal$(\mu,\sigma^2)$ distribution
   \item If $X$ has a Normal($\mu,\sigma^2$) distribution, then $Z = \frac{X - \mu}{\sigma}$ has a Normal(0,1) distribution.
   \item If $X$ has a Normal($\mu_x,\sigma^2_x)$ distribution, and $Y$ has a Normal($\mu_y,\sigma_y^2$) distribution, and $X$ and $Y$ are independent of each other, then $X + Y \sim $ Normal($\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100005 \end{field}
 \begin{field}
  Sample mean
 \end{field}
 \begin{field}
  $\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100006 \end{field}
 \begin{field}
  Sampling distribution for population $Y \sim $ Normal($\mu,\sigma$)
 \end{field}
 \begin{field}
  $N(\mu,\sigma^2/n)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100007 \end{field}
 \begin{field}
  Variance (Expected value)
 \end{field}
 \begin{field}
  $V(Y) = E[(X - E(X))^2] = E(X^2) - E[(X)]^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100008 \end{field}
 \begin{field}
  Covariance
 \end{field}
 \begin{field}
  $Cov(X,Y) = E[(X - E(X))(Y - E(Y))]$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100009 \end{field}
 \begin{field}
  If $X$ and $Y$ are independent (covariance)
 \end{field}
 \begin{field}
  The covariance is 0
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100010 \end{field}
 \begin{field}
  If $Cov(X,Y) = 0$, (independence)
 \end{field}
 \begin{field}
  Cannot say that $X$ and $Y$ are independent
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100011 \end{field}
 \begin{field}
  $Cov(X,X) =$
 \end{field}
 \begin{field}
  $Var(X)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100012 \end{field}
 \begin{field}
  $X \sim N(\mu,\sigma^2)$
  \begin{itemize}
   \item $E(\bar{X}) = $
   \item $V(\bar{X}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\bar{X}) = \mu$
   \item $V(\bar{X}) = \sigma^2/n$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100013 \end{field}
 \begin{field}
  Central Limit Theorem (in words)
 \end{field}
 \begin{field}
  If the population distribution of a variable $X$ has population mean $\mu$ and finite population variance $\sigma^2$, then the sampling distribution of the sample mean becomes closer and closer to a Normal distribution as the sample size $n$ increases: $\bar{X} \sim N(\mu,\sigma^2/n)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100014 \end{field}
 \begin{field}
  Central Limit Theorem (theoretical)
 \end{field}
 \begin{field}
  Let $X_1, X_2, \ldots X_n$ be an iid sample from some poupation distribution $F$ with mean $\mu$ and variance $\sigma^2 < \infty$. Then as the sample size $n \to \infty$, we have $$\frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} \to N(0,1)$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100015 \end{field}
 \begin{field}
  $X \sim (\mu,\sigma^2)$
  \begin{itemize}
   \item $E(\bar{X}) = $
   \item $V(\bar{X}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\bar{X}) = \mu$
   \item $V(\bar{X}) = \sigma^2/n$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100016 \end{field}
 \begin{field}
  Reject $H_0$ when $H_0$ True
 \end{field}
 \begin{field}
  Type I error (false positive)
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100017 \end{field}
 \begin{field}
  Type I error (false positive)
 \end{field}
 \begin{field}
  Reject $H_0$ when $H_0$ True
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100018 \end{field}
 \begin{field}
  Fail to Reject $H_0$ when $H_0$ false
 \end{field}
 \begin{field}
  Type II error
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100019 \end{field}
 \begin{field}
  Type II error
 \end{field}
 \begin{field}
  Fail to Reject $H_0$ when $H_0$ false
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100020 \end{field}
 \begin{field}
  Significance level
 \end{field}
 \begin{field}
  $\alpha$ the probability of a Type I error
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100021 \end{field}
 \begin{field}
  Power (at $\theta_1$)
 \end{field}
 \begin{field}
  Probability of rejecting the null hypothesis when $\theta_1$ is the truth
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100022 \end{field}
 \begin{field}
  Test for data setting: $X_1, X_2, \ldots X_n$ iid with sample mean $\bar{X}$, and known population variance $\sigma^2$, Null hypothesis $\mu = \mu_0$

  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value
         \begin{itemize}
          \item Lower
          \item Upper
          \item Two sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  z-test
  \begin{itemize}
   \item Test statistic: $Z(\mu_0) = \frac{\bar{X} - \mu_0}{\sqrt{\sigma^2/n}}$
   \item Reference Distribution: Under $H_0, Z(\mu_0) \sim N(0,1)$
         \begin{itemize}
          \item Lower: Reject when $Z(\mu_0) < z_{\alpha}$ = qnorm($\alpha$)
          \item Upper: Reject when $Z(\mu_0) > z_{1 - \alpha} = $qnorm(1-$\alpha$)
          \item Two sided:  Reject when $|Z(\mu_0)| > z_{1 - \alpha/2}$ = qnorm(1 - $\alpha/2$)
         \end{itemize}
   \item Confidence interval: $ \bar{X} \pm z_{1 - \alpha/2}\sqrt{\frac{\sigma^2}{n}}$
   \item pvalue:
         \begin{itemize}
          \item upper: 1 - $\Phi(z)$ = 1 - pnorm(z)
          \item lower: $\Phi(z)$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item Consistent: Yes /Finite Sample Exact: Yes if $X_i \sim N$/ Asymptotically Exact: Yes
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100023 \end{field}
 \begin{field}
  Exactness (finite/asymptotic)
 \end{field}
 \begin{field}
  Under any setting for which the null hypothesis is true, is the actual rejection probability equal to the desired level $\alpha$?
  \begin{itemize}
   \item Finite Sample Exact: for sample size $n$ is $P(Reject H_0) = \alpha$ when $H_0$ is true?
   \item Asymptotic Exactness: As $n \to \infty$ does $P(Reject H_0) \to \alpha$ when $H_0$ is true?
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100024 \end{field}
 \begin{field}
  When is a test exact?
 \end{field}
 \begin{field}
  \begin{itemize}
   \item A test is FSE if the reference distribution is the true distribution of the test statistic $T$ when $H_0$ is true
   \item A test is AE if the reference distribution is the asymptotic distribution of the test statistic when $H_0$ is true.
   \item (Distribution of p-values should be Unif(0.1))
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100025 \end{field}
 \begin{field}
  Consistency
 \end{field}
 \begin{field}
  When $H_0$ is false (the alternative hypothesis is true), does the rejection probability (probability reject the null) tend to 1 as $n \to \infty$?
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100026 \end{field}
 \begin{field}
  Interpretation of Confidence intervals
 \end{field}
 \begin{field}
  $(1 - \alpha)100$\% of the time, intervals constructed in this manner will include $\mu$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100027 \end{field}
 \begin{field}
  Test for data setting: $X_1, X_2, \ldots X_n$ iid with sample mean $\bar{X}$, and unknown population variance , Null hypothesis $\mu = \mu_0$
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: t-test
   \item Test Statistic: $t(\mu_0) = \frac{\bar{X} - \mu_0}{\sqrt{s^2/n}}$
   \item Test Reference Distribution: $t_{n-1}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: Reject if $t(\mu_0) > t_{(n-1),1-\alpha}$ = qt(1 - $\alpha$,n-1)
          \item lower: Reject if $t(\mu_0)  < t_{n-1,\alpha}$
          \item two sided: Reject if $|t(\mu_0)| > t_{n-1, 1 - \alpha/2}$
         \end{itemize}
   \item Confidence interval: $\bar{X} \pm t_{n-1,1-\alpha/2}\sqrt{\frac{s^2}{n}}$
   \item pvalue, with $t(\mu_0)  = t$, and pt representing the cdf of a t distribution
         \begin{itemize}
          \item upper: 1 - pt($t,n-1$)
          \item lower: pt($t$,n-1)
          \item two-sided: 2*(1 - pt(abs(t)),n-1)
         \end{itemize}
   \item Consistent Yes/Finite Sample Exact Yes if normal/ Asymptotically Exact Yes
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100028 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$ iid Bernoulli(p) (option 1), parameter of interest $p$
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$ iid Bernoulli(p), parameter of interest $p$
  \begin{itemize}
   \item Test name: Exact Binomial Test (uses the distribution of the sum of Bern(p) RVs)
   \item Test Statistic: $X = \sum_{i=1}^n Y_i = n\bar{Y}$
   \item Test Reference Distribution: Under $H_0$ Binomial$(n,p_0)$
   \item Critical Value/ Rejection region: Sometimes use randomized test
         \begin{itemize}
          \item upper: Reject $H_0$ for $X \geq c$ for c such that $P(X \geq c)\leq \alpha$
          \item lower: Reject $H_0$ for $X \leq c$ for c such that $P(X \leq c)\leq \alpha$
          \item two-sided: Reject $H_0$ for $p_0(X)\leq c$ for $c$ such that$P_{H_0}(p_0(X) \leq c)\leq \alpha$, where $p_0(X)$ is $P(X = x)$ under $H_0$
         \end{itemize}
   \item Confidence interval: Values that are not rejected
   \item pvalue: Sum of the probabilities that are less than or equal to the observed value (under the null hypothesis)
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100029 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$, parameter of interest: $p$ iid Bernoulli(p) (option 2)
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$, parameter of interest: $p$ iid Bernoulli(p) (option 2)
  \begin{itemize}
   \item Test name: Binomial $z$-test (Use when $np_0 > 5$ and $n(1-p_0) > 5$)
   \item Test Statistic: $X  = \sum_{i=1}^n = n\bar{Y}$, $\hat{p} = X/n$, $z(p_0) = \frac{\hat{p}- p_0}{\sqrt{p_0(1-p_0)/n}}$ (score)
   \item Test Reference Distribution: Under $H_0$, Approximately $X \sim N(np_0,np_0(1-p_0))$ and $z(p_0) \sim N(0,1)$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: $z(p_0) > z_{1-\alpha}$
          \item lower: $z(p_0) < z_\alpha$
          \item two-sided: $|z(p_0)| > z_{1-\alpha/2}$
         \end{itemize}
   \item Confidence interval: Uses wald interval (derived from t-test) (with $z_w(p_0) = \frac{\hat{p} - p_0}{\sqrt{\hat{p}(1 - \hat{p})/n}}$) $\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}$
   \item pvalue
         \begin{itemize}
          \item upper: 1 - $\Phi(z(p_0))$ = 1 - pnorm($z(p_0)$)
          \item lower: $\Phi(z(p_0))$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z(p_0)|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item Consistent: Yes/Finite Sample Exact: No/ Asymptotically Exact: Yes
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100030 \end{field}
 \begin{field}
  Continuity correction for Binomial z-test
 \end{field}
 \begin{field}
  With $X \sim Binom(n,p)$, instead of $P(X \leq x)$, use $P(W \leq x + 1/2)$ where $W \sim N(np,np(1-p))$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100031 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item If ties?
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: Sign Test
   \item Test Statistic: $Y_i = I(X_i < M_0)$ , $\hat{p}_{M_0} = \frac{\sum Y_i}{n}$ (proportion of observations less than or equal to hypothesized median)
   \item Test Reference Distribution: Normal distribution: with $p_0 = .5$
   \item Critical Value/ Rejection region: $z = \frac{\hat{p}_{M_0} - p_0}{\sqrt{p_0(1-p_0)/n}}$
         \begin{itemize}
          \item upper: $z > z_{1-\alpha}$
          \item lower: $z < z_\alpha$
          \item two-sided: $|z| > z_{1-\alpha/2}$
         \end{itemize}
   \item Confidence interval: cant use the binomial proportion CI
         Set of values of $M_0$ that wouldn't be rejected at level $\alpha$

         $$ \bigg( \frac{n - z_{1 - \alpha/2} \sqrt{n}}{2}\bigg)^{th} \text{ Smallest Observation}, \bigg( \frac{n - z_{1 - \alpha/2} \sqrt{n}}{2}\bigg)^{th}\text{ Smallest Observation} $$
   \item pvalue (binomial test on proportion)
         \begin{itemize}
          \item upper: 1 - $\Phi(z(p_0))$ = 1 - pnorm($z(p_0)$)
          \item lower: $\Phi(z(p_0))$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z(p_0)|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item If there are ties: remove all observations equal to $M_0$, then test prop of observations $< M_0$ given not equal to $M_0$ is .5
   \item Consistent: yes/Finite Sample Exact: No / Asymptotically Exact: yes
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100032 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$ (option 2)
  \begin{itemize}
   \item Test name:
   \item Procedure:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$ (option 1)
  \begin{itemize}
   \item Test name: Wilcoxon signed-rank test (require symmetry assumption) - equivalently a test of the mean - Tests the pseudo-median
   \item Procedure: testing $c_0$ is the center (median)
         \begin{itemize}
          \item Calculate distance of each observation from $c_0$
          \item Rank observations by the distance (abs value) from $c_0$
         \end{itemize}
   \item Test Statistic: $S$ sum of the ranks that correspond to observations larger than $c_0$, $Z = \frac{S - \frac{n(n+1)}{4}}{\sqrt{\frac{n(n+1)(2n+1)}{24}}} \sim N(0,1)$
   \item Test Reference Distribution:
         \begin{itemize}
          \item Exact p-value - assume each rank has the same chance of being assigned to observations above or below $c_0$ - all possible ways to assign the ranks
          \item Normal approximation to the null distribution $S \sim N\big(\frac{n(n+1)}{4}, \frac{n(n+1)(2n+1)}{24}\big)$
         \end{itemize}
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue - Same as for Normal
   \item Consistent Yes under symmetry assumption /Finite Sample Exact No/ Asymptotically Exact Yes (under symmetry assumption)
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100033 \end{field}
 \begin{field}
  Pseudomedian
 \end{field}
 \begin{field}
  Median of the distribution of sample means from samples of size 2
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100034 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid N($\mu,\sigma^2$) parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid N($\mu,\sigma^2$) parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name: $\chi^2$ for Population Variance
   \item Test Statistic $X(\sigma_0) = \frac{(n-1)s^2}{\sigma_0^2}$
   \item Test Reference Distribution: Under $H_0: X(\sigma_0) = \frac{(n-1)s^2}{\sigma_0^2} \sim \chi_{n-1}^2$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item  $\sigma^2 > \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) > \chi^2_{n-1}(1-\alpha)$
          \item $\sigma^2 < \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) < \chi^2_{n-1}(\alpha)$
          \item $\sigma^2 \neq \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) > \chi^2_{n-1}(1 - \alpha/2)$ or $X(\sigma_0) < \chi^2_{n-1}(\alpha/2)$
         \end{itemize}
   \item Confidence interval  $$ \bigg( \frac{(n-1)s^2}{\chi^2_{n-1}(1 - \alpha/2)}, \frac{(n-1)s^2}{\chi^2_{n-1}(\alpha/2)}\bigg)$$
   \item pvalue
         \begin{itemize}
          \item $\sigma^2 > \sigma_0^2$: $p = 1 - pchisq(X(\sigma_0)^2,n-1)$
          \item $\sigma^2 < \sigma_0^2: p = pchisq(X(\sigma_0^2),n-1)$
          \item $\sigma^2 \neq \sigma_0^2: p = 2\min(1 - pchisq(X(\sigma_0^2), n-1), pchisq(X(\sigma_0^2)),n-1)$
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100035 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid \\
  Parameter of interest: $\sigma^2 = Var(X)$,\\
  Sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, \\
  $H_0: \sigma^2 = \sigma_0^2$ (asymptotic)
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item pvalue
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name: Asymptotic $t$-test for population variance
   \item Test Statistic: $Y = (X_i - \bar{X})^2$, $$ t(\sigma_0^2)  = \frac{\bar{Y - \frac{n-1}{n}\sigma_0^2}}{\sqrt{s_y^2/n}} \to N(0,1)$$
         Note $\bar{Y} = \frac{n-1}{n}s^2$
   \item Tests that the population mean of the $Y_i$ is $\frac{n-1}{n}\sigma_0^2$
   \item Test Reference Distribution $ \frac{\frac{n-1}{n}s^2 - \frac{n-1}{n}\sigma^2}{\sqrt{Var(\frac{n-1}{n}s^2)}} = \frac{\bar{Y}- \frac{n-1}{n}\sigma^2}{\sqrt{Var(\bar{Y})}} \to N(0,1)$, so we can use t-test
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: Reject if $t(\sigma_0^2) > t_{(n-1),1-\alpha}$ = qt(1 - $\alpha$,n-1)
          \item lower: Reject if $t(\sigma_0^2)  < t_{n-1,\alpha}$
          \item two sided: Reject if $|t(\sigma_0^2)| > t_{n-1, 1 - \alpha/2}$
         \end{itemize}
   \item Confidence interval: $\bar{X} \pm t_{n-1,1-\alpha/2}\sqrt{\frac{s^2}{n}}$
   \item pvalue, with $t(\mu_0)  = t$, and pt representing the cdf of a t distribution
         \begin{itemize}
          \item upper: 1 - pt($t,n-1$)
          \item lower: pt($t$,n-1)
          \item two-sided: 2*(1 - pt(abs(t)),n-1)
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100036 \end{field}
 \begin{field}
  Test for data setting $X_1, \ldots X_n$ iid from population distribution $F$. Test $H_0: F = F_0$
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $X_1, \ldots X_n$ iid from population distribution $F$. Test $H_0: F = F_0$
  \begin{itemize}
   \item Test name: Kolmogorov-Smirnov Test
   \item Process
   \item Test Statistic: $D(F_0) = sup_x|\hat{F}(x) - F_0(x)$, where $\hat{F}(x) = \frac{1}{n}\sum_{i=1}^n 1(X_i \leq x)$ is the empirical cdf and $F_0(x)$ is the null hypothesis cdf (maximum values of difference between emperical and null)
   \item Test Reference Distribution: Kolmogorov distribution
   \item Critical Value/ Rejection region: Reject for large values of $\sqrt{n}D(F_0)$
   \item Note the one sided version does not have an easy interpretation
  \end{itemize}
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100037 \end{field}
 \begin{field}
  Data setting: $X_1, \ldots , X_n$ iid from discrete distribution. Test fit of distribution
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item If parameter values of discrete distribution are not known
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: $X_1, \ldots , X_n$ iid from discrete distribution. Test fit of distribution
  \begin{itemize}
   \item Test name: $\chi^2$ goodness of fit test, test for discrete distributions
   \item Process: Test the underlying population distribution is $P(X = x) = p_0(x)$, where $\hat{p}(x) = \frac{1}{n} \sum_{i=1}^n 1(X_i = x)$
         \begin{itemize}
          \item Let $j = 1, \ldots, k$ the different categories that $X_i$ can take
          \item Let $O_j$ be the observed number of observations that belong to category $j$
          \item Let $E_j = np_0(j)$ be the expected number of observations that would belong to category $j$ if the null hypothesis were true
         \end{itemize}
   \item Test Statistic: $X(p_0) = \sum_x\frac{n(\hat{p}(x) - p_0(x))^2}{p_0(x)} = \sum_{j=1}^k \frac{(O_j - E_j)^2}{E_j}$
   \item Test Reference Distribution: Under $H_0$, $X(p_0) \to \chi_{k-1}^2$
   \item Critical Value/ Rejection region: Reject for large values of $X(p_0)$ - Reject $H_0$ for $X(p_0) > \chi_{k-1}^2(1-\alpha)$
   \item Note: Null hypothesis doesnt completely specify the distribution, just the family of distributions with perhaps unknown parameters
         \begin{itemize}
          \item Estimate the parameters
          \item Use null distribution with estimated parameter values for $E_j$
          \item Compute $\chi^2$ test statistic
          \item Compare to $\chi_{k-d-1}^2$ distribution where $k$ = number of categories, $d = $ number of estimated parameters
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100038 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with known $\sigma_x, \sigma_y$. Estimate $d = \mu_x - \mu_y$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item p-value
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with known $\sigma_x, \sigma_y$. Estimate $d$,
  \begin{itemize}
   \item Test name: 2 sample $z$ test
   \item Test Statistic: $z(d_0) = \frac{(\bar{X}- \bar{Y}) - d_0}{\sqrt{\frac{\sigma_x^2}{m} - \frac{\sigma_y^2}{n}}}$
   \item Test Reference Distribution: Under $H_0$, $z(d_0) \sim N(0,1)$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item Lower: $d \leq d_0$ Reject when $z(d_0) < z_{\alpha}$ = qnorm($\alpha$)
          \item Upper: $d \geq d_0$ Reject when $z(d_0) > z_{1 - \alpha} = $qnorm(1-$\alpha$)
          \item Two sided: $d \neq d_0$ Reject when $|z(d_0)| > z_{1 - \alpha/2}$ = qnorm(1 - $\alpha/2$)
         \end{itemize}
   \item Confidence interval: $$ (\bar{X} - \bar{Y}) \pm z(1 - \frac{\alpha}{2})\sqrt{\frac{\sigma_x^2}{m} + \frac{\sigma_y^2}{n}} $$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100039 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with unknown but equal  $\sigma_x, \sigma_y$ Estimate $d$
  \begin{itemize}
   \item Test name:
   \item Estimate of $\sigma_x^2 = \sigma_y^2$
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item When not equal
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with unknown $\sigma_x, \sigma_y$. Estimate $d$
  \begin{itemize}
   \item Test name: Equal variance 2-sample t-test
   \item Note: Estimate of $\sigma_x^2 = \sigma_y^2 = s_p^2 = \frac{\sum_{i=1}^m (X_i - \bar{X})^2 + \sum_{i=1}^n (Y_i - \bar{Y})}{(m-1) + (n-1)} = \frac{(m-1)s_x^2 + (n-1)s_y^2}{(m+n-2)}$ (weighted average of the two sample variances )
   \item Test Statistic: $t(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{s_p^2(\frac{1}{m} + \frac{1}{n})}}$
   \item Test Reference Distribution: For Normal populations, under $H_0$: $t(d_0) \sim t_{m+n-2}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item $d > d_0$ Reject $H_0$ for $t_e(d_0) > t_{m+n-2}(1 - \alpha)$
          \item $d < d_0$ Reject $H_0$ for $t_e(d_0) < t_{m+n-2}(\alpha)$
          \item $d \neq d_0$ Reject $H_0$ for $|t_e(d_0)| > t_{m+n-2}(1 - \alpha/2)$
         \end{itemize}
   \item Confidence interval $ (\bar{X} - \bar{Y}) \pm t_{m+n-2}(1 - \frac{\alpha}{2})\sqrt{s_p^2(\frac{1}{m} + \frac{1}{n})}$
   \item When not equal:
         \begin{itemize}
          \item Expected value of Estimated variance is larger than it should be when the smaller sample comes from the population with smaller variance - the test statistic will be closer to zero than it should be, and rejection rates will be smaller - Less power - more conservative
          \item Expected value of Estimated variance is smaller than it shoudl be when smaller sample comes from the population with the larger variance - test statistic will have a larger absolute value than it should an rejection rates will be larger  - more power - anti conservative
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100040 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with unknown not equal  $\sigma_x, \sigma_y$ Estimate $d = \mu_x - \mu_y$
  \begin{itemize}
   \item Test name:
   \item Estimate of $Var(\bar{X} - \bar{Y})$
   \item Test Statistic
   \item Test Reference Distribution
   \item Degrees of freedom
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item Compare to equal variance
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with unknown not equal equal  $\sigma_x, \sigma_y$ Estimate $d$
  \begin{itemize}
   \item Test name: Unequal variance 2 sample t-test
   \item Estimate of $Var(\bar{X} - \bar{Y}) = \frac{s_x^2}{m} + \frac{s_y^2}{n}$
   \item Test Statistic: $t_U(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{s_x^2}{m} + \frac{s_y^2}{n}}}$
   \item Test Reference Distribution: If the two distributions are Normal, there is not an exact distribution for the test statistic -  Use Welch-Satterthwaite approximation: Estimate degrees of freedom
         $$ v = \frac{\big(\frac{s_x^2}{m} + \frac{s_y^2}{n}\big)^2}{\frac{s_x^4}{m^2(m-1)} + \frac{s_Y^4}{n^2(n-1)}}$$
         $\min(m-1,n-1) \leq v \leq m+n-2$
         Under $H_0$ $t_u(d_0) $ approx $\sim t_{v}$
   \item Critical Value/ Rejection region: same as t-test
   \item Confidence interval: $(\bar{X} - \bar{Y}) \pm t_v(1 - \frac{\alpha}{2})\sqrt{\frac{s_x^2}{m} + \frac{s_Y^2}{n}}$
   \item Compare to equal variance:
         \begin{itemize}
          \item For unequal sample sizes with unequal population variances, equal variance t-test does not have correct calibration
          \item When samples sizes are equal both test statistics are the same, but degrees of freedom differ
          \item When equal variance assumption is true, equal variance has slightly better power, and very slightly better calibration (more exact )
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100041 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$,\\
  $Y_1, \ldots, Y_n$ iid $F_y$,\\
  $X_i$ not independent $Y_i$,\\
  $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$. \\
  Estimate $d = \mu_x - \mu_y$, when $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ known
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$,\\
  $Y_1, \ldots, Y_n$ iid $F_y$,\\
  $X_i$ not independent $Y_i$,\\
  $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$. \\
  Estimate $d = \mu_x - \mu_y$, when $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ known
  \begin{itemize}
   \item Test name: Paired z-test
   \item Test Statistic: $z(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{\sigma_x^2}{n} + \frac{\sigma_Y^2}{n} - 2 \frac{\sigma_{XY}}{n}}} = \frac{\bar{D} - d_0}{\sqrt{\frac{\sigma_D^2}{n}}}$
   \item Test Reference Distribution: Under $H_0$, $z(d_0) $ aprox $\sim N(0,1)$
   \item Critical Value/ Rejection region: Same as normal
   \item Confidence interval : $$(\bar{X} - \bar{Y}) \pm z(1 - \frac{\alpha}{2})\sqrt{\frac{\sigma_x^2}{n} + \frac{\sigma_Y^2}{n} - 2 \frac{\sigma_{XY}}{n}} = \bar{D} \pm z(1 - \alpha/2) \sqrt{\frac{\sigma_D^2}{n}}$$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100042 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$, $X_i$ not independent $Y_i$, $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$ Estimate $d = \mu_x - \mu_y$, $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ unknown
  \begin{itemize}
   \item Test name:
   \item Estimate of $\sigma_{XY}$
   \item Estimate of $Var(\bar{X} - \bar{Y})$
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$, $X_i$ not independent $Y_i$, $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$ Estimate $d = \mu_x - \mu_y$, $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ unknown
  \begin{itemize}
   \item Test name: Paired Data t-test
   \item Estimate of $\sigma_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})$
   \item Estimate of $Var(\bar{X} - \bar{Y}) = \frac{s_d^2}{n} = \frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}$
   \item Test Statistic: $t(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}}} = \frac{\bar{D} - d_0}{\sqrt{\frac{s_D^2}{n}}}$
   \item Test Reference Distribution: If differences are Normal (note X,Y Normal does not imply Differences are normal unless X,Y are jointly multivariate-normal) Under $H_0$, $t(d_0) \sim t_{n-1}$ (exact distribution)
   \item Critical Value/ Rejection region Same as t
   \item Confidence interval
         $$ (\bar{X} - \bar{Y}) = t_{n-1}(1 - \frac{\alpha}{2})\sqrt{\frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}} = \bar{D} \pm  t_{n-1}(1 - \frac{\alpha}{2}) \sqrt{\frac{s_d^2}{n}}$$
   \item Equivalent to a one sample - t-test on the differences
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100043 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$
  \begin{itemize}
   \item Test name: Binomial proportions two-sample z-test
   \item Test Statistic: $$ z = \frac{\hat{p}_x - \hat{p}_y}{\sqrt{\hat{p}_c(1 - \hat{p}_c(\frac{1}{m} + \frac{1}{n}))}} $$ Where $\hat{p_c} = \frac{m\hat{p}_x + n\hat{p}_y}{m+n} = \frac{b + d}{N}$
   \item Test Reference Distribution: Under $H_0: z $ approx $\sim N(0,1)$
   \item Critical Value/ Rejection region: Same as regular 2-sample
   \item Confidence interval: $$  \hat{p}_x - \hat{p}_y \pm z_{1 - \alpha/2} \sqrt{\big(\frac{\hat{p}_x(1 - \hat{p}_x)}{m} + \frac{\hat{p}_y(1 - \hat{p}_y)}{n}\big)}$$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100044 \end{field}
 \begin{field}
  Multinomial sampling
 \end{field}
 \begin{field}
  Collection of random samples, recording what group they are in: Can estimate $P(X = x | G = g)$, where $G$ is the group
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100045 \end{field}
 \begin{field}
  Two-Sample Binomial sampling
 \end{field}
 \begin{field}
  Sample $m$ units from group 1 and $n $ units from group 2
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100046 \end{field}
 \begin{field}
  Can we estimate $P(X = x | G = g)$ with binomial sampling
 \end{field}
 \begin{field}
  Cannot estimate
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100047 \end{field}
 \begin{field}
  $P(X = x | G = g)$ with multinomial sampling
 \end{field}
 \begin{field}
  Can estimate
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100048 \end{field}
 \begin{field}
  $E(g(T)) = $
 \end{field}
 \begin{field}
  $E(g(T)) \neq g(E(T))$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100049 \end{field}
 \begin{field}
  Reason for performing transformations on data
 \end{field}
 \begin{field}
  Some tests are FSE only when population distribution is Normal (otherwise the methods are asymptotically exact), requiring a large $n$. Transformations that improve approximation of normality make Normal-based methods perform more exactly
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100050 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$ (Association/independent/relationship)
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$ (Association/independent/relationship)
  \begin{itemize}
   \item Test name: Pearson's Chi-squared Test
   \item Test Statistic: $X = \sum_{i,j \in \{1,2\}} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ Where $O_{ij} = n_{ij}$ and $E_{ij} = \frac{R_iC_j}{N}$
   \item Test Reference Distribution: Under $H_0$ $X \sim \chi^2_1$
   \item Critical Value/ Rejection region: Reject for $X > \chi_1^2(1 - \alpha)$
   \item Note: Equal to to sided z-test for binomial proportions: $X = z^2$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100051 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ (No association between response variable $X$ and grouping variable $G$)
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item pvalue
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ (No association between response variable $X$ and grouping variable $G$)
  \begin{itemize}
   \item Test name: Fisher's Exact Test (of homogeneity of proportions)
   \item Test Statistic: Probability of observed table conditioning on margins: Compute all tables with the same margin totals: $\frac{\binom{C_2}{O_{12}}\binom{C_1}{O_{11}}}{\binom{N}{R_1}}$
   \item pvalue: Sum of probability of all tables more extreme than observed table
         More Extreme:
         \begin{itemize}
          \item $p_x > p_y$ More extreme = larger $O_{12}$
          \item $p_x < p_y$ More extreme = smaller $O_{12}$
          \item $p_x \neq p_y$ More extreme = less likely table
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100052 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ Binomial sampling scheme
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ Binomial sampling scheme
  \begin{itemize}
   \item Test name: Log Odds - test $H_0: \omega = 1$
   \item Test Statistic: $\hat{\omega} = \frac{ad}{bc}$, $z = \frac{\log(\hat{omega})}{\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}$
   \item Test Reference Distribution $ \log(\hat{\omega}) $ approx $\sim N(\log(\omega), \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d})$, $z $ approx $\sim N(0,1)$
   \item Critical Value/ Rejection region
   \item Confidence interval $(\hat{omega}e^{-z(1 - \frac{\alpha}{2})\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}, \hat{omega}e^{z(1 - \frac{\alpha}{2})\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}})$
   \item: $\omega > 1, p_1 > p_2$, $\omega = 1, p_1 = p_2$, small $p_1, p_2$, $\omega = p_1/p_2$ = relative risk
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100053 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, $X,Y$ not independent (paired) test proportions equal in groups (equally likely/probability)
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, $X,Y$ not independent (paired), test proportions equal in groups (equally likely/probability)
  \begin{itemize}

   \item Note, requires a table that keeps track of the pairs
         \begin{tabular}{|c|c c|c|}
                        & Measurement 1 &       &       \\
          Measurement 2 & No            & Yes   & Total \\
          \hline                                        \\
          No            & a             & b     & $R_1$ \\
          Yes           & c             & d     & $R_2$ \\
          \hline                                        \\
          Total         & $C_1$         & $C_2$ & n     \\
          \hline
         \end{tabular}
   \item Test name: McNemar's Test
   \item Test Statistic: $z = \frac{b-c}{\sqrt{b+c}}$
   \item Test Reference Distribution: $z \sim N(0,1)$, $z^2 \sim \chi_1^2$
   \item Critical Value/ Rejection region: Two sided rejecct $|z| > z(1 - \alpha/2)$
   \item Note equivalent to performing a paired t-test on the differences:
         $$ t = \frac{b-c}{\sqrt{\frac{n}{n-1}(b + c - \frac{(b-c)^2}{40})}} $$ compare to $t_{n-1}$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100054 \end{field}
 \begin{field}
  Data setting: $n$ observations, record Group 1 and Group 2, where each group takes on $> 2$ values, Test if there is an association between the groups
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: $n$ observations, record Group 1 ($r$ values) and Group 2$(c)$ values, Test if there is an association between the groups
  \begin{itemize}
   \item Test name: Pearsons $\chi^2$
   \item Test Statistic: $X = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$, where $E_{ij} = \frac{n_in_j}{N}$
   \item Test Reference Distribution: Under $H_0$, $X$ approx $\sim \chi^2_{(r-1)(c-1)}$
   \item Note not FSE, but performance is good if $E_{ij} > 5$
  \end{itemize}
 \end{field}
\end{note}




\begin{note} \begin{field} \tiny 100055 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name:
   \item Assumptions
   \item Process
   \item pvalue
   \item Test Reference Distribution
   \item Test Statistic:
   \item Ties
   \item Continuity correction
   \item Consistency
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name: Wilcoxon Rank-Sum (Mann-Whitney U-test)
   \item Note this is only a test of medians only if just additive effect - $F_x$ is just a shift from $F_y$ (shape and scale must be same ) (but then just the same as a test of mean, 10th percentile, min, $F_x = F_y$ etc )
   \item If No additive assumption - test of $H_0: P(X > Y) = .5$
   \item Process:
         \begin{itemize}
          \item Combine samples
          \item Rank the observations in combined sample from smallest to largest (1 to $n + m$)
          \item Add ranks of the smaller group (assume wlog that $X$ is the smaller group)
         \end{itemize}
   \item pvalue: Calculate using permutations: Count number of permutations that lead to a R value more extreme than observed out of total permutations ($\binom{n+m}{m}$)
   \item Test Statistic: $R$ sum of the ranks, or $z = \frac{R - \frac{m(m+n+1)}{2}}{\sqrt{\frac{mn(m+n+1)}{12}}}$
   \item Test Reference Distribution: If there was no difference between two populations, then each rank has equal chance of being assigned to group 1 (belongs to $X$: $p = \frac{m}{n+m}$)
         Normal approximation: $R \dot\sim N( \frac{m(m+n+1)}{2}, \frac{mn(m+n+1)}{12})$, $z \dot\sim N(0,1)$
   \item Notes: If ties, assign ranks, and then average ranks of tied values
   \item Continuity correction to normal distribution: add .5 to R if lower probability, subtract .5 from R if upper probability (ie 1 - pnorm())
   \item Not consistent test unless under additive assumption. IS consistent test of $H_0: P(X > Y) = .5$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100056 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test statistic:
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name: Mood's Test for Equality of Population Medians
   \item Process:
         \begin{itemize}
          \item Find combined sample median $\hat{m}$
          \item Calculate $\hat{p}_x = $ proportion of $X$s greater than $\hat{m}$, $\hat{p}_y$, proportion of $Ys$ greater than $\hat{m}$
          \item Conduct two sample binomial z-test( Pearsons chi-squared test) or Fisher's exact test
          \item  Test statistic: $$ z = \frac{\hat{p}_x - \hat{p}_y}{\sqrt{\hat{p}_c(1 - \hat{p}_c(\frac{1}{m} + \frac{1}{n}))}} $$ Where $\hat{p_c} = \frac{m\hat{p}_x + n\hat{p}_y}{m+n} = \frac{b + d}{N}$
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100057 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test some statistic $W$
  \begin{itemize}
   \item Test name:
   \item Process
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test some statistic $W$
  \begin{itemize}
   \item Test name: Permutation test
   \item Process: Permute group labels across observations and recalculate statistic for each permutation to create permutation distribution - calculate p-values using the permutation distribution
   \item Performance: Many settings (like medians equal), will not reject correctly (even in large samples) if the medians are equal, but the distributions differ
   \item Permutation hypothesis is that the observations from the two pouplations are exchangable (ie same population distributions, not just equal medians )
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100058 \end{field}
 \begin{field}
  Data setting: Estimate value of nuisance parameter
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: Bootstrap
   \item Process: Since the empirical distribution function converges to the true distribution function, we can use samples from the empirical distribution to approximate how samples from the true distribution would behave.
   \item Confidence interval: $100 (\alpha/2)$ largest resampled statistic $100(1 - (\alpha/2))$ largest resampled statistic
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100059 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $N$, $Y_1, \ldots, Y_n$ iid $N$. $H_0: \sigma_x^2 = \sigma_y^2$ or $H_0 \sigma_x^2/\sigma_y^2 = r$
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$ or $H_0 \sigma_x^2/\sigma_y^2 = r$
  \begin{itemize}
   \item Test name: F
   \item Recall $s_x^2 = \frac{1}{n-1}\sum_{i=1}^m(X_i - \bar{X})^2$
   \item Note that $\frac{(m-1)s_x^2}{\sigma_x^2} \sim \chi^2_{m-1}, \frac{(n-1)s_y^2}{\sigma_y^2} \sim \chi^2_{n-1},$
   \item Test Statistic: $F(r) = \frac{s_x^2/\sigma_x^2}{s_y^2/\sigma_y^2} = \frac{s_x^2}{s_y^2} \frac{1}{r}$
   \item Test Reference Distribution: Under $H_0: F(r) \sim F_{m-1,n-1}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item $\sigma_x^2/\sigma_y^2 > r$ Reject for $F(r) > F_{m-1,n-1}(1-\alpha)$
          \item $\sigma_x^2/\sigma_y^2 > r$ Reject for $F(r) > F_{m-1,n-1}(\alpha)$
          \item $\sigma_x^2/\sigma_y^2 \neq r$ Reject for $F(r) > F_{m-1,n-1}(1-\alpha/2)$ or $F(r) < F_{m-1,n-1}(\alpha/2)$
         \end{itemize}
   \item Performance: Not Well if underlying population is not normal: Not FSE or AE (but is consistent ) - don't use if population is not normal
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100060 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$
  \begin{itemize}
   \item Test name:
   \item Process:
   \item Interpretation
   \item Assumptions
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$
  \begin{itemize}
   \item Test name: Levene's Test
   \item Process:
         \begin{itemize}
          \item Construct new variables:
                \begin{itemize}
                 \item $U_i = |X_i - med(X)|$ or $(X_i - med(X))^2$ or $|X_i - \bar{X}|$ or $(X_i - \bar{X})^2$
                 \item $V_i = |Y_i - med(Y)|$ or $(Y_i - med(Y))^2$ or $|Y_i - \bar{Y}|$ or $(Y_i - \bar{Y})^2$
                \end{itemize}
          \item Perform two-sample $t$ test on $U_i$ and $V_i$ (use Welch)
         \end{itemize}
   \item Interpretation: If last option used, can be a test in difference in population variances
   \item Assumptions:
         \begin{itemize}
          \item Independence
          \item Large sample sizes, so t-test assumptions are met
         \end{itemize}
   \item Note: dont use as a test to determine which t-test version to use
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100061 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $H_0: F_x = F_y$
  \begin{itemize}
   \item Test name
   \item Test statistic

  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $H_0: F_x = F_y$
  \begin{itemize}
   \item Test name: Two-sample Kolmogorov-Smirnov Test
   \item Test statistic: $D = sup_x|\hat{F}_x(x) - \hat{F}_y(y)|$ ie the largest distance between the empirical CDF for $X$ and $Y$
   \item Reject for large values of $\sqrt{\frac{mn}{m+n}} D$
   \item Only for continuous distributions, for discrete distributions, use Pearsons $\chi^2$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100062 \end{field}
 \begin{field}
  Multiple 2x2 tables under $k$ different conditions $p_{xj} = P(X = 1 \text{ in Table }j), p_{yj} = P(Y = 1 \text{ in Table }j)$ $H_0: p_{xj} = p_{yj}$ for all $j$
 \end{field}

 \begin{field}
  \begin{itemize}
   \item Test name: Mantel-Haenszel Test
   \item Test statistic: $\omega_j = \frac{p_{xj}(1 - p_{xj})}{p_{yj}(1 - p_{yj})}$, $H_0: \omega_j = 1$ for all $j$
         $$E(n_{X1j}) = \mu_{X1j} = \frac{n_{X\cdot j}n_{\cdot 1 j}}{n_{\cdot j}}, V(n_{X1j}) = \sigma^2_{X1j} = \frac{n_{X\cdot j}n_{Y\cdot j}n_{\cdot 1j} n_{\cdot 0j}}{n^2_{\cdot \cdot j}(n_{\cdot \cdot j} -1)} $$
         $$ C = \frac{[\sum_{j}(n_{X1j} - \mu_{X1j})]^2}{\sum_j \sigma^2_{X1j}}$$
   \item Under $H_0$ $C \dot\sim \chi^2(1)$
   \item Assumes the odds-ratios are the same in all $k$ tables
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100063 \end{field}
 \begin{field}
  Test for data setting: Sample 1: $X_{1,1}, \ldots , X_{1n_1}$ from population 1 with mean $\mu_1$,\\
  Sample 2: $X_{2,1}, \ldots , X_{2n_2}$ from population 2 with mean $\mu_2$,\\ $\ldots $ Sample M: $X_{M,1}, \ldots , X_{Mn_M}$ from population M with mean $\mu_M$
  \begin{itemize}
   \item Independence within and between groups
   \item Populations (approximately ) normal
   \item Equal variances
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: ANOVA
   \item Estimate of common variance $s_p = \frac{(n_1-1)s_1^2 + \cdots + (n_M -1)s_M^2}{(n_1-1) + \cdots + (n_M-1)}$
   \item Could use two-sample-t test on two population means
   \item Could test are population means 1 through M equal to each other?
   \item Compare the variability between groups to the variability withing groups
   \item Sum of squares within groups:
         $$ SSW = (n-M)s_p^2  = \sum_{i=1}^{n_1}(X_{1i} - \bar{X}_1)^2 + \cdots +  \sum_{i=1}^{n_M}(X_{Mi} - \bar{X}_M)^2$$
         degrees of freedom: $n-M$
   \item Sum of squares total
         $$ SST  = \sum_{i=1}^{n_1}(X_{1,i} - \bar{X})^2 + \cdots + \sum_{i=1}^{n_M} (X_{M,i} - \bar{X})^2$$
         degrees of freedom: $n-1$
   \item Sum of squares between groups: $ SSB = SST - SSW = \sum_{j=1}^Mn_j(\bar{X}_j - \bar{X})^2$ df: $(n-1) - (n-M) = M-1$
   \item Test statistic: $$ F = \frac{MSB}{MSW} = \frac{SSB/(M-1)}{SSW/(n-M)} $$
   \item Reference distribution: Under $H_0, F \sim F_{M-1, n-M}$
  \end{itemize}
 \end{field}
\end{note}

%%end_tag

%%start_tag Methods 2 100064-100129
\tags{Methods2}

\begin{note} \begin{field} \tiny 100064 \end{field}
 \begin{field}
  Vectors $\mathbf{x}$ and $\mathbf{y}$ orthogonal
 \end{field}
 \begin{field}
  Vectors $\mathbf{x}$ and $\mathbf{y}$ orthogonal (perpendicular) if $(x,y) = \mathbf{x}^t \mathbf{y} = 0$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100065 \end{field}
 \begin{field}
  A matrix $\mathbf{A}$ is orthogonal if:
 \end{field}
 \begin{field}
  A matrix $\mathbf{A}$ is orthogonal if $\mathbf{A}^t \mathbf{A} = \mathbf{A} \mathbf{A}^t = \mathbf{I}_n$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100066 \end{field}
 \begin{field}
  A set of $n$ vectors are linearly dependent
 \end{field}
 \begin{field}
  A set of $n$ vectors are linearly dependent if there exist constants $c_1, \ldots c_n$ not all 0 such that $\sum_{j=1}^n c_j \mathbf{x})j = 0$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100067 \end{field}
 \begin{field}
  Inverse of a square matrix: $\mathbf{A}_{n\times n}$
 \end{field}
 \begin{field}
  The matrix that will satisfy $AA^{-1} = I$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100068 \end{field}
 \begin{field}
  Inverse of $\mathbf{A}$, $\mathbf{A}^{-1}$ where $\mathbf{A}$ is $2 \times 2$
 \end{field}
 \begin{field}
  $\mathbf{A}^{-1} = \begin{pmatrix}
    a & b \\ c & d
   \end{pmatrix}^{-1} = \frac{1}{ad - bc}\begin{pmatrix}
    d & -b \\ -c & a
   \end{pmatrix}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100069 \end{field}
 \begin{field}
  A square matrix is invertible if:
 \end{field}
 \begin{field}
  A square matrix is invertible if the columns (rows) are linearly independent. (If the columns are not independent, the matrix is called singular)
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100071 \end{field}
 \begin{field}
  Square of matrix $\mathbf{A}$
 \end{field}
 \begin{field}
  $\mathbf{AA}^t$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100072 \end{field}
 \begin{field}
  Norm of a vector $|\mathbf{x}|$
 \end{field}
 \begin{field}
  $|\mathbf{x}| = \sqrt{\sum_{j=1}^p x_j^2}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100073 \end{field}
 \begin{field}
  Determinant of a $2\times 2 $ matrix
 \end{field}
 \begin{field}
  $\bigg| \begin{pmatrix}
    a & b \\ c & d
   \end{pmatrix}\bigg| = ad - bc $
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100074 \end{field}
 \begin{field}
  Trace of a square matrix
 \end{field}
 \begin{field}
  Sum of the diagonal elements
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100075 \end{field}
 \begin{field}
  Rank of a matrix
 \end{field}
 \begin{field}
  Number of linearly independent columns
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100076 \end{field}
 \begin{field}
  Eigenvalue and eigenvector
 \end{field}
 \begin{field}
  $\lambda $ is an eigen value and  $\mathbf{u}_{n \times 2}$ is the eigen vector of $\mathbf{A}_{n \times n}$ if $\mathbf{Au} = \lambda \mathbf{u}$
  \begin{itemize}
   \item A real symmetric matrix has $n$ eigen values  and $n$ eigen vectors, and each are orthogonal to each other
   \item Roots of $det(\mathbf{A} - \lambda  \mathbf{I})$ determine the eigenvalues of $A$
  \end{itemize}
 \end{field}
\end{note}

% Come back to lecture 1 for more linear algebra notes

\begin{note} \begin{field} \tiny 100077 \end{field}
 \begin{field}
  Matrix properties
  \begin{itemize}
   \item $(AB)^{t} = $
   \item $(A+B)^t = $
   \item $(AB)^{-1} = $
   \item $(\mathbf{A}^{-1})^t = $
  \end{itemize}
 \end{field}
 \begin{field}
  Matrix properties
  \begin{itemize}
   \item $(AB)^{t} = B^tA^t$
   \item $(A+B)^t = A^t + B^t$
   \item For invertible matrices $(AB)^{-1} = B^{-1}A^{-1}$
   \item For invertible matrices $(\mathbf{A}^{-1})^t = (\mathbf{A}^t)^{-1}$
  \end{itemize}
 \end{field}
\end{note}




\begin{note} \begin{field} \tiny 100079 \end{field}
 \begin{field}
  $$ E(Y_i | X_{i1}, \ldots , X_{ip}) = $$
  Where $Y_i$ is the $i$th response and $X_{ij}$ is the $i$th value of the $j$th predictor
 \end{field}
 \begin{field}
  Since the error terms $\epsilon_i$ are independent and normally distributed with mean 0,
  $$ E(Y_i | X_{i1}, \ldots , X_{ip}) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_pX_{ip}$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100080 \end{field}
 \begin{field}
  Matrix form of linear Model and data
 \end{field}
 \begin{field}
  $$ \begin{pmatrix}
    Y_1 \\ Y_2 \\ \vdots \\ Y_n
   \end{pmatrix}  = \begin{pmatrix}
    1      & X_{11} & X_{12} & \cdots & X_{1p} \\
    1      & X_{21} & X_{22} & \cdots & X_{2p} \\
    \vdots & \vdots & \vdots &        & \vdots \\
    1      & X_{n1} & X_{n2} & \cdots & X_{np}
   \end{pmatrix} \begin{pmatrix}
    \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n
   \end{pmatrix}$$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100081 \end{field}
 \begin{field}
  Assumptions of a linear model
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Linearity: $E(\epsilon_i) = 0$ or $E(\epsilon) = \mathbf{0}$ or $E(\mathbf{Y}) = \mathbf{X}\beta$
   \item Constant variance $V(Y_i) = \sigma^2 = Var(\epsilon_i)$ or $V(\epsilon) = \sigma^2 \mathbf{I}_n$
   \item Normality $Y_i$ follows normal distribution, equivalently, $\epsilon_i$ follows normal distribution
   \item Independence $Y_i$ are indepednent equivalently under normality $Cov(\epsilon_i, \epsilon_j) = 0$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100082 \end{field}
 \begin{field}
  Interpretation of intercept of linear model
 \end{field}
 \begin{field}
  Mean response when all explanatory variables are 0
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100083 \end{field}
 \begin{field}
  Interpretation of slopes of linear model
 \end{field}
 \begin{field}
  Change in mean response for 1 unit change in the value of the explanatory, keeping all other variables constant. When $p = 2$
  $$ E(Y|X_1 + 1, X_2) - E(Y|X_1,X_2) = \beta_1$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100084 \end{field}
 \begin{field}
  Reason for $g-1$ indicator variables for a variable with $g$ values
 \end{field}
 \begin{field}
  The model matrix $X_{n \times(p+1)}$ needs to be full column rank - $\mathbf{X}^t \mathbf{X}$ needs to be non-singular
  If there is no intercept, we can include all groups, but interpretation will be different
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100085 \end{field}
 \begin{field}
  Interpretation of slope coefficient for indicator variable $\beta$
 \end{field}
 \begin{field}
  Difference in expected value of $Y$ between group value $a$ and $b$ where $a$ is the associated value for $\beta_j$ and $b$ is the base category
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100086 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\textbf{AU} + \textbf{b}) = $
   \item $V(\mathbf{AU + \mathbf{b}}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\textbf{AU} + \textbf{b}) = \mathbf{A} E(\mathbf{U}) + \mathbf{b}$
   \item $V(\mathbf{AU + \mathbf{B}}) = \mathbf{A}V(\mathbf{U}) \mathbf{A}^t$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100087 \end{field}
 \begin{field}
  Least squares estimate of $\beta$ (process to find )
 \end{field}
 \begin{field}
  Minimize the squared error loss ($L(\beta)$) with respect to $\beta$

  $$ L(\beta ) = \sum_{i=1}^n Y_i - (\beta_0 + \beta_1 X_{i1} + \cdots + \beta_p X_{ip}))^2 = (\mathbf{Y} - \mathbf{X}\beta)^t(\mathbf{Y} - \mathbf{X}\beta)$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100088 \end{field}
 \begin{field}
  $$  \frac{\partial}{\partial \beta} L(\beta) = $$
 \end{field}
 \begin{field}
  \begin{align*}
   \frac{\partial}{\partial \beta} L(\beta) & = \frac{\partial}{\partial \beta} (\mathbf{Y} - \mathbf{X}\beta)^t(\mathbf{Y} - \mathbf{X}\beta) \\
                                            & = \frac{\partial}{\partial \beta}  \mathbf{Y}^t \mathbf{Y} - \beta^t \mathbf{X}^t
   \textbf{Y} - \mathbf{Y}^t \mathbf{X}\beta - \beta^t \mathbf{X}^t \mathbf{X} \beta                                                           \\
                                            & = 0 - \mathbf{X}^t \mathbf{Y} - \mathbf{X}^t \mathbf{Y} + 2 \mathbf{X}^t \mathbf{X} \beta        \\
   \mathbf{X^t}\mathbf{X}\beta = \mathbf{X}^t \mathbf{Y}
  \end{align*}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100089 \end{field}
 \begin{field}
  Least squares estimate of $\hat{\beta}$
 \end{field}
 \begin{field}
  $$ \hat{\beta} = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y} $$ (if $\mathbf{X}^t \mathbf{X}$ is invertible )
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100090 \end{field}
 \begin{field}
  Residual
 \end{field}
 \begin{field}
  $e_i = Y_i - \hat{Y}_i$, $\mathbf{e}_{n\times 1} = \mathbf{Y} - \hat{\mathbf{Y}}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100091 \end{field}
 \begin{field}
  Vector of fitted values (linear regression )
 \end{field}
 \begin{field}
  $\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta} = \mathbf{X} (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100092 \end{field}
 \begin{field}
  Projection matrix
 \end{field}
 \begin{field}
  Hat matrix
  $$\mathbf{H_{n \times n}} = \mathbf{X} (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t$$
  $H_{ij}$ is the rate at which the $i$th fitted value changes as we vary the $j$th observation (influence )
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100093 \end{field}
 \begin{field}
  Properties of projection matrix
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $H$ and $\mathbf{I} - \mathbf{H}$ are symmetric matrices
   \item $\mathbf{HX} = X$
         item $(\mathbf{I} - \mathbf{X})\mathbf{X} = \mathbf{0}$
   \item $\mathbf{H}^2 = \mathbf{H}$
   \item $(\mathbf{I} - \mathbf{H})\mathbf{H} = 0$
   \item $\mathbf{X}^t \mathbf{e} = 0$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100094 \end{field}
 \begin{field}
  Unbiased estimate of $\sigma^2$
 \end{field}
 \begin{field}
  $\hat{\sigma}^2 = \frac{1}{n- (p+1)} \sum_{i=1}^n e_i^2 = \frac{1}{n- (p+1)} \mathbf{e}^t \mathbf{e}$
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100095 \end{field}
 \begin{field}
  $\mathbf{e}^t \mathbf{e} = $
 \end{field}
 \begin{field}
  $\mathbf{e}^t \mathbf{e} = \mathbf{Y}^t \mathbf{Y} - \mathbf{Y}^t \mathbf{HY}$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100096 \end{field}
 \begin{field}
  $E(\hat{\beta}) = $
 \end{field}
 \begin{field}
  $E(\hat{\beta}) = E((\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}) = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t E(\mathbf{Y}) = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{X}\beta  = \beta$

  So $\hat{\beta}$ is an unbiased estimate
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100097 \end{field}
 \begin{field}
  Gauss - Markov Theorem
 \end{field}
 \begin{field}
  If $E(\mathbf{Y}) = \mathbf{X}\beta$ and $V(\mathbf{Y}) = \sigma^2 \mathbf{I}$, then the least squares estimate $\hat{\beta}$ has the least variance among all linear unbiased estimators of $\beta$. (BLUE)

  Note that non-normal (or iid) residuals is not nescessary, just must be uncorrelated.
 \end{field}
\end{note}

% add proof of gauss markov


\begin{note} \begin{field} \tiny 100098 \end{field}
 \begin{field}
  $V(\hat{\beta}) = $
 \end{field}
 \begin{field}
  $V(\hat{\beta}) = \sigma^2 (\mathbf{X}^t \mathbf{X})^{-1}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100099 \end{field}
 \begin{field}
  $E(\hat{\sigma}^2) = $
 \end{field}
 \begin{field}
  $E(\hat{\sigma}^2) = \sigma^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100100 \end{field}
 \begin{field}
  If $\mathbf{X}_{p \times 1}$ has a multivariate normal distribution $N(\mu_{p\times 1}, \Sigma_{p \times p})$, then $\mathbf{AX} + b \sim $
 \end{field}
 \begin{field}
  If $\mathbf{X}_{p \times 1}$ has a multivariate normal distribution $N(\mu_{p\times 1}, \Sigma_{p \times p})$, then $\mathbf{AX} + \mathbf{b} \sim N(\mathbf{A}\mu + \mathbf{b}, \mathbf{A}\Sigma \mathbf{A}^t)$



 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100101 \end{field}
 \begin{field}
  Multivariate normal properties for $\mathbf{X}_{p \times 1} \sim N(\mu_{p\times 1}, \Sigma_{p \times p})$
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $Cov(X_j,X_k) = 0$ if and only if $X_j,X_k$ are independent (two way due to multivariate normal )
   \item All subsets of elements of $\mathbf{X}$ have a multivarite normal distribution
   \item All linear combinations of the components of $X$ are normally distributed
   \item $\mathbf{a}^t \mathbf{X} \sim N(\mathbf{a}^t, \mathbf{a}^t \Sigma \mathbf{a})$ for a vector $a$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100102 \end{field}
 \begin{field}
  Linear Hypothesis testing single parameter $H_0: \mathbf{c}^t \beta = d$
  \begin{itemize}
   \item $E(\mathbf{c}^t \beta)$, $V(\mathbf{c}^t \beta)= $
   \item Test statistic and distribution
   \item Item of setting up hypothesis test
   \item Rejection Region
  \end{itemize}
 \end{field}
 \begin{field}
  For a vector $\mathbf{c}_{(p+1)\times 1}$, we have that
  \begin{itemize}
   \item $E(\textbf{c}^t\hat{\beta}) = \mathbf{c}^t \beta$, $V(\mathbf{c}^t \hat{\beta}) = \sigma^2 \mathbf{c}^t (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}$
   \item Thus $$ \frac{\mathbf{c}^t \hat{\beta} - \mathbf{c}^t \beta }{\sigma \sqrt{\mathbf{c}^t(\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}}} \sim N(0,1)$$ and under $H_0$
         $$ T = \frac{\mathbf{c}^t \hat{\beta} - d}{ \sqrt{\hat{\sigma}^2\mathbf{c}^t(\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}}} \sim t_{n - (p+1)}$$
   \item Example: testing $H_0: \beta_1 = \beta_2, \mathbf{c} = (0,1,-1)^t, d = 0$
   \item Reject $H_a: c^t \beta \neq d:  |T| > t_{n - (p+1)}(1-\alpha/2)$\\ $c^t \beta > d, T > t_{n-(p+1)}(\alpha)$\\ $c^t\beta < d: T < t(1 - \alpha)$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100103 \end{field}
 \begin{field}
  Confidence interval for a single parameter (linear regression slope estimate)
 \end{field}
 \begin{field}
  $$ \hat{\beta}_j \pm t_{n-(p-1)}(1 - \alpha/2)\sqrt{\hat{\sigma}^2((\mathbf{X}^t \mathbf{X})^{-1})_{j+1,j+1}} $$
  $$ \mathbf{c}^t \beta \pm t_{n-(p-1)}(1 - \alpha/2)\sqrt{\hat{\sigma}^2 \mathbf{c}^t((\mathbf{X}^t \mathbf{X})^{-1})\mathbf{c}}  $$
  eg if we were testing $\beta_1 - \beta_2, c = (0,1,-1)$
 \end{field}
\end{note}





\begin{note} \begin{field} \tiny 100104 \end{field}
 \begin{field}
  F statistic in matrix form
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $\mathbf{K}$  is $p\times k$, $\mathbf{m}$ is $k\times 1$
   \item Testing $H_0: \mathbf{K}^t\beta = \mathbf{m}$
   \item $F = \frac{\big((\mathbf{K}\hat{\beta} - \mathbf{m})^t (\mathbf{K}(\mathbf{X}^t \mathbf{X})^{-1}\mathbf{K}^{-1})(\mathbf{K}\hat{\beta} - \mathbf{m})\big)}{k\hat{\sigma}^2} \sim F_{k,n-p}$
   \item Eg$K = \begin{pmatrix}
           0 \\ 1 \\ \vdots \\ 0
          \end{pmatrix}$, $m = 0$
   \item Tests $\beta_1 = 0$
   \item Note the $\mathbf{K}^t$ matrix is the coefficients of the system of linear equations for the the null hypothesis, and $m$ is what they are equal to
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100105 \end{field}
 \begin{field}
  Overall regression F-test
 \end{field}
 \begin{field}
  Tests if any predictors are related to the response
  \begin{itemize}
   \item Full model: $\mathbf{Y} = \mathbf{X}\beta + \epsilon$
   \item Reduced model a nested model with $q$ estimated parameters
   \item eg: Reduced model: $\mathbf{Y} = \beta_0 + \epsilon$, $q = 1$
   \item $H_0: \beta_1 =  \ldots = \beta_p = 0$
   \item $F = \frac{(RSS_\omega - RSS_\Omega)/(p-q)}{RSS_\Omega/(n-p)}$
   \item
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100106 \end{field}
 \begin{field}
  Analysis of Variance Table and calculated F stat
 \end{field}
 \begin{field}
  \begin{tabular}{|c|c|c|c|}
   \hline                                                                                                \\
   Type       & df      & Sum of Squares                & Mean SS                                        \\
   \hline                                                                                                \\
   Regression & $p$     & SS(Reg)                       & SS(Reg)/p                                      \\
   Residual   & $n-p+1$ & SS(Res)                       & $\hat{\sigma}^2 =$ SS(Res)/$n-p-1$             \\
   Total      & $n-1$   & SS(Total) = SS(Reg) + SS(Res) & $\frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2$ \\
   \hline
  \end{tabular}
  and $F = \frac{Mean(SSREG)}{Mean(SSRES)}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100107 \end{field}
 \begin{field}
  Distribution of $\hat{\beta}$, where $\hat{\beta}$ are the estimated coefficients of linear regression.
 \end{field}
 \begin{field}
  $\hat{\beta} \sim N(\beta, \sigma^2(\mathbf{X}^t \mathbf{X})^{-1})$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100108 \end{field}
 \begin{field}
  RSS (in terms of $\Omega$ and $\omega$)
 \end{field}
 \begin{field}
  $$RSS_\Omega = \sum_{i=1}^n e_i^2$$
  $$RSS_\omega = \sum_{i=1}^n (Y_i - \bar{Y})^2$$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100109 \end{field}
 \begin{field}
  $R^2$
 \end{field}
 \begin{field}
  $R^2 = \frac{SS(Reg)}{SS(Tot)} = 1 - \frac{SS(Res)}{SS(Tot)}$

  Where SS(Reg) is the regression sum of square: $\sum_{i} (\hat{y}_i - \bar{y})^2$ (fitted minus mean)
  and SS(Tot) or TSS is the total sum of squares $\sum_{i} (y_i - \bar{y})^2$
  and SS(Res) (or error sum of squares) $SS_E$ or RSS is the residual sum of squares $\sum_i (y_i - \hat{y}_i)^2 = \sum_{i} e_i$

  And SS(Tot) = SS(Res) + SS(Reg)
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100110 \end{field}
 \begin{field}
  Properties of the estimate of $\sigma^2$
 \end{field}
 \begin{field}
  \begin{itemize}
   \item   $\hat{\sigma}^2 = \frac{|\mathbf{e}|^2}{n - (p+1)}$
   \item Under normality: $\frac{(n - (p+1))\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n - (p+1)}$
   \item $\hat{\sigma}^2$ is independent from $\hat{\beta}$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100111 \end{field}
 \begin{field}
  Prediction Interval
 \end{field}
 \begin{field}
  Predicting a future response
  $\mathbf{x}_0^t \hat{\beta} \pm t_{n-p}(\alpha/2)\hat{\sigma} \sqrt{1 + x_0^t(X^tX)^{-1}x_0}$

  A 95\% prediction interval for a response with (list values) is between and
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100112 \end{field}
 \begin{field}
  Confidence interval
 \end{field}
 \begin{field}
  Confidence in mean response $\mathbf{x}_0^t \hat{\beta} \pm t_{n-p}(\alpha/2)\hat{\sigma^2} \sqrt{x_0^t(X^tX)^{-1}x_0}$
  With 95\% confidence, the expected mean response
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100113 \end{field}
 \begin{field}
  Residual Plot
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Plot residuals against fitted values (so there is only 1 plot vs against explanatory variables)
   \item Verifies linearity and constant variance
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100114 \end{field}
 \begin{field}
  Leverege
 \end{field}
 \begin{field}
  \begin{itemize}
   \item An obervarion has high leverage if the explanatory variable values of the observation are different from general pattern
   \item $h_i = H_{ii} = (X(X^tX)^{-1}X^t)_{ii}$
   \item High leverage $h_i > \frac{2(p+1)}{n}$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100115 \end{field}
 \begin{field}
  Standardized Residual
 \end{field}
 \begin{field}
  $r_i = \frac{e_i}{\hat{\sigma}\sqrt{1 - h_i}}$
  Large if $|r_i| > 2$ - indicates outlier
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100116 \end{field}
 \begin{field}
  Influential - if fitted model depends highly on the value
 \end{field}
 \begin{field}
  Measure using cook's distance
  $$ D_i = \frac{(\hat{Y} - \hat{Y}_{(i)})^t(\hat{Y} - \hat{Y}_{(i)})}{(p+1)\hat{\sigma}^2} = \frac{1}{p+1}r_i^2 \frac{h_i}{(1 - h_i)}$$
  Where $Y_{i}$ is the vector of fitted values when the model is fitted to the data without the $i$ ths observationl Moderate if $>1 $ Large if $>6$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100117 \end{field}
 \begin{field}
  Multicollinearity
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $X^tX$ is close to singular
   \item Some columns are highly correlated
   \item there is a relationship between predictors \item leads to large standard errors
   \item Not a violation of assumptions, but leads to issues in interpretations
   \item Calculate using Condition number if $>30$ than large , or Variance inflation factors $VIF_j = \frac{1}{1 - R^2_j}$ where $R^2_j$ is $R^2$ from regression of the $j$th explanatory variable on all the other explanatory variables
   \item Not a problem for prediction
   \item Fix using selection of explanatory variables, generalized inverse, ridge regression
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100118 \end{field}
 \begin{field}
  Ridge Regression
 \end{field}
 \begin{field}
  $\hat{\beta} = (X^tX + \lambda I)^{-1} X^t Y$, where $\lambda$ is chosen. Note these are biased estimators
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100119 \end{field}
 \begin{field}
  Fix non-constant spread/variance
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Transform response (box-cox)
   \item Use more complicated model (glm)
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100120 \end{field}
 \begin{field}
  Fix non-linearity
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Transform response
   \item Transform predictor
   \item allow for curvature: predictor squared, splines, gam
   \item use a non linear model
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100121 \end{field}
 \begin{field}
  Fix Non-normality
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Transform response
   \item more complicated models : glm
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100122 \end{field}
 \begin{field}
  Missing data completely at random (MCAR)
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Throwing out cases with missing data does not bias inferences
   \item There’s no relationship between whether a data point is missing and any values in the data set, missing or observed.
   \item The missing data are just a random subset of the data.
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100123 \end{field}
 \begin{field}
  Missing at random (MAR)
 \end{field}
 \begin{field}
  \begin{itemize}
   \item the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data.
   \item Probability of missingness depends only on available information, like the explanatory variables and the response variables present in the regression - impute missing data
   \item A better name would actually be Missing Conditionally at Random, because the missingness is conditional on another variable.
  \end{itemize}

 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100124 \end{field}
 \begin{field}
  Model Selection methods
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Sequential Methods: Backward/Forward (eliminate untill all values have p-value below critical value) Elimination
   \item Penalized Regression: Ridge and Lasso
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100125 \end{field}
 \begin{field}
  AIC
 \end{field}
 \begin{field}
  Estimate the distance of a candidate model from the true model (small good)
  $$ n \log (RSS/n) + 2(p+1)$$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100126 \end{field}
 \begin{field}
  BIC
 \end{field}
 \begin{field}
  Estimate the best parsimonious model, using a prior distribution on the parameters (small good)
  $$  n \log (RSS/n) + \log(n)(p+1)$$

  Where $n$ is the number of observations, $p$ is the number of predictors (not including intercept), and $RSS = \sum (Y_i - \hat{Y})^2 = \sum e_i^2 $
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100127 \end{field}
 \begin{field}
  Adjusted $R^2$
 \end{field}
 \begin{field}
  Adjusts for multiple parameters
  $$ 1 - \frac{n-1}{n-p}(1 - R^2)$$ (large is good)
  (where p includes the intercept)

  $\frac{MS(Reg)}{MS(Total)} = 1 - \frac{SS(Reg)/(n - p - 1)}{SS(Tot)/(n-1)}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100128 \end{field}
 \begin{field}
  Mallow's Cp
 \end{field}
 \begin{field}
  $$ RSS/\hat{\sigma^2} + 2p - n $$ (small good)
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100129 \end{field}
 \begin{field}
  Box-Cox Transformation
 \end{field}
 \begin{field}
  Transform so model is $g(Y) = X\beta + \epsilon$
  where $g(y) = \frac{y^\lambda -1}{\lambda} if \lambda \neq 0, 0 $ otherwise
 \end{field}
\end{note}


%%end_tag ends on 100129

%%start_tag Methods 3 Starts on 100130
\tags{Methods3}

% Lecture 2
\begin{note}
 \begin{field}
  \tiny 100130
 \end{field}
 \begin{field}
  Components of an experiment
 \end{field}
 \begin{field}
  Experimental units, treatment, design (how eus are allocated to treatments)
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100130
 \end{field}
 \begin{field}
  Model and assumptions for CRD
 \end{field}
 \begin{field}
  Model and assumptions for Completely randomized design
  $$y_{ij} = \mu_i + \epsilon_{ij}$$

  Where
  \begin{itemize}
   \item $y_{ij}$ is the response on the $j$th eu in the $i$th group
   \item $\mu_i$ is the population mean in the $i$th group
   \item $\epsilon_{ij}$ is the random error for the $j$th eu in the $i$th group
   \item Assume $\epsilon_{ij} \sim iid N(0,\sigma^2)$
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100131
 \end{field}
 \begin{field}
  Point estimate of $\hat{\mu}_i$
 \end{field}
 \begin{field}
  $\hat{\mu}_i = \bar{y}_i$ = mean in the $i$th group
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100132
 \end{field}
 \begin{field}
  Point estimate of
  $\hat{\sigma}^2$
 \end{field}
 \begin{field}
  \begin{align}
   \hat{\sigma}^2 & = MSE = \frac{\text{error sum of squares}}{df} = \frac{\text{residual SS}}{df} \\
                  & = \frac{\sum_{i=1}^g \sum_{j=1}^{n_i}(y_{ij} - \bar{y}_{i\cdot})^2}{N-g}       \\
                  & = s^2
  \end{align}

  Where
  \begin{itemize}
   \item $g$ is the number of groups
   \item $N$ is the overall sample size
   \item $n_i$ the number of eus in the $i$th group
   \item Df = sample size - number of parameters = $N-g$
   \item $i$th residual = $y_{ij} - \hat{y}_{ij} = y_{ij} - \bar{y}_{i\cdot }$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100133
 \end{field}
 \begin{field}
  Hypothesis test and interval estimates for $\mu_i$ in CRD
 \end{field}
 \begin{field}
  $\hat{\mu}_i = \bar{y}_i$ = sample mean of $y_{i1}, \cdots y_{in_{i}} \sim iid N(\mu_i,\sigma^2)$

  $$ \bar{y}_i \sim N(\mu_i, \frac{\sigma^2}{n_i})$$

  $SE(\bar{y}_i) = \sqrt{\frac{s^2}{n_i}}$

  CI: $\bar{y}_i \pm t_{(a/2,N-g)} \sqrt{\frac{s^2}{n_i}}$

  $H_0: \mu_i = 0$
  $t = \frac{\bar{y}_i}{\sqrt{s^2/n_i}} \sim t_{(N-g)}$

 \end{field}
\end{note}

% Lecture 3
\begin{note}
 \begin{field}
  \tiny 100134
 \end{field}
 \begin{field}
  Cell Means Parametrization
  (eg $g = 3, n_i = 2$)
 \end{field}
 \begin{field}
  $$ \begin{pmatrix}
    y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\y_{32}
   \end{pmatrix}  = \begin{pmatrix}
    1 & 0 & 0 \\
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    0 & 0 & 1
   \end{pmatrix} \begin{pmatrix}
    \mu_{1} \\ \mu_2 \\ \mu_3
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_{11} \\ \vdots \epsilon_{32}
   \end{pmatrix}$$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100135
 \end{field}
 \begin{field}
  Regression parametrization (eg $g = 3, n_i = 2$)
 \end{field}
 \begin{field}
  Code categorical variables using indicators
  $y_{ij} = \beta_0 + \beta_1X_{1,ij} + \beta_2X_{2,ij} + \epsilon_{ij}$
  $$ \begin{pmatrix}
    y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\y_{32}
   \end{pmatrix}  = \begin{pmatrix}
    1 & 1 & 0 \\
    1 & 1 & 0 \\
    1 & 0 & 1 \\
    1 & 0 & 1 \\
    1 & 0 & 0 \\
    1 & 0 & 0
   \end{pmatrix} \begin{pmatrix}
    \beta_0 \\ \beta_1 \\ \beta_2
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_{11} \\ \vdots \\ \epsilon_{32}
   \end{pmatrix}$$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100136
 \end{field}
 \begin{field}
  Factor (Treatment) Effects Parametrization
 \end{field}
 \begin{field}
  $$ y_{ij} = \mu + \alpha_i + \epsilon_{ij}$$
  Where
  \begin{itemize}
   \item $\mu$ = overall mean: average of $\mu_i$
   \item $\alpha_i$ = effect of level $i$ of the treatment factor, deviation away from $\mu$ associated with the $i$th treatment
  \end{itemize}

  $$ \begin{pmatrix}
    y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\y_{32}
   \end{pmatrix}  = \begin{pmatrix}
    1 & 1  & 0  \\
    1 & 1  & 0  \\
    1 & 0  & 1  \\
    1 & 0  & 1  \\
    1 & -1 & -1 \\
    1 & -1 & -1
   \end{pmatrix} \begin{pmatrix}
    \mu \\ \alpha_1 \\ \alpha_2
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_{11} \\ \vdots \epsilon_{32}
   \end{pmatrix} $$

  Note that $\alpha_3 = -\alpha_1 - \alpha_2$
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100137
 \end{field}
 \begin{field}
  Extra Sum of Squares $F$- test
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Compares full and reduced models
         $$ F = \frac{(SS_E(\text{red}) - SS_E(\text{full}))/(df(\text{red}) - df(\text{full}))}{SS_E(\text{full})/df(\text{full})}$$
   \item Can use to test for differences across the group means
         $$ H_0: \mu_1 = \ldots = \mu_g = \mu$$
         $$H_A: \mu_i \neq \mu_j  \text{ for some } i \neq j$$
   \item Reduced model: $y_{ij} = \mu + \epsilon_{ij}$
   \item Full model: $y_{ij} = \mu_i + \epsilon_{ij}$
   \item $SS_E = \sum_{j}(y_j - \hat{y}_j)^2 = $ residual SS
   \item $SS_E(\text{full}) = \sum_{i = 1}^g \sum_{j = 1}^{n_i}(y_{ij} - \bar{y}_{i\cdot})^2$ Where $\bar{y}_{i\cdot}$ is the fitted value for obs in $i$th group
   \item $SS_E(\text{red}) = \sum_{i=1}^g \sum_{j=1}^{n_i}(y_{ij} - \bar{y}_{\cdot \cdot})^2$ where $\bar{y}_{\cdot\cdot} = \sum_{i}\sum_j y_{ij}/N$ mean of all obs
   \item df(full) = $N-g$
   \item df(red) = $N-1$
   \item SSE(red) - SSE(full) = SSTreatment for CRD
   \item Reduced model will have more unexplained variation
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100138
 \end{field}
 \begin{field}
  CRD ANOVA Table
 \end{field}
 \begin{field}
  \begin{tabular}{c|c c c c}
             & DF    & SS      & MS              & F                                \\
   \hline                                                                           \\
   Treatment & $g-1$ & SS(Trt) & SS(Trt)/$(g-1)$ & MS(trt)/MS(E) $\sim F_{g-1,N-g}$ \\
   Error     & $N-g$ & SS(E)   & SS(E)/$(N-g)$   &                                  \\
   \hline                                                                           \\
   Total     & $N-1$ & SS(T)   &                 &
  \end{tabular}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100139
 \end{field}
 \begin{field}
  Distribution of SS(Total)/$\sigma^2$, SS(Treatment)/$\sigma^2$ and SS(E)/$\sigma^2$
 \end{field}
 \begin{field}
  $\chi^2_{N-1}$, $\chi^2_{g-1}$ , $\chi^2_{N-g}$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100140
 \end{field}
 \begin{field}
  $E(MS_{\text{Trt}}) = $
 \end{field}
 \begin{field}
  $$E(MS_{\text{Trt}}) = $$
  \begin{itemize}
   \item If $H_0$ true, then $E(MS_{\text{Trt}}) = \sigma^2$
   \item If $H_A$ true, then $E(MS_{\text{Trt}})> E(MS_E)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100141
 \end{field}
 \begin{field}
  $E(MS_{\text{E}}) = $
 \end{field}
 \begin{field}
  $E(MS_{\text{E}}) = \sigma^2$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100142
 \end{field}
 \begin{field}
  Contrast
 \end{field}
 \begin{field}
  A contrast is a linear combination of treatment means where the coefficients sum to 0
  $C = \sum_{i=1}^g w_i \mu_i$ where $\sum_{i=1}^g w_i = 0$
  Examples:
  \begin{itemize}
   \item $\frac{\mu_1+\mu_2 +\mu_3 }{3} - \mu_4$, $C = 1/3,1/3,1/3,-1$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100143
 \end{field}
 \begin{field}
  Hypothesis test of contrast
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $\hat{C} = \sum_{i=1}^g w_i \bar{y}_{i\cdot}$
   \item $V(\hat{C}) = V\bigg(\sum_{i=1}^g w_i\bar{y}_{i\cdot}\bigg) = \sum_{i=1}^g w_i^2 \frac{\sigma^2}{n_i}$
   \item $\hat{V}(\hat{C}) = \sum_{i=1}^g w_i^2 \frac{MS_E}{n_i} =  \sum_{i=1}^g w_i^2 \frac{\hat{\sigma}^2}{n_i} $
   \item CI: $\hat{C} \pm t_{(1 - \alpha/2, N-g)}SE(\hat{C})$
   \item $t = \frac{\hat{C} - 0}{SE(\hat{C})} \sim t_{N-g}$
   \item Eg if $C = \mu_1 - \mu_4$ a test of $C = 0$ is testing $\mu_1 = \mu_4$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100144
 \end{field}
 \begin{field}
  Contrast sums of squares
 \end{field}
 \begin{field}
  $SS_{\text{Contrast}} = SS_E(\text{reduced}) - SS_E(\text{full})$

  \begin{itemize}
   \item The full model is the separate means model $y_{ij} = \mu_i + \epsilon_{ij}$
   \item The reduced model is the full model with the restriction $H_0: C = 0$ imposed on the $\mu_i$
   \item Eg: $C = \frac{\mu_1 + \mu_2 + \mu_3}{3} = \mu_4$
         Full model parameter vecotr $(\mu_1, \cdots, \mu_4)^t$, reduced model parameter vector: $(\mu_1, \mu_2, \mu_3)$ with $\mu_4 = \frac{\mu_1+ \mu_2 + \mu_3}{3}$
   \item df full = $N-4$, df reduced = $N-3$, df contrast = 1 = $(N-3) - (N-4)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100145
 \end{field}
 \begin{field}
  Orthogonal contrasts
 \end{field}
 \begin{field}
  Contrasts $C_1$ and $C_2$ are orthogonal if $\sum_{i=1}^g \frac{w_i w_i^*}{n_i} = 0$

  We usually only consider orthogonal contrasts when $n_i = n$ (balanced design)

  With $g$ treatments, we can have at most $g-1$ orthogonal contrasts

  If contrasts are orthogonal $SS(trt) = SS(C1) + \ldots + SS(C_{g-1})$
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100146
 \end{field}
 \begin{field}
  Orthogonal polynomial contrasts and polynomial regression
 \end{field}
 \begin{field}
  \begin{itemize}
   \item When data are balanced and treatments are incremental and equally spaced, we can use orthogonal polynomial contrasts
   \item With $g$ treatments, fit a $g-1$ degree polynomial model. Fitted polynomial will fit each treatment mean exactly
   \item The $g-1$ degree polynomial is another parametrization of the separate means model
   \item The cell means model ignores the incremental nature of treatment - polynomial one doesnt
   \item Polynomial models imply something about interpolation
   \item ex: $y_{ij}  = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \beta_3 X_i^3 + \epsilon_{ij}$ $X_i$ is the amount of treatment in the $i$th group.
   \item SS(trt) = SS(linar) + SS(quad) + SS(cubic)
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100147
 \end{field}
 \begin{field}
  Design matrix for orthogonal polynomial contrasts
 \end{field}
 \begin{field}
  $$ X = \begin{pmatrix}
    1      & 0      & 0^2    & 0^3    \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 50     & 50^2   & 50^3   \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 100    & 100^2  & 100^3  \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 150    & 150^2  & 150^3  \\
    \vdots & \vdots & \vdots & \vdots \\
   \end{pmatrix}$$
  $ \beta = (\beta_0, \beta_1, \beta_2, \beta_3)^t$
 \end{field}
\end{note}

% Lecture 8

\begin{note}
 \begin{field}
  \tiny 100148
 \end{field}
 \begin{field}
  Per comparison error rate
 \end{field}
 \begin{field}
  \begin{itemize}
   \item P(reject $H_{0i}$) when $H_{0i}$ is true
   \item Usual $\alpha$
   \item No correction for multiple comparisons
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100149
 \end{field}
 \begin{field}
  Experimentwise error rate
 \end{field}
 \begin{field}
  $\alpha_E = $ P(reject at least one $H_{0i}$) when $H_0$ is true (all $H_{0i}$ true )
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100150
 \end{field}
 \begin{field}
  False Discovery rate (FDR)
 \end{field}
 \begin{field}
  $FDR = \frac{\text{number false rejections}}{total number rejections}$,
  or 0 when no rejections

  Allows more incorrect rejections as the number of true rejections increases
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100151
 \end{field}
 \begin{field}
  Strong familywise error rate
 \end{field}
 \begin{field}
  $\alpha_F = P(\text{at least one false rejection}) = P(FDR > 0)$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100152
 \end{field}
 \begin{field}
  Tradeoff of multiple comparisons
 \end{field}
 \begin{field}
  Stronger error control - less powerful test
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100153
 \end{field}
 \begin{field}
  Bonferroni correction
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $K$ comparisons
   \item Fix $\alpha_F = P(\text{at least one false rejection})$ and set per comparison error rate $\alpha = \alpha_F/K$
   \item Reject $H_{0i}$ if its p value is less than $\alpha_F/K$
   \item Very strict, but easy test
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100154
 \end{field}
 \begin{field}
  Holm multiple comparison
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $K$ comparisons
   \item Sort individual p-values from small to large $p_1, \ldots, p_k$
   \item Reject $H_{0i}$ if $p_i < \frac{\alpha_F}{K-i+1}$
   \item Note $\frac{\alpha_F}{K-i+1} \geq \frac{\alpha_F}{K}$, so Holm is more powerful than Bonferroni , but still conservative
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100155
 \end{field}
 \begin{field}
  Multiple comparison method: FDR
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $K$ comparisons
   \item Sort p-values
   \item Reject $H_{0i}$ if $p_i < \frac{i \cdot FDR}{K}$
   \item Controls the false discovery rate
  \end{itemize}
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100156
 \end{field}
 \begin{field}
  Scheffes method
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Only method that controls $\alpha_F$ if we've snooped the data
   \item Tests all possible contrasts (all are 0 )
   \item very conservative
   \item Reject $H_{0i}: C_i = 0$ if $$\frac{SS_{C_i}(g-1)}{MS_E} > F_{\alpha_F, g-1, N-g} $$
   \item Confidence interval: $$\hat{C_i} \pm \sqrt{(g-1)F_{\alpha_F, g-1, N-g}}SE(\hat{C_i}) $$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100157
 \end{field}
 \begin{field}
  Multiple comparison for pairwise comparisons
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Contrasts of the form $\mu_i - \mu_j$
   \item For $g$ treatment groups there are $\binom{g}{2}$ possible pairwise comparisons
   \item Tukey's Honestly Significant Difference (balanced)
   \item Tukey-Kramer (not balanced )
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100157
 \end{field}
 \begin{field}
  Tukey's Honestly Significant Difference (HSD)
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Pairwise comparisons
   \item Simultaneous tests and CIs of all $C = \mu_i - \mu_j$
   \item Controls $\alpha_F$
   \item CI: $$\bar{y}_{i\cdot} - \bar{y}_{j\cdot} \pm q_{\alpha_F, g, N-g}\sqrt{\frac{MS_E}{n}} $$
   \item Assumes $n$ observations in each group (balanced)
   \item Where $q$ is the studentized range distribution - dividing a statistic by the estimate of its standard error
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100158
 \end{field}
 \begin{field}
  Tukey-Kramer
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Pairwise comparison
   \item If the data are not balanced (but close)
   \item Replace $\sqrt{\frac{MS_E}{n}}$ with $\sqrt{MS_E \frac{n_i + n_j}{2n_in_j}}$ in
   \item  CI: $$\bar{y}_{i\cdot} - \bar{y}_{j\cdot} \pm q_{\alpha_F, g, N-g}\sqrt{\frac{MS_E}{n}} $$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100159
 \end{field}
 \begin{field}
  Ryan-Einot-Gabriel-Welsch Range (REGWR) test
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Controls $\alpha_F$
   \item Stepdown procedure
   \item Order sample means from small to large
   \item Test ranges, starting with largest range $\mu_{(1)} = \mu_{(g)}$
   \item If fail to reject, stop, conclude that no means differ. Otherwise stop down and test next largest ranges. etc
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100160
 \end{field}
 \begin{field}
  Dunnett's Procedure
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Compare all treatments to control
   \item
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100161
 \end{field}
 \begin{field}
  Multiple Comparisons with the best $MCB$
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Identifies either $max(\mu_i)$ or $min(\mu_i)$
   \item Intervals either contain 0 (not different from best) or have 0 as an endpoint, which implies they are different from the best.
   \item Usually done with ANOVA
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100162
 \end{field}
 \begin{field}
  Difference between Type I and Type III Sum of Squares
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Type I is a nested model - variables are added
   \item Type III removes one variable
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100163
 \end{field}
 \begin{field}
  Effect of non-normality ("Robustness")
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If tails are too long (compared to normal) estimate of variance will be too large, inference will be conservative (CI to wide, p-values too big, type I error smaller than $\alpha$, lower power)
   \item If tails are too short, reverse is true
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100164
 \end{field}
 \begin{field}
  Equal variance diagnostics
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Levene's test
   \item Plot residuals vs fitted values
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100165
 \end{field}
 \begin{field}
  Effect of non-constant variance + Remedy
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If data are balance and variances are not too unequal, standard procedures work pretty well
   \item If data are unbalanced and large $n_i$ corresponds to larger variances, procedures too conservative
   \item Small $n_i$ correspond to large variances, opposite
   \item Remedy using Welch's ANOVA/weighted least squares, larger balanced sample
  \end{itemize}
 \end{field}
\end{note}

% Lecture 12

\begin{note}
 \begin{field}
  \tiny 100166
 \end{field}
 \begin{field}
  RF Plot
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Residual-Fit Spread plot
   \item Left plot has sorted centered fits $\hat{y}_{ij} - \hat{\bar{y}}_{ij}$
   \item Right plot has sorted residuals $y_{ij} - \hat{y}_{ij}$
   \item Left plot shows variability explained by the model
   \item Right plot shows unexplained variability
   \item Want spread of left plot to be larger than right plot - indicates we have a good model
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100167
 \end{field}
 \begin{field}
  Sample size to perform a 2 sample z test
 \end{field}
 \begin{field}
  For a 2 sample $z$ test $H_0: \mu_1 = \mu_2$ with $\sigma^2$ known
  $$ n \geq 2(z_{\alpha/2} + z_{\beta})^2 \frac{\sigma^2}{\delta^2}$$
  \begin{itemize}
   \item $n$ sample size in each group
   \item $\alpha = $ Type I error rate
   \item $\beta = $ Type II error rate = 1-Power
   \item $z_{\alpha/2}= $ standard normal $1 - \alpha/2$ quantile
   \item $z_{\beta} = $ standard normal $1 - \beta$ quantile
   \item $\sigma^2 = $ common variance
   \item $\delta = \mu_1 - \mu_2 $
  \end{itemize}
 \end{field}
\end{note}

% Lecture 13

\begin{note}
 \begin{field}
  \tiny 100168
 \end{field}
 \begin{field}
  Sample size for one-way ANOVA
 \end{field}
 \begin{field}
  Depends on the distribution when $H_A$ is the case - non central F distribution - to find sample size, simulate repeated sampling under $H_A$ to calculate power for different $N$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100169
 \end{field}
 \begin{field}
  $2\times 2$ Factorial design difference from ANOVA
 \end{field}
 \begin{field}
  ANOVA fits a model like, for group 1 with treatments C,F and group2 treatments HL
  \begin{tabular}{|c |c |c |c |}
   CH & CL & FH & FL \\
   \hline            \\
      &    &    &    \\
   \hline
  \end{tabular}

  (ignores the structure of treatments )

  Factorial design:
  \begin{tabular}{c|c|c|c|}
          &   & Liquid &   \\
          &   & L      & H \\
   \hline                  \\
   Screen & C &        &   \\
          & F &        &   \\
   \hline
  \end{tabular}

  Uses contrasts
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100170
 \end{field}
 \begin{field}
  Interaction plot
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If the interaction contrast is 0, then the lines will be parallel
   \item If we see non parallel lines, it indicates there is an interaction
   \item Parallel lines associated with large p values of interaction term
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100171
 \end{field}
 \begin{field}
  Model for a $2\times 2$ factorial design
 \end{field}
 \begin{field}
  $y_{ijk}$ is response from the $k$th replicate with $i$th level of factor A, and $j$th level of factor B


  eg:

  \begin{center}
   \begin{tabular}{|c|c|c|c|}
    \hline                              \\
      &         & B         &           \\
      &         & $j=1$     & $j = 2$   \\
    \hline                              \\
    A & $i = 1$ & $y_{11k}$ & $y_{12k}$ \\
      & $i=2$   & $y_{21k}$ & $y_{22k}$ \\
    \hline
   \end{tabular}
  \end{center}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100172
 \end{field}
 \begin{field}
  Cell means parametrization for $2 \times 2$ factorial design
 \end{field}
 \begin{field}
  $$y_{ijk} = \mu_{ij} + \epsilon_{ijk}$$



  $y_{ijk}$ is response from the $k$th replicate with $i$th level of factor A, and $j$th level of factor B

  $\epsilon_{ijk} \sim N(0,\sigma^2)$
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100173
 \end{field}
 \begin{field}
  Factor effects parametrization for $2 \times 2$ design
 \end{field}
 \begin{field}
  $$y_{ijk} = \mu + \alpha_i +\beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}$$
  Where
  \begin{itemize}
   \item $\mu$ is the overall mean
   \item $\alpha_i$ effect of $i$th level of factor A
   \item $\beta_j$ effect of $j$th level of factor B
   \item $(\alpha\beta)_{ij}$ interaction of $i$th level of A and $j$th level of B
   \item Where $\epsilon_{ijk} \sim N(0,\sigma^2)$
   \item $0 = \sum_{i=1}^2 \alpha_i = \sum_{j=1}^2\beta_j = \sum_{i=1}^2 (\alpha\beta)_{ij} = \sum_{i=1}^2(\alpha\beta)_{ij}$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100174
 \end{field}
 \begin{field}
  Equivalence of cell means and factor effects parametrizations
 \end{field}
 \begin{field}
  \begin{center}
   \begin{tabular}{c| c | c | }
          & $j=1$                                                                                                      & $j=2$                                                                                                      \\
    \hline                                                                                                                                                                                                                          \\
    $i=1$ & $\mu_{11} = \mu + \alpha_1 + \beta_1 + (\alpha\beta)_{11}$                                                 & $\mu_{12} = \mu + \alpha_1 + \beta_2 + (\alpha\beta)_{12}
     = \mu + \alpha_1 - \beta_1 - (\alpha\beta)_{11}$                                                                                                                                                                               \\
    $i=1$ & $\mu_{21} = \mu + \alpha_2 + \beta_1 + (\alpha\beta)_{21} = \mu - \alpha_1 + \beta_1 - (\alpha\beta)_{11}$ & $\mu_{22} = \mu + \alpha_2 + \beta_2 + (\alpha\beta)_{22} = \mu - \alpha_1 - \beta_1 + (\alpha\beta)_{11}$
   \end{tabular}
  \end{center}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100175
 \end{field}
 \begin{field}
  Design matrix for $2 \times 2$ factorial design, where each group has 2 options
 \end{field}
 \begin{field}
  $$ \begin{pmatrix}
    y_{111} \\ \vdots \\ y_{11n} \\ y_{121} \\ \vdots \\
    y_{12n} \\ y_{211}\\ \vdots \\ y_{21n} \\ y_{221} \\ \vdots \\
    \\ y_{22n}
   \end{pmatrix} =
   \begin{pmatrix}
    1      & 1      & 1      & 1      \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 1      & -1     & -1     \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & -1     & 1      & -1     \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & -1     & -1     & 1      \\
    \vdots & \vdots & \vdots & \vdots \\
   \end{pmatrix}
   \begin{pmatrix}
    \mu \\ \alpha_1 \\ \beta_1 \\ (\alpha\beta)_{11}
   \end{pmatrix} + \epsilon $$
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100176
 \end{field}
 \begin{field}
  General model for a 2-factor design
 \end{field}
 \begin{field}
  $y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}$
  \begin{itemize}
   \item A has levels $1 \cdots a$
   \item B has levels $1 \cdots b$
   \item $\epsilon_{ijk} \sim N(0,\sigma^2)$
   \item $0 = \sum_{i=1}^a \alpha_i = \sum_{j=1}^b \beta_j = \sum_{i=1}^a (\alpha\beta)_{ij} = \sum_{j=1}^b (\alpha\beta)_{ij}$
   \item There are $a \times b $ parameters to estimate
   \item $a-1 \alpha_i$s , $b-1 \beta_j$s, $(a-1)(b-1) (\alpha\beta)_{ij}$s = ab total parameters
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100177
 \end{field}
 \begin{field}
  Parameter estimates for $2 \times 2$ factorial design
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $\hat{\mu} = \bar{y}_{\cdot\cdot \cdot}$
   \item $\hat{\alpha}_i = \hat{\mu}_{i\cdot} - \hat{\mu} = \bar{y}_{i\cdot \cdot} - \bar{y}_{\cdot \cdot \cdot }$
   \item $\hat{\beta}_j =\hat{\mu}_{\cdot j} - \hat{\mu} = \bar{y}_{\cdot j \cdot} - \bar{y}_{\cdot \cdot \cdot } $
   \item $\hat{(\alpha\beta)}_{ij} = \hat{\mu}_{ij} - \hat{\alpha}_i - \hat{\beta}_j - \hat{\mu}$
   \item Where :


   \item $\mu_{i\cdot }$ population mean for $i$th level of factor A
   \item $\mu_{\cdot j}$ population mean for $j$th level of factor B
   \item $\alpha_i$ deviation from the overall mean associated with $i$th level of factor A
   \item $(\alpha\beta)_{ij}$ deviation of cell mean from the row column and overall mean
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100178
 \end{field}
 \begin{field}
  ANOVA for 2 factor design - Hypothesis test interpretation
 \end{field}
 \begin{field}
  Degrees of freedom:
  \begin{itemize}
   \item $A: a - 1$
   \item $B: b-1$
   \item $AB: (a-1)(b-1)$
   \item Error $N - ab$
   \item Total $N-1$
  \end{itemize}
  Each row in anova sum of squares table gives the F value for if that row was zero, ie test all $\alpha_i = 0$ indicates that that factor has no effect
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100179
 \end{field}
 \begin{field}
  General factorial design (eg $8 \times 2 \times 2$)
 \end{field}
 \begin{field}
  $$ y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + (\alpha\beta)_{ij} + (\alpha\gamma)_{ik} + (\beta\gamma)_{jk} + (\alpha\beta\gamma)_{ijk} + \epsilon_{ijkl} $$


  Where
  \begin{itemize}
   \item $y_{ijkl}$ response for $l$th replicate at $i$th level of A, $j$th level of $B$ and $k$th level of C
   \item $\mu$ overall mean
   \item $\alpha_i$ effect of $i$th level of A
   \item $(\alpha\beta)_{ij}$ interaction of A and B
   \item $(\alpha\beta\gamma)$ interaction of A and B and C
   \item $\epsilon_{ijkl} iid \sim N(0,\sigma^2)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100180
 \end{field}
 \begin{field}
  Type I Type II Type III Sum of squares (unbalanced )
 \end{field}
 \begin{field}
  \begin{itemize}
   \item When data are unbalanced Type I and Type III SS are different
   \item I is sequential
   \item II is partial
   \item III is hierarchical
  \end{itemize}

  \begin{center}
   \begin{tabular}{|c|c|c|c|}
    Type & SS & Effects in full model & Effects in reduced model \\
    \hline                                                       \\
    I    & A  & A                     & intercept only           \\
    III  & A  & A,B,C,AB,AC,BC,ABC    & B,C,AB,AC,BC,ABC         \\
    II   & A  & A,B,C,BC              & B,C,BC                   \\
    III  & AB & A,B,C,AB,AC,BC,ABC    & A,B,C,AC,BC,ABC          \\
    II   & AB & A,B,C,AB,AC,BC        & A,B,C,AC,BC
   \end{tabular}
  \end{center}
 \end{field}
\end{note}


%%end_tag

\end{document}
