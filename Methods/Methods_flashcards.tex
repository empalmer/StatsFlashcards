% -*- coding: utf-8 -*-
\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}
\usepackage{mathrsfs}
\pagestyle{empty}
\setlength{\parindent}{0in}

\newenvironment{note}{\paragraph{NOTE:}}{}
\newenvironment{field}{\paragraph{field:}}{}
\newenvironment{tags}{\paragraph{tags:}}{}

\begin{document}

%%start_tag Methods 1 100000 - 1000063
\tags{Methods1}
\begin{note} \begin{field} \tiny 100000 \end{field}
 \begin{field}
  Epidemiology Definition of Causation
 \end{field}
 \begin{field}
  Factor/variable $X$ \textbf{causes} result $Y$ if some cases of $Y$ would not have occurred if X had been absent.
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100001 \end{field}
 \begin{field}
  Sample variance
 \end{field}
 \begin{field}
  $s^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i - \bar{x})^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100002 \end{field}
 \begin{field}
  Population(s) of interest
 \end{field}
 \begin{field}
  The group to which you would like your answer to apply
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100003 \end{field}
 \begin{field}
  Variable of Interest
 \end{field}
 \begin{field}
  A measurement that can be made on each individual/member of the population
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100004 \end{field}
 \begin{field}
  Facts about Normal Distributions
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If $Z$ has a Normal(0,1) distribution then $X = \sigma Z + \mu$ has a Normal$(\mu,\sigma^2)$ distribution
   \item If $X$ has a Normal($\mu,\sigma^2$) distribution, then $Z = \frac{X - \mu}{\sigma}$ has a Normal(0,1) distribution.
   \item If $X$ has a Normal($\mu_x,\sigma^2_x)$ distribution, and $Y$ has a Normal($\mu_y,\sigma_y^2$) distribution, and $X$ and $Y$ are independent of each other, then $X + Y \sim $ Normal($\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100005 \end{field}
 \begin{field}
  Sample mean
 \end{field}
 \begin{field}
  $\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100006 \end{field}
 \begin{field}
  Sampling distribution for population $Y \sim $ Normal($\mu,\sigma^2$)
 \end{field}
 \begin{field}
  $N(\mu,\sigma^2/n)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100007 \end{field}
 \begin{field}
  Variance (Expected value)
 \end{field}
 \begin{field}
  $V(Y) = E[(X - E(X))^2] = E(X^2) - E[(X)]^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100008 \end{field}
 \begin{field}
  Covariance
 \end{field}
 \begin{field}
  $Cov(X,Y) = E[(X - E(X))(Y - E(Y))]$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100009 \end{field}
 \begin{field}
  If $X$ and $Y$ are independent (covariance)
 \end{field}
 \begin{field}
  The covariance is 0
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100010 \end{field}
 \begin{field}
  If $Cov(X,Y) = 0$, (independence)
 \end{field}
 \begin{field}
  Cannot say that $X$ and $Y$ are independent
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100011 \end{field}
 \begin{field}
  $Cov(X,X) =$
 \end{field}
 \begin{field}
  $Var(X)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100012 \end{field}
 \begin{field}
  $X \sim N(\mu,\sigma^2)$
  \begin{itemize}
   \item $E(\bar{X}) = $
   \item $V(\bar{X}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\bar{X}) = \mu$
   \item $V(\bar{X}) = \sigma^2/n$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100013 \end{field}
 \begin{field}
  Central Limit Theorem (in words)
 \end{field}
 \begin{field}
  If the population distribution of a variable $X$ has population mean $\mu$ and finite population variance $\sigma^2$, then the sampling distribution of the sample mean becomes closer and closer to a Normal distribution as the sample size $n$ increases: $\bar{X} \sim N(\mu,\sigma^2/n)$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100014 \end{field}
 \begin{field}
  Central Limit Theorem (theoretical)
 \end{field}
 \begin{field}
  Let $X_1, X_2, \ldots X_n$ be an iid sample from some poupation distribution $F$ with mean $\mu$ and variance $\sigma^2 < \infty$. Then as the sample size $n \to \infty$, we have $$\frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} \to N(0,1)$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100015 \end{field}
 \begin{field}
  $X \sim (\mu,\sigma^2)$
  \begin{itemize}
   \item $E(\bar{X}) = $
   \item $V(\bar{X}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\bar{X}) = \mu$
   \item $V(\bar{X}) = \sigma^2/n$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100016 \end{field}
 \begin{field}
  Reject $H_0$ when $H_0$ True
 \end{field}
 \begin{field}
  Type I error (false positive)
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100017 \end{field}
 \begin{field}
  Type I error (false positive)
 \end{field}
 \begin{field}
  Reject $H_0$ when $H_0$ True
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100018 \end{field}
 \begin{field}
  Fail to Reject $H_0$ when $H_0$ false
 \end{field}
 \begin{field}
  Type II error
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100019 \end{field}
 \begin{field}
  Type II error
 \end{field}
 \begin{field}
  Fail to Reject $H_0$ when $H_0$ false
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100020 \end{field}
 \begin{field}
  Significance level
 \end{field}
 \begin{field}
  $\alpha$ the probability of a Type I error
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100021 \end{field}
 \begin{field}
  Power (at $\theta_1$)
 \end{field}
 \begin{field}
  Probability of rejecting the null hypothesis when $\theta_1$ is the truth
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100022 \end{field}
 \begin{field}
  Test for data setting: $X_1, X_2, \ldots X_n$ iid with sample mean $\bar{X}$, and known population variance $\sigma^2$, Null hypothesis $\mu = \mu_0$

  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value
         \begin{itemize}
          \item Lower
          \item Upper
          \item Two sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  z-test
  \begin{itemize}
   \item Test statistic: $Z(\mu_0) = \frac{\bar{X} - \mu_0}{\sqrt{\sigma^2/n}}$
   \item Reference Distribution: Under $H_0, Z(\mu_0) \sim N(0,1)$
         \begin{itemize}
          \item Lower: Reject when $Z(\mu_0) < z_{\alpha}$ = qnorm($\alpha$)
          \item Upper: Reject when $Z(\mu_0) > z_{1 - \alpha} = $qnorm(1-$\alpha$)
          \item Two sided:  Reject when $|Z(\mu_0)| > z_{1 - \alpha/2}$ = qnorm(1 - $\alpha/2$)
         \end{itemize}
   \item Confidence interval: $ \bar{X} \pm z_{1 - \alpha/2}\sqrt{\frac{\sigma^2}{n}}$
   \item pvalue:
         \begin{itemize}
          \item upper: 1 - $\Phi(z)$ = 1 - pnorm(z)
          \item lower: $\Phi(z)$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item Consistent: Yes /Finite Sample Exact: Yes if $X_i \sim N$/ Asymptotically Exact: Yes
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100023 \end{field}
 \begin{field}
  Exactness (finite/asymptotic)
 \end{field}
 \begin{field}
  Under any setting for which the null hypothesis is true, is the actual rejection probability equal to the desired level $\alpha$?
  \begin{itemize}
   \item Finite Sample Exact: for sample size $n$ is $P(Reject H_0) = \alpha$ when $H_0$ is true?
   \item Asymptotic Exactness: As $n \to \infty$ does $P(Reject H_0) \to \alpha$ when $H_0$ is true?
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100024 \end{field}
 \begin{field}
  When is a test exact?
 \end{field}
 \begin{field}
  \begin{itemize}
   \item A test is FSE if the reference distribution is the true distribution of the test statistic $T$ when $H_0$ is true
   \item A test is AE if the reference distribution is the asymptotic distribution of the test statistic when $H_0$ is true.
   \item (Distribution of p-values should be Unif(0.1))
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100025 \end{field}
 \begin{field}
  Consistency
 \end{field}
 \begin{field}
  When $H_0$ is false (the alternative hypothesis is true), does the rejection probability (probability reject the null) tend to 1 as $n \to \infty$?
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100026 \end{field}
 \begin{field}
  Interpretation of Confidence intervals
 \end{field}
 \begin{field}
  $(1 - \alpha)100$\% of the time, intervals constructed in this manner will include $\mu$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100027 \end{field}
 \begin{field}
  Test for data setting: $X_1, X_2, \ldots X_n$ iid with sample mean $\bar{X}$, and unknown population variance , Null hypothesis $\mu = \mu_0$
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: t-test
   \item Test Statistic: $t(\mu_0) = \frac{\bar{X} - \mu_0}{\sqrt{s^2/n}}$
   \item Test Reference Distribution: $t_{n-1}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: Reject if $t(\mu_0) > t_{(n-1),1-\alpha}$ = qt(1 - $\alpha$,n-1)
          \item lower: Reject if $t(\mu_0)  < t_{n-1,\alpha}$
          \item two sided: Reject if $|t(\mu_0)| > t_{n-1, 1 - \alpha/2}$
         \end{itemize}
   \item Confidence interval: $\bar{X} \pm t_{n-1,1-\alpha/2}\sqrt{\frac{s^2}{n}}$
   \item pvalue, with $t(\mu_0)  = t$, and pt representing the cdf of a t distribution
         \begin{itemize}
          \item upper: 1 - pt($t,n-1$)
          \item lower: pt($t$,n-1)
          \item two-sided: 2*(1 - pt(abs(t)),n-1)
         \end{itemize}
   \item Consistent Yes/Finite Sample Exact Yes if normal/ Asymptotically Exact Yes
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100028 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$ iid Bernoulli(p) (option 1), parameter of interest $p$
  \begin{itemize}
   \item Test name
   \item Test Statistic, Test Reference Distribution
   \item Critical Value/ Rejection region:
         upper, lower, two-sided
   \item Confidence interval + pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$ iid Bernoulli(p), parameter of interest $p$
  \begin{itemize}
   \item Test name: Exact Binomial Test (uses the distribution of the sum of Bern(p) RVs)
   \item Test Statistic: $X = \sum_{i=1}^n Y_i = n\bar{Y}$
   \item Test Reference Distribution: Under $H_0$ Binomial$(n,p_0)$
   \item Critical Value/ Rejection region: Sometimes use randomized test
         \begin{itemize}
          \item upper: Reject $H_0$ for $X \geq c$ for c such that $P(X \geq c)\leq \alpha$
          \item lower: Reject $H_0$ for $X \leq c$ for c such that $P(X \leq c)\leq \alpha$
          \item two-sided: Reject $H_0$ for $p_0(X)\leq c$ for $c$ such that$P_{H_0}(p_0(X) \leq c)\leq \alpha$, where $p_0(X)$ is $P(X = x)$ under $H_0$
         \end{itemize}
   \item Confidence interval: Values that are not rejected
   \item pvalue: Sum of the probabilities that are less than or equal to the observed value (under the null hypothesis)
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100029 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$, parameter of interest: $p$ iid Bernoulli(p) (option 2)
  \begin{itemize}
   \item Test name
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $Y_1, \ldots, Y_n$, parameter of interest: $p$ iid Bernoulli(p) (option 2)
  \begin{itemize}
   \item Test name: Binomial $z$-test (Use when $np_0 > 5$ and $n(1-p_0) > 5$)
   \item Test Statistic: $X  = \sum_{i=1}^n = n\bar{Y}$, $\hat{p} = X/n$, $z(p_0) = \frac{\hat{p}- p_0}{\sqrt{p_0(1-p_0)/n}}$ (score)
   \item Test Reference Distribution: Under $H_0$, Approximately $X \sim N(np_0,np_0(1-p_0))$ and $z(p_0) \sim N(0,1)$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: $z(p_0) > z_{1-\alpha}$
          \item lower: $z(p_0) < z_\alpha$
          \item two-sided: $|z(p_0)| > z_{1-\alpha/2}$
         \end{itemize}
   \item Confidence interval: Uses wald interval (derived from t-test) (with $z_w(p_0) = \frac{\hat{p} - p_0}{\sqrt{\hat{p}(1 - \hat{p})/n}}$) $\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}$
   \item pvalue
         \begin{itemize}
          \item upper: 1 - $\Phi(z(p_0))$ = 1 - pnorm($z(p_0)$)
          \item lower: $\Phi(z(p_0))$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z(p_0)|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item Consistent: Yes/Finite Sample Exact: No/ Asymptotically Exact: Yes
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100030 \end{field}
 \begin{field}
  Continuity correction for Binomial z-test
 \end{field}
 \begin{field}
  With $X \sim Binom(n,p)$, instead of $P(X \leq x)$, use $P(W \leq x + 1/2)$ where $W \sim N(np,np(1-p))$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100031 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item If ties?
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: Sign Test
   \item Test Statistic: $Y_i = I(X_i < M_0)$ , $\hat{p}_{M_0} = \frac{\sum Y_i}{n}$ (proportion of observations less than or equal to hypothesized median)
   \item Test Reference Distribution: Normal distribution: with $p_0 = .5$
   \item Critical Value/ Rejection region: $z = \frac{\hat{p}_{M_0} - p_0}{\sqrt{p_0(1-p_0)/n}}$
         \begin{itemize}
          \item upper: $z > z_{1-\alpha}$
          \item lower: $z < z_\alpha$
          \item two-sided: $|z| > z_{1-\alpha/2}$
         \end{itemize}
   \item Confidence interval: cant use the binomial proportion CI
         Set of values of $M_0$ that wouldn't be rejected at level $\alpha$

         $$ \bigg( \frac{n - z_{1 - \alpha/2} \sqrt{n}}{2}\bigg)^{th} \text{ Smallest Observation}, \bigg( \frac{n - z_{1 - \alpha/2} \sqrt{n}}{2}\bigg)^{th}\text{ Smallest Observation} $$
   \item pvalue (binomial test on proportion)
         \begin{itemize}
          \item upper: 1 - $\Phi(z(p_0))$ = 1 - pnorm($z(p_0)$)
          \item lower: $\Phi(z(p_0))$ = pnorm(z)
          \item two-sided: $2(1 - \Phi(|z(p_0)|))$ = 2*(1 - pnorm(abs(z)))
         \end{itemize}
   \item If there are ties: remove all observations equal to $M_0$, then test prop of observations $< M_0$ given not equal to $M_0$ is .5
   \item Consistent: yes/Finite Sample Exact: No / Asymptotically Exact: yes
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100032 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$ (option 2)
  \begin{itemize}
   \item Test name:
   \item Assumptions
   \item Procedure:
   \item Test Statistic, Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $M$ - median $H_0: M = M_0$ (option 1)
  \begin{itemize}
   \item Test name: Wilcoxon signed-rank test
   \item Assumptions: symmetry - equivalently a test of the mean. otherwise tests the pseudo-median
   \item Procedure: testing $c_0$ is the center (median)
         \begin{itemize}
          \item Calculate distance of each observation from $c_0$
          \item Rank observations by the distance (abs value) from $c_0$
         \end{itemize}
   \item Test Statistic: $S$ sum of the ranks that correspond to observations larger than $c_0$, $Z = \frac{S - \frac{n(n+1)}{4}}{\sqrt{\frac{n(n+1)(2n+1)}{24}}} \sim N(0,1)$
   \item Test Reference Distribution:
         \begin{itemize}
          \item Exact p-value - assume each rank has the same chance of being assigned to observations above or below $c_0$ - all possible ways to assign the ranks
          \item Normal approximation to the null distribution $S \sim N\big(\frac{n(n+1)}{4}, \frac{n(n+1)(2n+1)}{24}\big)$
         \end{itemize}
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper:
          \item lower:
          \item two-sided
         \end{itemize}
   \item Confidence interval
   \item pvalue - Same as for Normal
   \item Consistent Yes under symmetry assumption /Finite Sample Exact No/ Asymptotically Exact Yes (under symmetry assumption)
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100033 \end{field}
 \begin{field}
  Pseudomedian
 \end{field}
 \begin{field}
  Median of the distribution of sample means from samples of size 2
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100034 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid N($\mu,\sigma^2$) parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item pvalue
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid N($\mu,\sigma^2$) parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name: $\chi^2$ for Population Variance
   \item Test Statistic $X(\sigma_0) = \frac{(n-1)s^2}{\sigma_0^2}$
   \item Test Reference Distribution: Under $H_0: X(\sigma_0) = \frac{(n-1)s^2}{\sigma_0^2} \sim \chi_{n-1}^2$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item  $\sigma^2 > \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) > \chi^2_{n-1,(1-\alpha)}$
          \item $\sigma^2 < \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) < \chi^2_{n-1,(\alpha)}$
          \item $\sigma^2 \neq \sigma_0^2$ Reject $H_0$ for $X(\sigma_0^2) > \chi^2_{n-1,(1 - \alpha/2)}$ or $X(\sigma_0) < \chi^2_{n-1,(\alpha/2)}$
         \end{itemize}
   \item Confidence interval  $$ \bigg( \frac{(n-1)s^2}{\chi^2_{n-1,1 - \alpha/2}}, \frac{(n-1)s^2}{\chi^2_{n-1,(\alpha/2)}}\bigg)$$
   \item pvalue
         \begin{itemize}
          \item $\sigma^2 > \sigma_0^2$: $p = 1 - pchisq(X(\sigma_0)^2,n-1)$
          \item $\sigma^2 < \sigma_0^2: p = pchisq(X(\sigma_0^2),n-1)$
          \item $\sigma^2 \neq \sigma_0^2: p = 2\min(1 - pchisq(X(\sigma_0^2), n-1), pchisq(X(\sigma_0^2)),n-1)$
         \end{itemize}
   \item Consistent/Finite Sample Exact/ Asymptotically Exact
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100035 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid \\
  Parameter of interest: $\sigma^2 = Var(X)$,\\
  Sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, \\
  $H_0: \sigma^2 = \sigma_0^2$ (asymptotic)
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item pvalue
  \end{itemize}
 \end{field}
 \begin{field}
  Data Setting: $X_1, \ldots, X_n$, iid parameter of interest: $\sigma^2 = Var(X)$, sample variance: $s^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$, $H_0: \sigma^2 = \sigma_0^2$
  \begin{itemize}
   \item Test name: Asymptotic $t$-test for population variance
   \item Test Statistic: $Y = (X_i - \bar{X})^2$, $$ t(\sigma_0^2)  = \frac{\bar{Y - \frac{n-1}{n}\sigma_0^2}}{\sqrt{s_y^2/n}} \to N(0,1)$$
         Note $\bar{Y} = \frac{n-1}{n}s^2$
   \item Tests that the population mean of the $Y_i$ is $\frac{n-1}{n}\sigma_0^2$
   \item Test Reference Distribution $ \frac{\frac{n-1}{n}s^2 - \frac{n-1}{n}\sigma^2}{\sqrt{Var(\frac{n-1}{n}s^2)}} = \frac{\bar{Y}- \frac{n-1}{n}\sigma^2}{\sqrt{Var(\bar{Y})}} \to N(0,1)$, so we can use t-test
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item upper: Reject if $t(\sigma_0^2) > t_{(n-1),1-\alpha}$ = qt(1 - $\alpha$,n-1)
          \item lower: Reject if $t(\sigma_0^2)  < t_{n-1,\alpha}$
          \item two sided: Reject if $|t(\sigma_0^2)| > t_{n-1, 1 - \alpha/2}$
         \end{itemize}
   \item Confidence interval: $\bar{X} \pm t_{n-1,1-\alpha/2}\sqrt{\frac{s^2}{n}}$
   \item pvalue, with $t(\mu_0)  = t$, and pt representing the cdf of a t distribution
         \begin{itemize}
          \item upper: 1 - pt($t,n-1$)
          \item lower: pt($t$,n-1)
          \item two-sided: 2*(1 - pt(abs(t)),n-1)
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100036 \end{field}
 \begin{field}
  Test for data setting $X_1, \ldots X_n$ iid from population distribution $F$. Test $H_0: F = F_0$
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Test for data setting $X_1, \ldots X_n$ iid from population distribution $F$. Test $H_0: F = F_0$
  \begin{itemize}
   \item Test name: Kolmogorov-Smirnov Test
   \item Process
   \item Test Statistic: $D(F_0) = sup_x|\hat{F}(x) - F_0(x)$, where $\hat{F}(x) = \frac{1}{n}\sum_{i=1}^n 1(X_i \leq x)$ is the empirical cdf and $F_0(x)$ is the null hypothesis cdf (maximum values of difference between emperical and null)
   \item Test Reference Distribution: Kolmogorov distribution
   \item Critical Value/ Rejection region: Reject for large values of $\sqrt{n}D(F_0)$
   \item Note the one sided version does not have an easy interpretation
  \end{itemize}
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100037 \end{field}
 \begin{field}
  Data setting: $X_1, \ldots , X_n$ iid from discrete distribution. Test fit of distribution
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item If parameter values of discrete distribution are not known
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: $X_1, \ldots , X_n$ iid from discrete distribution. Test fit of distribution
  \begin{itemize}
   \item Test name: $\chi^2$ goodness of fit test, test for discrete distributions
   \item Process: Test the underlying population distribution is $P(X = x) = p_0(x)$, where $\hat{p}(x) = \frac{1}{n} \sum_{i=1}^n 1(X_i = x)$
         \begin{itemize}
          \item Let $j = 1, \ldots, k$ the different categories that $X_i$ can take
          \item Let $O_j$ be the observed number of observations that belong to category $j$
          \item Let $E_j = np_0(j)$ be the expected number of observations that would belong to category $j$ if the null hypothesis were true
         \end{itemize}
   \item Test Statistic: $X(p_0) = \sum_x\frac{n(\hat{p}(x) - p_0(x))^2}{p_0(x)} = \sum_{j=1}^k \frac{(O_j - E_j)^2}{E_j}$
   \item Test Reference Distribution: Under $H_0$, $X(p_0) \to \chi_{k-1}^2$
   \item Critical Value/ Rejection region: Reject for large values of $X(p_0)$ - Reject $H_0$ for $X(p_0) > \chi_{k-1}^2(1-\alpha)$
   \item Note: Null hypothesis doesnt completely specify the distribution, just the family of distributions with perhaps unknown parameters
         \begin{itemize}
          \item Estimate the parameters
          \item Use null distribution with estimated parameter values for $E_j$
          \item Compute $\chi^2$ test statistic
          \item Compare to $\chi_{k-d-1}^2$ distribution where $k$ = number of categories, $d = $ number of estimated parameters
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100038 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with known $\sigma_x, \sigma_y$. Estimate $d = \mu_x - \mu_y$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item p-value
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with known $\sigma_x, \sigma_y$. Estimate $d$,
  \begin{itemize}
   \item Test name: 2 sample $z$ test
   \item Test Statistic: $z(d_0) = \frac{(\bar{X}- \bar{Y}) - d_0}{\sqrt{\frac{\sigma_x^2}{m} + \frac{\sigma_y^2}{n}}}$
   \item Test Reference Distribution: Under $H_0$, $z(d_0) \sim N(0,1)$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item Lower: $d \leq d_0$ Reject when $z(d_0) < z_{\alpha}$ = qnorm($\alpha$)
          \item Upper: $d \geq d_0$ Reject when $z(d_0) > z_{1 - \alpha} = $qnorm(1-$\alpha$)
          \item Two sided: $d \neq d_0$ Reject when $|z(d_0)| > z_{1 - \alpha/2}$ = qnorm(1 - $\alpha/2$)
         \end{itemize}
   \item Confidence interval: $$ (\bar{X} - \bar{Y}) \pm z(1 - \frac{\alpha}{2})\sqrt{\frac{\sigma_x^2}{m} + \frac{\sigma_y^2}{n}} $$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100039 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with unknown but equal  $\sigma_x, \sigma_y$ Estimate $d$
  \begin{itemize}
   \item Test name:
   \item Estimate of $\sigma_x^2 = \sigma_y^2$
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item When not equal
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$, $Y_1, \ldots, Y_m$ iid with unknown $\sigma_x, \sigma_y$. Estimate $d$
  \begin{itemize}
   \item Test name: Equal variance 2-sample t-test
   \item Note: Estimate of $\sigma_x^2 = \sigma_y^2 = s_p^2 = \frac{\sum_{i=1}^m (X_i - \bar{X})^2 + \sum_{i=1}^n (Y_i - \bar{Y})}{(m-1) + (n-1)} = \frac{(m-1)s_x^2 + (n-1)s_y^2}{(m+n-2)}$ (weighted average of the two sample variances )
   \item Test Statistic: $t(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{s_p^2(\frac{1}{m} + \frac{1}{n})}}$
   \item Test Reference Distribution: For Normal populations, under $H_0$: $t(d_0) \sim t_{m+n-2}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item $d > d_0$ Reject $H_0$ for $t_e(d_0) > t_{m+n-2}(1 - \alpha)$
          \item $d < d_0$ Reject $H_0$ for $t_e(d_0) < t_{m+n-2}(\alpha)$
          \item $d \neq d_0$ Reject $H_0$ for $|t_e(d_0)| > t_{m+n-2}(1 - \alpha/2)$
         \end{itemize}
   \item Confidence interval $ (\bar{X} - \bar{Y}) \pm t_{m+n-2}(1 - \frac{\alpha}{2})\sqrt{s_p^2(\frac{1}{m} + \frac{1}{n})}$
   \item When not equal:
         \begin{itemize}
          \item Expected value of Estimated variance is larger than it should be when the smaller sample comes from the population with smaller variance - the test statistic will be closer to zero than it should be, and rejection rates will be smaller - Less power - more conservative
          \item Expected value of Estimated variance is smaller than it shoudl be when smaller sample comes from the population with the larger variance - test statistic will have a larger absolute value than it should an rejection rates will be larger  - more power - anti conservative
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100040 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with unknown not equal  $\sigma_x, \sigma_y$ Estimate $d = \mu_x - \mu_y$
  \begin{itemize}
   \item Test name:
   \item Estimate of $Var(\bar{X} - \bar{Y})$
   \item Test Statistic
   \item Test Reference Distribution
   \item Degrees of freedom
   \item Critical Value/ Rejection region
   \item Confidence interval
   \item Compare to equal variance
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$, $Y_1, \ldots, Y_n$ iid with unknown not equal equal  $\sigma_x, \sigma_y$ Estimate $d$
  \begin{itemize}
   \item Test name: Unequal variance 2 sample t-test
   \item Estimate of $Var(\bar{X} - \bar{Y}) = \frac{s_x^2}{m} + \frac{s_y^2}{n}$
   \item Test Statistic: $t_U(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{s_x^2}{m} + \frac{s_y^2}{n}}}$
   \item Test Reference Distribution: If the two distributions are Normal, there is not an exact distribution for the test statistic -  Use Welch-Satterthwaite approximation: Estimate degrees of freedom
         $$ v = \frac{\big(\frac{s_x^2}{m} + \frac{s_y^2}{n}\big)^2}{\frac{s_x^4}{m^2(m-1)} + \frac{s_Y^4}{n^2(n-1)}}$$
         $\min(m-1,n-1) \leq v \leq m+n-2$
         Under $H_0$ $t_u(d_0) $ approx $\sim t_{v}$
   \item Critical Value/ Rejection region: same as t-test
   \item Confidence interval: $(\bar{X} - \bar{Y}) \pm t_v(1 - \frac{\alpha}{2})\sqrt{\frac{s_x^2}{m} + \frac{s_Y^2}{n}}$
   \item Compare to equal variance:
         \begin{itemize}
          \item For unequal sample sizes with unequal population variances, equal variance t-test does not have correct calibration
          \item When samples sizes are equal both test statistics are the same, but degrees of freedom differ
          \item When equal variance assumption is true, equal variance has slightly better power, and very slightly better calibration (more exact )
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100041 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$,\\
  $Y_1, \ldots, Y_n$ iid $F_y$,\\
  $X_i$ not independent $Y_i$,\\
  $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$. \\
  Estimate $d = \mu_x - \mu_y$, when $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ known
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$,\\
  $Y_1, \ldots, Y_n$ iid $F_y$,\\
  $X_i$ not independent $Y_i$,\\
  $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$. \\
  Estimate $d = \mu_x - \mu_y$, when $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ known
  \begin{itemize}
   \item Test name: Paired z-test
   \item Test Statistic: $z(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{\sigma_x^2}{n} + \frac{\sigma_Y^2}{n} - 2 \frac{\sigma_{XY}}{n}}} = \frac{\bar{D} - d_0}{\sqrt{\frac{\sigma_D^2}{n}}}$
   \item Test Reference Distribution: Under $H_0$, $z(d_0) $ aprox $\sim N(0,1)$
   \item Critical Value/ Rejection region: Same as normal
   \item Confidence interval : $$(\bar{X} - \bar{Y}) \pm z(1 - \frac{\alpha}{2})\sqrt{\frac{\sigma_x^2}{n} + \frac{\sigma_Y^2}{n} - 2 \frac{\sigma_{XY}}{n}} = \bar{D} \pm z(1 - \alpha/2) \sqrt{\frac{\sigma_D^2}{n}}$$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100042 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$, $X_i$ not independent $Y_i$, $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$ Estimate $d = \mu_x - \mu_y$, $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ unknown
  \begin{itemize}
   \item Test name:
   \item Estimate of $\sigma_{XY}$
   \item Estimate of $Var(\bar{X} - \bar{Y})$
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$, $X_i$ not independent $Y_i$, $(X_1, Y_1), \ldots , (X_n,Y_n)$ iid $F_{XY}$ $Cov(X_i,Y_i) = \sigma_{XY}$, $Cov(X_i,Y_j) = 0$ Estimate $d = \mu_x - \mu_y$, $\sigma_x^2, \sigma_y^2, \sigma_{XY}$ unknown
  \begin{itemize}
   \item Test name: Paired Data t-test
   \item Estimate of $\sigma_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})$
   \item Estimate of $Var(\bar{X} - \bar{Y}) = \frac{s_d^2}{n} = \frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}$
   \item Test Statistic: $t(d_0) = \frac{(\bar{X} - \bar{Y}) - d_0}{\sqrt{\frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}}} = \frac{\bar{D} - d_0}{\sqrt{\frac{s_D^2}{n}}}$
   \item Test Reference Distribution: If differences are Normal (note X,Y Normal does not imply Differences are normal unless X,Y are jointly multivariate-normal) Under $H_0$, $t(d_0) \sim t_{n-1}$ (exact distribution)
   \item Critical Value/ Rejection region Same as t
   \item Confidence interval
         $$ (\bar{X} - \bar{Y}) \pm t_{n-1,(1 - \frac{\alpha}{2})}\sqrt{\frac{s_x^2}{n} + \frac{s_Y^2}{n} - 2 \frac{s_{XY}}{n}} = \bar{D} \pm  t_{n-1,(1 - \frac{\alpha}{2})} \sqrt{\frac{s_d^2}{n}}$$
   \item Equivalent to a one sample - t-test on the differences
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100043 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$
  \begin{itemize}
   \item Test name: Binomial proportions two-sample z-test
   \item Test Statistic: $$ z = \frac{\hat{p}_x - \hat{p}_y}{\sqrt{\hat{p}_c(1 - \hat{p}_c(\frac{1}{m} + \frac{1}{n}))}} $$ Where $\hat{p_c} = \frac{m\hat{p}_x + n\hat{p}_y}{m+n} = \frac{b + d}{N}$
   \item Test Reference Distribution: Under $H_0: z $ approx $\sim N(0,1)$
   \item Critical Value/ Rejection region: Same as regular 2-sample
   \item Confidence interval: $$  \hat{p}_x - \hat{p}_y \pm z_{1 - \alpha/2} \sqrt{\big(\frac{\hat{p}_x(1 - \hat{p}_x)}{m} + \frac{\hat{p}_y(1 - \hat{p}_y)}{n}\big)}$$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100044 \end{field}
 \begin{field}
  Multinomial sampling
 \end{field}
 \begin{field}
  Collection of random samples, recording what group they are in: Can estimate $P(X = x | G = g)$, where $G$ is the group
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100045 \end{field}
 \begin{field}
  Two-Sample Binomial sampling
 \end{field}
 \begin{field}
  Sample $m$ units from group 1 and $n $ units from group 2
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100046 \end{field}
 \begin{field}
  Can we estimate $P(X = x | G = g)$ with binomial sampling
 \end{field}
 \begin{field}
  Cannot estimate
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100047 \end{field}
 \begin{field}
  $P(X = x | G = g)$ with multinomial sampling
 \end{field}
 \begin{field}
  Can estimate
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100048 \end{field}
 \begin{field}
  $E(g(T)) = $
 \end{field}
 \begin{field}
  $E(g(T)) \neq g(E(T))$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100049 \end{field}
 \begin{field}
  Reason for performing transformations on data
 \end{field}
 \begin{field}
  Some tests are FSE only when population distribution is Normal (otherwise the methods are asymptotically exact), requiring a large $n$. Transformations that improve approximation of normality make Normal-based methods perform more exactly
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100050 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$ (Association/independent/relationship)
  \begin{itemize}
   \item Test name:
   \item Test Statistic
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x - p_y = 0$ (Association/independent/relationship)
  \begin{itemize}
   \item Test name: Pearson's Chi-squared Test
   \item Test Statistic: $X = \sum_{i,j \in \{1,2\}} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ Where $O_{ij} = n_{ij}$ and $E_{ij} = \frac{R_iC_j}{N}$
   \item Test Reference Distribution: Under $H_0$ $X \sim \chi^2_1$
   \item Critical Value/ Rejection region: Reject for $X > \chi_1^2(1 - \alpha)$
   \item Note: Equal to to sided z-test for binomial proportions: $X = z^2$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100051 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ (No association between response variable $X$ and grouping variable $G$) (Fisher)
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item pvalue
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ (No association between response variable $X$ and grouping variable $G$)
  \begin{itemize}
   \item Test name: Fisher's Exact Test (of homogeneity of proportions)
   \item Test Statistic: Probability of observed table conditioning on margins: Compute all tables with the same margin totals: $\frac{\binom{C_1}{O_{11}}\binom{C_2}{O_{12}}}{\binom{N}{R_1}}$
   \item pvalue: Sum of probability of all tables more extreme than observed table
         More Extreme:
         \begin{itemize}
          \item $p_x > p_y$ More extreme = larger $O_{12}$
          \item $p_x < p_y$ More extreme = smaller $O_{12}$
          \item $p_x \neq p_y$ More extreme = less likely table
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100052 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ Binomial sampling scheme
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
   \item Confidence interval
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, Test $H_0: p_x = p_y $ Binomial sampling scheme
  \begin{itemize}
   \item Test name: Log Odds - test $H_0: \omega = 1$
   \item Test Statistic: $\hat{\omega} = \frac{ad}{bc}$, $z = \frac{\log(\hat{omega})}{\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}$
   \item Test Reference Distribution $ \log(\hat{\omega}) $ approx $\sim N(\log(\omega), \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d})$, $z $ approx $\sim N(0,1)$
   \item Critical Value/ Rejection region
   \item Confidence interval $(\hat{omega}e^{-z(1 - \frac{\alpha}{2})\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}, \hat{omega}e^{z(1 - \frac{\alpha}{2})\sqrt{frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}})$
   \item: $\omega > 1, p_1 > p_2$, $\omega = 1, p_1 = p_2$, small $p_1, p_2$, $\omega = p_1/p_2$ = relative risk
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100053 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, $X,Y$ not independent (paired) test proportions equal in groups (equally likely/probability)
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
   \item Critical Value/ Rejection region
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid Bernoulli($p_x$), $Y_1, \ldots, Y_n$ iid Bernoulli$(p_y)$, $X,Y$ not independent (paired), test proportions equal in groups (equally likely/probability)
  \begin{itemize}

   \item Note, requires a table that keeps track of the pairs
         \begin{tabular}{|c|c c|c|}
                        & Measurement 1 &       &       \\
          Measurement 2 & No            & Yes   & Total \\
          \hline                                        \\
          No            & a             & b     & $R_1$ \\
          Yes           & c             & d     & $R_2$ \\
          \hline                                        \\
          Total         & $C_1$         & $C_2$ & n     \\
          \hline
         \end{tabular}
   \item Test name: McNemar's Test
   \item Test Statistic: $z = \frac{b-c}{\sqrt{b+c}}$
   \item Test Reference Distribution: $z \sim N(0,1)$, $z^2 \sim \chi_1^2$
   \item Critical Value/ Rejection region: Two sided rejecct $|z| > z(1 - \alpha/2)$
   \item Note equivalent to performing a paired t-test on the differences:
         $$ t = \frac{b-c}{\sqrt{\frac{n}{n-1}(b + c - \frac{(b-c)^2}{40})}} $$ compare to $t_{n-1}$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100054 \end{field}
 \begin{field}
  Data setting: $n$ observations, record Group 1 and Group 2, where each group takes on $> 2$ values, Test if there is an association between the groups
  \begin{itemize}
   \item Test name:
   \item Test Statistic:
   \item Test Reference Distribution
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: $n$ observations, record Group 1 ($r$ values) and Group 2$(c)$ values, Test if there is an association between the groups
  \begin{itemize}
   \item Test name: Pearsons $\chi^2$
   \item Test Statistic: $X = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$, where $E_{ij} = \frac{n_in_j}{N}$
   \item Test Reference Distribution: Under $H_0$, $X$ approx $\sim \chi^2_{(r-1)(c-1)}$
   \item Note not FSE, but performance is good if $E_{ij} > 5$
  \end{itemize}
 \end{field}
\end{note}




\begin{note} \begin{field} \tiny 100055 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name:
   \item Assumptions
   \item Process
   \item pvalue
   \item Test Reference Distribution
   \item Test Statistic:
   \item Ties
   \item Continuity correction
   \item Consistency
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name: Wilcoxon Rank-Sum (Mann-Whitney U-test)
   \item Note this is only a test of medians only if just additive effect - $F_x$ is just a shift from $F_y$ (shape and scale must be same ) (but then just the same as a test of mean, 10th percentile, min, $F_x = F_y$ etc )
   \item If No additive assumption - test of $H_0: P(X > Y) = .5$
   \item Process:
         \begin{itemize}
          \item Combine samples
          \item Rank the observations in combined sample from smallest to largest (1 to $n + m$)
          \item Add ranks of the smaller group (assume wlog that $X$ is the smaller group)
         \end{itemize}
   \item pvalue: Calculate using permutations: Count number of permutations that lead to a R value more extreme than observed out of total permutations ($\binom{n+m}{m}$)
   \item Test Statistic: $R$ sum of the ranks, or $z = \frac{R - \frac{m(m+n+1)}{2}}{\sqrt{\frac{mn(m+n+1)}{12}}}$
   \item Test Reference Distribution: If there was no difference between two populations, then each rank has equal chance of being assigned to group 1 (belongs to $X$: $p = \frac{m}{n+m}$)
         Normal approximation: $R \dot\sim N( \frac{m(m+n+1)}{2}, \frac{mn(m+n+1)}{12})$, $z \dot\sim N(0,1)$
   \item Notes: If ties, assign ranks, and then average ranks of tied values
   \item Continuity correction to normal distribution: add .5 to R if lower probability, subtract .5 from R if upper probability (ie 1 - pnorm())
   \item Not consistent test unless under additive assumption. IS consistent test of $H_0: P(X > Y) = .5$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100056 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name:
   \item Process
   \item Test statistic:
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $m_x = m_y$ (medians )
  \begin{itemize}
   \item Test name: Mood's Test for Equality of Population Medians
   \item Process:
         \begin{itemize}
          \item Find combined sample median $\hat{m}$
          \item Calculate $\hat{p}_x = $ proportion of $X$s greater than $\hat{m}$, $\hat{p}_y$, proportion of $Ys$ greater than $\hat{m}$
          \item Conduct two sample binomial z-test( Pearsons chi-squared test) or Fisher's exact test
          \item  Test statistic: $$ z = \frac{\hat{p}_x - \hat{p}_y}{\sqrt{\hat{p}_c(1 - \hat{p}_c(\frac{1}{m} + \frac{1}{n}))}} $$ Where $\hat{p_c} = \frac{m\hat{p}_x + n\hat{p}_y}{m+n} = \frac{b + d}{N}$
         \end{itemize}
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100057 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test some statistic $W$
  \begin{itemize}
   \item Test name:
   \item Process
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting $X_1, \ldots , X_n$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test some statistic $W$
  \begin{itemize}
   \item Test name: Permutation test
   \item Process: Permute group labels across observations and recalculate statistic for each permutation to create permutation distribution - calculate p-values using the permutation distribution
   \item Performance: Many settings (like medians equal), will not reject correctly (even in large samples) if the medians are equal, but the distributions differ
   \item Permutation hypothesis is that the observations from the two pouplations are exchangable (ie same population distributions, not just equal medians )
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100058 \end{field}
 \begin{field}
  Data setting: Estimate value of nuisance parameter
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: Bootstrap
   \item Process: Since the empirical distribution function converges to the true distribution function, we can use samples from the empirical distribution to approximate how samples from the true distribution would behave.
   \item Confidence interval: $100 (\alpha/2)$ largest resampled statistic $100(1 - (\alpha/2))$ largest resampled statistic
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100059 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $N$, $Y_1, \ldots, Y_n$ iid $N$. $H_0: \sigma_x^2 = \sigma_y^2$ or $H_0 \sigma_x^2/\sigma_y^2 = r$
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$ or $H_0 \sigma_x^2/\sigma_y^2 = r$
  \begin{itemize}
   \item Test name: F
   \item Recall $s_x^2 = \frac{1}{n-1}\sum_{i=1}^m(X_i - \bar{X})^2$
   \item Note that $\frac{(m-1)s_x^2}{\sigma_x^2} \sim \chi^2_{m-1}, \frac{(n-1)s_y^2}{\sigma_y^2} \sim \chi^2_{n-1},$
   \item Test Statistic: $F(r) = \frac{s_x^2/\sigma_x^2}{s_y^2/\sigma_y^2} = \frac{s_x^2}{s_y^2} \frac{1}{r}$
   \item Test Reference Distribution: Under $H_0: F(r) \sim F_{m-1,n-1}$
   \item Critical Value/ Rejection region
         \begin{itemize}
          \item $\sigma_x^2/\sigma_y^2 > r$ Reject for $F(r) > F_{m-1,n-1}(1-\alpha)$
          \item $\sigma_x^2/\sigma_y^2 > r$ Reject for $F(r) > F_{m-1,n-1}(\alpha)$
          \item $\sigma_x^2/\sigma_y^2 \neq r$ Reject for $F(r) > F_{m-1,n-1}(1-\alpha/2)$ or $F(r) < F_{m-1,n-1}(\alpha/2)$
         \end{itemize}
   \item Performance: Not Well if underlying population is not normal: Not FSE or AE (but is consistent ) - don't use if population is not normal
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100060 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$
  \begin{itemize}
   \item Test name:
   \item Process:
   \item Interpretation
   \item Assumptions
  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. $H_0: \sigma_x^2 = \sigma_y^2$
  \begin{itemize}
   \item Test name: Levene's Test
   \item Process:
         \begin{itemize}
          \item Construct new variables:
                \begin{itemize}
                 \item $U_i = |X_i - med(X)|$ or $(X_i - med(X))^2$ or $|X_i - \bar{X}|$ or $(X_i - \bar{X})^2$
                 \item $V_i = |Y_i - med(Y)|$ or $(Y_i - med(Y))^2$ or $|Y_i - \bar{Y}|$ or $(Y_i - \bar{Y})^2$
                \end{itemize}
          \item Perform two-sample $t$ test on $U_i$ and $V_i$ (use Welch)
         \end{itemize}
   \item Interpretation: If last option used, can be a test in difference in population variances
   \item Assumptions:
         \begin{itemize}
          \item Independence
          \item Large sample sizes, so t-test assumptions are met
         \end{itemize}
   \item Note: dont use as a test to determine which t-test version to use
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100061 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $H_0: F_x = F_y$
  \begin{itemize}
   \item Test name
   \item Test statistic

  \end{itemize}
 \end{field}
 \begin{field}
  Data setting: Data setting $X_1, \ldots , X_m$ iid $F_x$, $Y_1, \ldots, Y_n$ iid $F_y$. Test $H_0: F_x = F_y$
  \begin{itemize}
   \item Test name: Two-sample Kolmogorov-Smirnov Test
   \item Test statistic: $D = sup_x|\hat{F}_x(x) - \hat{F}_y(y)|$ ie the largest distance between the empirical CDF for $X$ and $Y$
   \item Reject for large values of $\sqrt{\frac{mn}{m+n}} D$
   \item Only for continuous distributions, for discrete distributions, use Pearsons $\chi^2$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100062 \end{field}
 \begin{field}
  Multiple 2x2 tables under $k$ different conditions $p_{xj} = P(X = 1 \text{ in Table }j), p_{yj} = P(Y = 1 \text{ in Table }j)$ $H_0: p_{xj} = p_{yj}$ for all $j$
 \end{field}

 \begin{field}
  \begin{itemize}
   \item Test name: Mantel-Haenszel Test
   \item Test statistic: $\omega_j = \frac{p_{xj}(1 - p_{xj})}{p_{yj}(1 - p_{yj})}$, $H_0: \omega_j = 1$ for all $j$
         $$E(n_{X1j}) = \mu_{X1j} = \frac{n_{X\cdot j}n_{\cdot 1 j}}{n_{\cdot j}}, V(n_{X1j}) = \sigma^2_{X1j} = \frac{n_{X\cdot j}n_{Y\cdot j}n_{\cdot 1j} n_{\cdot 0j}}{n^2_{\cdot \cdot j}(n_{\cdot \cdot j} -1)} $$
         $$ C = \frac{[\sum_{j}(n_{X1j} - \mu_{X1j})]^2}{\sum_j \sigma^2_{X1j}}$$
   \item Under $H_0$ $C \dot\sim \chi^2(1)$
   \item Assumes the odds-ratios are the same in all $k$ tables
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100063 \end{field}
 \begin{field}
  Test for data setting:
\begin{itemize}
  \item Sample 1: $X_{1,1}, \ldots , X_{1n_1}$ from population 1 with mean $\mu_1$,
  \item Sample 2: $X_{2,1}, \ldots , X_{2n_2}$ from population 2 with mean $\mu_2$,\\ $\ldots $
  \item \ldots
  \item Sample M: $X_{M,1}, \ldots , X_{Mn_M}$ from population M with mean $\mu_M$
  \begin{itemize}
    \item Independence within and between groups
    \item Populations (approximately ) normal
    \item Equal variances
  \end{itemize}
\end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Test name: ANOVA
   \item Estimate of common variance $s_p = \frac{(n_1-1)s_1^2 + \cdots + (n_M -1)s_M^2}{(n_1-1) + \cdots + (n_M-1)}$
   \item Could use two-sample-t test on two population means
   \item Could test are population means 1 through M equal to each other?
   \item Compare the variability between groups to the variability withing groups
   \item Sum of squares within groups:
         $$ SSW = (n-M)s_p^2  = \sum_{i=1}^{n_1}(X_{1i} - \bar{X}_1)^2 + \cdots +  \sum_{i=1}^{n_M}(X_{Mi} - \bar{X}_M)^2$$
         degrees of freedom: $n-M$
   \item Sum of squares total
         $$ SST  = \sum_{i=1}^{n_1}(X_{1,i} - \bar{X})^2 + \cdots + \sum_{i=1}^{n_M} (X_{M,i} - \bar{X})^2$$
         degrees of freedom: $n-1$
   \item Sum of squares between groups: $ SSB = SST - SSW = \sum_{j=1}^Mn_j(\bar{X}_j - \bar{X})^2$ df: $(n-1) - (n-M) = M-1$
   \item Test statistic: $$ F = \frac{MSB}{MSW} = \frac{SSB/(M-1)}{SSW/(n-M)} $$
   \item Reference distribution: Under $H_0, F \sim F_{M-1, n-M}$
  \end{itemize}
 \end{field}
\end{note}

%%end_tag

%%start_tag Methods 2 100064-100129
\tags{Methods2}

\begin{note} \begin{field} \tiny 100064 \end{field}
 \begin{field}
  Vectors $\mathbf{x}$ and $\mathbf{y}$ orthogonal
 \end{field}
 \begin{field}
  Vectors $\mathbf{x}$ and $\mathbf{y}$ orthogonal (perpendicular) if $(x,y) = \mathbf{x}^t \mathbf{y} = 0$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100065 \end{field}
 \begin{field}
  A matrix $\mathbf{A}$ is orthogonal if:
 \end{field}
 \begin{field}
  A matrix $\mathbf{A}$ is orthogonal if $\mathbf{A}^t \mathbf{A} = \mathbf{A} \mathbf{A}^t = \mathbf{I}_n$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100066 \end{field}
 \begin{field}
  A set of $n$ vectors are linearly dependent
 \end{field}
 \begin{field}
  A set of $n$ vectors are linearly dependent if there exist constants $c_1, \ldots c_n$ not all 0 such that $\sum_{j=1}^n c_j \mathbf{x}_j = 0$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100067 \end{field}
 \begin{field}
  Inverse of a square matrix: $\mathbf{A}_{n\times n}$
 \end{field}
 \begin{field}
  The matrix that will satisfy $AA^{-1} = I$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100068 \end{field}
 \begin{field}
  Inverse of $\mathbf{A}$, $\mathbf{A}^{-1}$ where $\mathbf{A}$ is $2 \times 2$
 \end{field}
 \begin{field}
  $\mathbf{A}^{-1} = \begin{pmatrix}
    a & b \\ c & d
   \end{pmatrix}^{-1} = \frac{1}{ad - bc}\begin{pmatrix}
    d & -b \\ -c & a
   \end{pmatrix}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100069 \end{field}
 \begin{field}
  A square matrix is invertible if:
 \end{field}
 \begin{field}
  A square matrix is invertible if the columns (rows) are linearly independent. (If the columns are not independent, the matrix is called singular)
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100071 \end{field}
 \begin{field}
  Square of matrix $\mathbf{A}$
 \end{field}
 \begin{field}
  $\mathbf{AA}^t$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100072 \end{field}
 \begin{field}
  Norm of a vector $|\mathbf{x}|$
 \end{field}
 \begin{field}
  $|\mathbf{x}| = \sqrt{\sum_{j=1}^p x_j^2}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100073 \end{field}
 \begin{field}
  Determinant of a $2\times 2 $ matrix
 \end{field}
 \begin{field}
  $\bigg| \begin{pmatrix}
    a & b \\ c & d
   \end{pmatrix}\bigg| = ad - bc $
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100074 \end{field}
 \begin{field}
  Trace of a square matrix
 \end{field}
 \begin{field}
  Sum of the diagonal elements
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100075 \end{field}
 \begin{field}
  Rank of a matrix
 \end{field}
 \begin{field}
  Number of linearly independent columns
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100076 \end{field}
 \begin{field}
  Eigenvalue and eigenvector
 \end{field}
 \begin{field}
  $\lambda $ is an eigen value and  $\mathbf{u}_{n \times 2}$ is the eigen vector of $\mathbf{A}_{n \times n}$ if $\mathbf{Au} = \lambda \mathbf{u}$
  \begin{itemize}
   \item A real symmetric matrix has $n$ eigen values  and $n$ eigen vectors, and each are orthogonal to each other
   \item Roots of $det(\mathbf{A} - \lambda  \mathbf{I})$ determine the eigenvalues of $A$
  \end{itemize}
 \end{field}
\end{note}

% Come back to lecture 1 for more linear algebra notes

\begin{note} \begin{field} \tiny 100077 \end{field}
 \begin{field}
  Matrix properties
  \begin{itemize}
   \item $(AB)^{t} = $
   \item $(A+B)^t = $
   \item $(AB)^{-1} = $
   \item $(\mathbf{A}^{-1})^t = $
  \end{itemize}
 \end{field}
 \begin{field}
  Matrix properties
  \begin{itemize}
   \item $(AB)^{t} = B^tA^t$
   \item $(A+B)^t = A^t + B^t$
   \item For invertible matrices $(AB)^{-1} = B^{-1}A^{-1}$
   \item For invertible matrices $(\mathbf{A}^{-1})^t = (\mathbf{A}^t)^{-1}$
  \end{itemize}
 \end{field}
\end{note}




\begin{note} \begin{field} \tiny 100079 \end{field}
 \begin{field}
  $$ E(Y_i | X_{i1}, \ldots , X_{ip}) = $$
  Where $Y_i$ is the $i$th response and $X_{ij}$ is the $i$th value of the $j$th predictor
 \end{field}
 \begin{field}
  Since the error terms $\epsilon_i$ are independent and normally distributed with mean 0,
  $$ E(Y_i | X_{i1}, \ldots , X_{ip}) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_pX_{ip}$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100080 \end{field}
 \begin{field}
  Matrix form of linear Model and data
 \end{field}
 \begin{field}
  $$ \begin{pmatrix}
    Y_1 \\ Y_2 \\ \vdots \\ Y_n
   \end{pmatrix}  = \begin{pmatrix}
    1      & X_{11} & X_{12} & \cdots & X_{1p} \\
    1      & X_{21} & X_{22} & \cdots & X_{2p} \\
    \vdots & \vdots & \vdots &        & \vdots \\
    1      & X_{n1} & X_{n2} & \cdots & X_{np}
   \end{pmatrix} \begin{pmatrix}
    \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n
   \end{pmatrix}$$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100081 \end{field}
 \begin{field}
  Assumptions of a linear model
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Linearity: $E(\epsilon_i) = 0$ or $E(\epsilon) = \mathbf{0}$ or $E(\mathbf{Y}) = \mathbf{X}\beta$
   \item Constant variance $V(Y_i) = \sigma^2 = Var(\epsilon_i)$ or $V(\epsilon) = \sigma^2 \mathbf{I}_n$
   \item Normality $Y_i$ follows normal distribution, equivalently, $\epsilon_i$ follows normal distribution
   \item Independence $Y_i$ are indepednent equivalently under normality $Cov(\epsilon_i, \epsilon_j) = 0$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100082 \end{field}
 \begin{field}
  Interpretation of intercept of linear model
 \end{field}
 \begin{field}
  Mean response when all explanatory variables are 0
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100083 \end{field}
 \begin{field}
  Interpretation of slopes of linear model
 \end{field}
 \begin{field}
  Change in mean response for 1 unit change in the value of the explanatory, keeping all other variables constant. When $p = 2$
  $$ E(Y|X_1 + 1, X_2) - E(Y|X_1,X_2) = \beta_1$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100084 \end{field}
 \begin{field}
  Reason for $g-1$ indicator variables for a variable with $g$ values
 \end{field}
 \begin{field}
  The model matrix $X_{n \times(p+1)}$ needs to be full column rank - $\mathbf{X}^t \mathbf{X}$ needs to be non-singular
  If there is no intercept, we can include all groups, but interpretation will be different
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100085 \end{field}
 \begin{field}
  Interpretation of slope coefficient for indicator variable $\beta$
 \end{field}
 \begin{field}
  Difference in expected value of $Y$ between group value $a$ and $b$ where $a$ is the associated value for $\beta_j$ and $b$ is the base category
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100086 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\textbf{AU} + \textbf{b}) = $
   \item $V(\mathbf{AU + \mathbf{b}}) = $
  \end{itemize}
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $E(\textbf{AU} + \textbf{b}) = \mathbf{A} E(\mathbf{U}) + \mathbf{b}$
   \item $V(\mathbf{AU + \mathbf{B}}) = \mathbf{A}V(\mathbf{U}) \mathbf{A}^t$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100087 \end{field}
 \begin{field}
  Least squares estimate of $\beta$ (process to find )
 \end{field}
 \begin{field}
  Minimize the squared error loss ($L(\beta)$) with respect to $\beta$

  $$ L(\beta ) = \sum_{i=1}^n Y_i - (\beta_0 + \beta_1 X_{i1} + \cdots + \beta_p X_{ip}))^2 = (\mathbf{Y} - \mathbf{X}\beta)^t(\mathbf{Y} - \mathbf{X}\beta)$$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100088 \end{field}
 \begin{field}
  $$  \frac{\partial}{\partial \beta} L(\beta) = $$
 \end{field}
 \begin{field}
  \begin{align*}
   \frac{\partial}{\partial \beta} L(\beta) & = \frac{\partial}{\partial \beta} (\mathbf{Y} - \mathbf{X}\beta)^t(\mathbf{Y} - \mathbf{X}\beta) \\
                                            & = \frac{\partial}{\partial \beta}  \mathbf{Y}^t \mathbf{Y} - \beta^t \mathbf{X}^t
   \textbf{Y} - \mathbf{Y}^t \mathbf{X}\beta - \beta^t \mathbf{X}^t \mathbf{X} \beta                                                           \\
                                            & = 0 - \mathbf{X}^t \mathbf{Y} - \mathbf{X}^t \mathbf{Y} + 2 \mathbf{X}^t \mathbf{X} \beta        \\
   \mathbf{X^t}\mathbf{X}\beta &= \mathbf{X}^t \mathbf{Y}\\
   \beta &= (X^tX)^{-1}X^tY
  \end{align*}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100089 \end{field}
 \begin{field}
  Least squares estimate of $\hat{\beta}$
 \end{field}
 \begin{field}
  $$ \hat{\beta} = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y} $$ (if $\mathbf{X}^t \mathbf{X}$ is invertible )
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100090 \end{field}
 \begin{field}
  Residual
 \end{field}
 \begin{field}
  $e_i = Y_i - \hat{Y}_i$, $\mathbf{e}_{n\times 1} = \mathbf{Y} - \hat{\mathbf{Y}}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100091 \end{field}
 \begin{field}
  Vector of fitted values (linear regression )
 \end{field}
 \begin{field}
  $\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta} = \mathbf{X} (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100092 \end{field}
 \begin{field}
  Projection matrix
 \end{field}
 \begin{field}
  Hat matrix
  $$\mathbf{H_{n \times n}} = \mathbf{X} (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t$$
  $H_{ij}$ is the rate at which the $i$th fitted value changes as we vary the $j$th observation (influence )
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100093 \end{field}
 \begin{field}
  Properties of projection matrix
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $H$ and $\mathbf{I} - \mathbf{H}$ are symmetric matrices
   \item $\mathbf{HX} = X$
         item $(\mathbf{I} - \mathbf{X})\mathbf{X} = \mathbf{0}$
   \item $\mathbf{H}^2 = \mathbf{H}$
   \item $(\mathbf{I} - \mathbf{H})\mathbf{H} = 0$
   \item $\mathbf{X}^t \mathbf{e} = 0$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100094 \end{field}
 \begin{field}
  Unbiased estimate of $\sigma^2$
 \end{field}
 \begin{field}
  $\hat{\sigma}^2 = \frac{1}{n- (p+1)} \sum_{i=1}^n e_i^2 = \frac{1}{n- (p+1)} \mathbf{e}^t \mathbf{e}$
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100095 \end{field}
 \begin{field}
  $\mathbf{e}^t \mathbf{e} = $
 \end{field}
 \begin{field}
  $\mathbf{e}^t \mathbf{e} = \mathbf{Y}^t \mathbf{Y} - \mathbf{Y}^t \mathbf{HY}$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100096 \end{field}
 \begin{field}
  $E(\hat{\beta}) = $
 \end{field}
 \begin{field}
  $E(\hat{\beta}) = E((\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{Y}) = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t E(\mathbf{Y}) = (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{X}^t \mathbf{X}\beta  = \beta$

  So $\hat{\beta}$ is an unbiased estimate
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100097 \end{field}
 \begin{field}
  Gauss - Markov Theorem
 \end{field}
 \begin{field}
  If $E(\mathbf{Y}) = \mathbf{X}\beta$ and $V(\mathbf{Y}) = \sigma^2 \mathbf{I}$, then the least squares estimate $\hat{\beta}$ has the least variance among all linear unbiased estimators of $\beta$. (BLUE)

  Note that non-normal (or iid) residuals is not nescessary, just must be uncorrelated.
 \end{field}
\end{note}

% add proof of gauss markov


\begin{note} \begin{field} \tiny 100098 \end{field}
 \begin{field}
  $V(\hat{\beta}) = $
 \end{field}
 \begin{field}
  $V(\hat{\beta}) = \sigma^2 (\mathbf{X}^t \mathbf{X})^{-1}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100099 \end{field}
 \begin{field}
  $E(\hat{\sigma}^2) = $
 \end{field}
 \begin{field}
  $E(\hat{\sigma}^2) = \sigma^2$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100100 \end{field}
 \begin{field}
  If $\mathbf{X}_{p \times 1}$ has a multivariate normal distribution $N(\mu_{p\times 1}, \Sigma_{p \times p})$, then $\mathbf{AX} + b \sim $
 \end{field}
 \begin{field}
  If $\mathbf{X}_{p \times 1}$ has a multivariate normal distribution $N(\mu_{p\times 1}, \Sigma_{p \times p})$, then $\mathbf{AX} + \mathbf{b} \sim N(\mathbf{A}\mu + \mathbf{b}, \mathbf{A}\Sigma \mathbf{A}^t)$



 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100101 \end{field}
 \begin{field}
  Multivariate normal properties for $\mathbf{X}_{p \times 1} \sim N(\mu_{p\times 1}, \Sigma_{p \times p})$
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $Cov(X_j,X_k) = 0$ if and only if $X_j,X_k$ are independent (two way due to multivariate normal )
   \item All subsets of elements of $\mathbf{X}$ have a multivarite normal distribution
   \item All linear combinations of the components of $X$ are normally distributed
   \item $\mathbf{a}^t \mathbf{X} \sim N(\mathbf{a}^t, \mathbf{a}^t \Sigma \mathbf{a})$ for a vector $a$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100102 \end{field}
 \begin{field}
  Linear Hypothesis testing single parameter $H_0: \mathbf{c}^t \beta = d$
  \begin{itemize}
   \item $E(\mathbf{c}^t \beta)$, $V(\mathbf{c}^t \beta)= $
   \item Test statistic and distribution
   \item Item of setting up hypothesis test
   \item Rejection Region
  \end{itemize}
 \end{field}
 \begin{field}
  For a vector $\mathbf{c}_{(p+1)\times 1}$, we have that
  \begin{itemize}
   \item $E(\textbf{c}^t\hat{\beta}) = \mathbf{c}^t \beta$, $V(\mathbf{c}^t \hat{\beta}) = \sigma^2 \mathbf{c}^t (\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}$
   \item Thus $$ \frac{\mathbf{c}^t \hat{\beta} - \mathbf{c}^t \beta }{\sigma \sqrt{\mathbf{c}^t(\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}}} \sim N(0,1)$$ and under $H_0$
         $$ T = \frac{\mathbf{c}^t \hat{\beta} - d}{ \sqrt{\hat{\sigma}^2\mathbf{c}^t(\mathbf{X}^t \mathbf{X})^{-1} \mathbf{c}}} \sim t_{n - (p+1)}$$
   \item Example: testing $H_0: \beta_1 = \beta_2, \mathbf{c} = (0,1,-1)^t, d = 0$
   \item Reject $H_a: c^t \beta \neq d:  |T| > t_{n - (p+1)}(1-\alpha/2)$\\ $c^t \beta > d, T > t_{n-(p+1)}(\alpha)$\\ $c^t\beta < d: T < t(1 - \alpha)$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100103 \end{field}
 \begin{field}
  Confidence interval for a single parameter (linear regression slope estimate)
 \end{field}
 \begin{field}
  $$ \hat{\beta}_j \pm t_{n-(p-1)}(1 - \alpha/2)\sqrt{\hat{\sigma}^2((\mathbf{X}^t \mathbf{X})^{-1})_{j+1,j+1}} $$
  $$ \mathbf{c}^t \beta \pm t_{n-(p-1)}(1 - \alpha/2)\sqrt{\hat{\sigma}^2 \mathbf{c}^t((\mathbf{X}^t \mathbf{X})^{-1})\mathbf{c}}  $$
  eg if we were testing $\beta_1 - \beta_2, c = (0,1,-1)$
 \end{field}
\end{note}





\begin{note} \begin{field} \tiny 100104 \end{field}
 \begin{field}
  F statistic in matrix form
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $\mathbf{K}$  is $p\times k$, $\mathbf{m}$ is $k\times 1$
   \item Testing $H_0: \mathbf{K}^t\beta = \mathbf{m}$
   \item $F = \frac{\big((\mathbf{K}\hat{\beta} - \mathbf{m})^t (\mathbf{K}(\mathbf{X}^t \mathbf{X})^{-1}\mathbf{K}^{-1})(\mathbf{K}\hat{\beta} - \mathbf{m})\big)}{k\hat{\sigma}^2} \sim F_{k,n-p}$
   \item Eg$K = \begin{pmatrix}
           0 \\ 1 \\ \vdots \\ 0
          \end{pmatrix}$, $m = 0$
   \item Tests $\beta_1 = 0$
   \item Note the $\mathbf{K}^t$ matrix is the coefficients of the system of linear equations for the the null hypothesis, and $m$ is what they are equal to
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100105 \end{field}
 \begin{field}
  Overall regression F-test
 \end{field}
 \begin{field}
  Tests if any predictors are related to the response
  \begin{itemize}
   \item Full model: $\mathbf{Y} = \mathbf{X}\beta + \epsilon$
   \item Reduced model a nested model with $q$ estimated parameters
   \item eg: Reduced model: $\mathbf{Y} = \beta_0 + \epsilon$, $q = 1$
   \item $H_0: \beta_1 =  \ldots = \beta_p = 0$
   \item $F = \frac{(RSS_\omega - RSS_\Omega)/(p-q)}{RSS_\Omega/(n-p)}$
   \item
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100106 \end{field}
 \begin{field}
  Analysis of Variance Table and calculated F stat
 \end{field}
 \begin{field}
  \begin{tabular}{|c|c|c|c|}
   \hline                                                                                                \\
   Type       & df      & Sum of Squares                & Mean SS                                        \\
   \hline                                                                                                \\
   Regression & $p$     & SS(Reg)                       & SS(Reg)/p                                      \\
   Residual   & $n-p+1$ & SS(Res)                       & $\hat{\sigma}^2 =$ SS(Res)/$n-p-1$             \\
   Total      & $n-1$   & SS(Total) = SS(Reg) + SS(Res) & $\frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2$ \\
   \hline
  \end{tabular}
  and $F = \frac{Mean(SSREG)}{Mean(SSRES)}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100107 \end{field}
 \begin{field}
  Distribution of $\hat{\beta}$, where $\hat{\beta}$ are the estimated coefficients of linear regression.
 \end{field}
 \begin{field}
  $\hat{\beta} \sim N(\beta, \sigma^2(\mathbf{X}^t \mathbf{X})^{-1})$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100108 \end{field}
 \begin{field}
  RSS (in terms of $\Omega$ and $\omega$)
 \end{field}
 \begin{field}
  $$RSS_\Omega = \sum_{i=1}^n e_i^2$$
  $$RSS_\omega = \sum_{i=1}^n (Y_i - \bar{Y})^2$$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100109 \end{field}
 \begin{field}
  $R^2$
 \end{field}
 \begin{field}
  $R^2 = \frac{SS(Reg)}{SS(Tot)} = 1 - \frac{SS(Res)}{SS(Tot)}$

  Where SS(Reg) is the regression sum of square: $\sum_{i} (\hat{y}_i - \bar{y})^2$ (fitted minus mean)
  and SS(Tot) or TSS is the total sum of squares $\sum_{i} (y_i - \bar{y})^2$
  and SS(Res) (or error sum of squares) $SS_E$ or RSS is the residual sum of squares $\sum_i (y_i - \hat{y}_i)^2 = \sum_{i} e_i$

  And SS(Tot) = SS(Res) + SS(Reg)
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100110 \end{field}
 \begin{field}
  Properties of the estimate of $\sigma^2$
 \end{field}
 \begin{field}
  \begin{itemize}
   \item   $\hat{\sigma}^2 = \frac{|\mathbf{e}|^2}{n - (p+1)}$
   \item Under normality: $\frac{(n - (p+1))\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n - (p+1)}$
   \item $\hat{\sigma}^2$ is independent from $\hat{\beta}$
  \end{itemize}
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100111 \end{field}
 \begin{field}
  Prediction Interval
 \end{field}
 \begin{field}
  Predicting a future response
  $\mathbf{x}_0^t \hat{\beta} \pm t_{n-p}(\alpha/2)\hat{\sigma} \sqrt{1 + x_0^t(X^tX)^{-1}x_0}$

  A 95\% prediction interval for a response with (list values) is between and
 \end{field}
\end{note}



\begin{note} \begin{field} \tiny 100112 \end{field}
 \begin{field}
  Confidence interval
 \end{field}
 \begin{field}
  Confidence in mean response $\mathbf{x}_0^t \hat{\beta} \pm t_{n-p}(\alpha/2)\hat{\sigma} \sqrt{x_0^t(X^tX)^{-1}x_0}$
  With 95\% confidence, the expected mean response
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100113 \end{field}
 \begin{field}
  Residual Plot
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Plot residuals against fitted values (so there is only 1 plot vs against explanatory variables)
   \item Verifies linearity and constant variance
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100114 \end{field}
 \begin{field}
  Leverege
 \end{field}
 \begin{field}
  \begin{itemize}
   \item An obervarion has high leverage if the explanatory variable values of the observation are different from general pattern
   \item $h_i = H_{ii} = (X(X^tX)^{-1}X^t)_{ii}$
   \item High leverage $h_i > \frac{2(p+1)}{n}$
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100115 \end{field}
 \begin{field}
  Standardized Residual
 \end{field}
 \begin{field}
  $r_i = \frac{e_i}{\hat{\sigma}\sqrt{1 - h_i}}$
  Large if $|r_i| > 2$ - indicates outlier
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100116 \end{field}
 \begin{field}
  Influential
 \end{field}
 \begin{field}
    if fitted model depends highly on the value
  Measure using cook's distance
  $$ D_i = \frac{(\hat{Y} - \hat{Y}_{(i)})^t(\hat{Y} - \hat{Y}_{(i)})}{(p+1)\hat{\sigma}^2} = \frac{1}{p+1}r_i^2 \frac{h_i}{(1 - h_i)}$$
  Where $Y_{i}$ is the vector of fitted values when the model is fitted to the data without the $i$ ths observationl Moderate if $>1 $ Large if $>6$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100117 \end{field}
 \begin{field}
  Multicollinearity
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $X^tX$ is close to singular
   \item Some columns are highly correlated
   \item there is a relationship between predictors \item leads to large standard errors
   \item Not a violation of assumptions, but leads to issues in interpretations
   \item Calculate using Condition number if $>30$ than large , or Variance inflation factors $VIF_j = \frac{1}{1 - R^2_j}$ where $R^2_j$ is $R^2$ from regression of the $j$th explanatory variable on all the other explanatory variables
   \item Not a problem for prediction
   \item Fix using selection of explanatory variables, generalized inverse, ridge regression
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100118 \end{field}
 \begin{field}
  Ridge Regression
 \end{field}
 \begin{field}
  $\hat{\beta} = (X^tX + \lambda I)^{-1} X^t Y$, where $\lambda$ is chosen. Note these are biased estimators
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100119 \end{field}
 \begin{field}
  Fix non-constant spread/variance
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Transform response (box-cox)
   \item Use more complicated model (glm)
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100120 \end{field}
 \begin{field}
  Fix non-linearity
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Transform response
   \item Transform predictor
   \item allow for curvature: predictor squared, splines, gam
   \item use a non linear model
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100121 \end{field}
 \begin{field}
  Fix Non-normality
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Transform response
   \item more complicated models : glm
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100122 \end{field}
 \begin{field}
  Missing data completely at random (MCAR)
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Throwing out cases with missing data does not bias inferences
   \item Theres no relationship between whether a data point is missing and any values in the data set, missing or observed.
   \item The missing data are just a random subset of the data.
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100123 \end{field}
 \begin{field}
  Missing at random (MAR)
 \end{field}
 \begin{field}
  \begin{itemize}
   \item the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data.
   \item Probability of missingness depends only on available information, like the explanatory variables and the response variables present in the regression - impute missing data
   \item A better name would actually be Missing Conditionally at Random, because the missingness is conditional on another variable.
  \end{itemize}

 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100124 \end{field}
 \begin{field}
  Model Selection methods
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Sequential Methods: Backward/Forward (eliminate untill all values have p-value below critical value) Elimination
   \item Penalized Regression: Ridge and Lasso
  \end{itemize}
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100125 \end{field}
 \begin{field}
  AIC
 \end{field}
 \begin{field}
  Estimate the distance of a candidate model from the true model (small good)
  $$ n \log (RSS/n) + 2(p+1)$$
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100126 \end{field}
 \begin{field}
  BIC
 \end{field}
 \begin{field}
  Estimate the best parsimonious model, using a prior distribution on the parameters (small good)
  $$  n \log (RSS/n) + \log(n)(p+1)$$

  Where $n$ is the number of observations, $p$ is the number of predictors (not including intercept), and $RSS = \sum (Y_i - \hat{Y})^2 = \sum e_i^2 $
 \end{field}
\end{note}


\begin{note} \begin{field} \tiny 100127 \end{field}
 \begin{field}
  Adjusted $R^2$
 \end{field}
 \begin{field}
  Adjusts for multiple parameters
  $$ 1 - \frac{n-1}{n-p}(1 - R^2)$$ (large is good)
  (where p includes the intercept)

  $\frac{MS(Reg)}{MS(Total)} = 1 - \frac{SS(Reg)/(n - p - 1)}{SS(Tot)/(n-1)}$
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100128 \end{field}
 \begin{field}
  Mallow's Cp
 \end{field}
 \begin{field}
  $$ RSS/\hat{\sigma^2} + 2p - n $$ (small good)
 \end{field}
\end{note}

\begin{note} \begin{field} \tiny 100129 \end{field}
 \begin{field}
  Box-Cox Transformation
 \end{field}
 \begin{field}
  Transform so model is $g(Y) = X\beta + \epsilon$
  where $g(y) = \frac{y^\lambda -1}{\lambda} if \lambda \neq 0, 0 $ otherwise
 \end{field}
\end{note}


%%end_tag ends on 100129

%%start_tag Methods 3 Starts on 100130
\tags{Methods3}

% Lecture 2
\begin{note}
 \begin{field}
  \tiny 100130x
 \end{field}
 \begin{field}
  Components of an experiment
 \end{field}
 \begin{field}
  Experimental units, treatment, design (how eus are allocated to treatments)
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100130
 \end{field}
 \begin{field}
  Model and assumptions for CRD
 \end{field}
 \begin{field}
  Model and assumptions for Completely randomized design
  $$y_{ij} = \mu_i + \epsilon_{ij}$$

  Where
  \begin{itemize}
   \item $y_{ij}$ is the response on the $j$th eu in the $i$th group
   \item $\mu_i$ is the population mean in the $i$th group
   \item $\epsilon_{ij}$ is the random error for the $j$th eu in the $i$th group
   \item Assume $\epsilon_{ij} \sim iid N(0,\sigma^2)$
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100131
 \end{field}
 \begin{field}
  Point estimate of $\hat{\mu}_i$
 \end{field}
 \begin{field}
  $\hat{\mu}_i = \bar{y}_i$ = mean in the $i$th group
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100132
 \end{field}
 \begin{field}
  Point estimate of
  $\hat{\sigma}^2$
 \end{field}
 \begin{field}
  \begin{align}
   \hat{\sigma}^2 & = MSE = \frac{\text{error sum of squares}}{df} = \frac{\text{residual SS}}{df} \\
                  & = \frac{\sum_{i=1}^g \sum_{j=1}^{n_i}(y_{ij} - \bar{y}_{i\cdot})^2}{N-g}       \\
                  & = s^2
  \end{align}

  Where
  \begin{itemize}
   \item $g$ is the number of groups
   \item $N$ is the overall sample size
   \item $n_i$ the number of eus in the $i$th group
   \item Df = sample size - number of parameters = $N-g$
   \item $i$th residual = $y_{ij} - \hat{y}_{ij} = y_{ij} - \bar{y}_{i\cdot }$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100133
 \end{field}
 \begin{field}
  Hypothesis test and interval estimates for $\mu_i$ in CRD
 \end{field}
 \begin{field}
  $\hat{\mu}_i = \bar{y}_i$ = sample mean of $y_{i1}, \cdots y_{in_{i}} \sim iid N(\mu_i,\sigma^2)$

  $$ \bar{y}_i \sim N(\mu_i, \frac{\sigma^2}{n_i})$$

  $SE(\bar{y}_i) = \sqrt{\frac{s^2}{n_i}}$

  CI: $\bar{y}_i \pm t_{(a/2,N-g)} \sqrt{\frac{s^2}{n_i}}$

  $H_0: \mu_i = 0$
  $t = \frac{\bar{y}_i}{\sqrt{s^2/n_i}} \sim t_{(N-g)}$

 \end{field}
\end{note}

% Lecture 3
\begin{note}
 \begin{field}
  \tiny 100134
 \end{field}
 \begin{field}
  Cell Means Parametrization
  (eg $g = 3, n_i = 2$)
 \end{field}
 \begin{field}
  $$ \begin{pmatrix}
    y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\y_{32}
   \end{pmatrix}  = \begin{pmatrix}
    1 & 0 & 0 \\
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    0 & 0 & 1
   \end{pmatrix} \begin{pmatrix}
    \mu_{1} \\ \mu_2 \\ \mu_3
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_{11} \\ \vdots \epsilon_{32}
   \end{pmatrix}$$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100135
 \end{field}
 \begin{field}
  Regression parametrization (eg $g = 3, n_i = 2$)
 \end{field}
 \begin{field}
  Code categorical variables using indicators
  $y_{ij} = \beta_0 + \beta_1X_{1,ij} + \beta_2X_{2,ij} + \epsilon_{ij}$
  $$ \begin{pmatrix}
    y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\y_{32}
   \end{pmatrix}  = \begin{pmatrix}
    1 & 1 & 0 \\
    1 & 1 & 0 \\
    1 & 0 & 1 \\
    1 & 0 & 1 \\
    1 & 0 & 0 \\
    1 & 0 & 0
   \end{pmatrix} \begin{pmatrix}
    \beta_0 \\ \beta_1 \\ \beta_2
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_{11} \\ \vdots \\ \epsilon_{32}
   \end{pmatrix}$$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100136
 \end{field}
 \begin{field}
  Factor (Treatment) Effects Parametrization
 \end{field}
 \begin{field}
  $$ y_{ij} = \mu + \alpha_i + \epsilon_{ij}$$
  Where
  \begin{itemize}
   \item $\mu$ = overall mean: average of $\mu_i$
   \item $\alpha_i$ = effect of level $i$ of the treatment factor, deviation away from $\mu$ associated with the $i$th treatment
  \end{itemize}

  $$ \begin{pmatrix}
    y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\y_{32}
   \end{pmatrix}  = \begin{pmatrix}
    1 & 1  & 0  \\
    1 & 1  & 0  \\
    1 & 0  & 1  \\
    1 & 0  & 1  \\
    1 & -1 & -1 \\
    1 & -1 & -1
   \end{pmatrix} \begin{pmatrix}
    \mu \\ \alpha_1 \\ \alpha_2
   \end{pmatrix} + \begin{pmatrix}
    \epsilon_{11} \\ \vdots \epsilon_{32}
   \end{pmatrix} $$

  Note that $\alpha_3 = -\alpha_1 - \alpha_2$
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100137
 \end{field}
 \begin{field}
  Extra Sum of Squares $F$- test
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Compares full and reduced models
         $$ F = \frac{(SS_E(\text{red}) - SS_E(\text{full}))/(df(\text{red}) - df(\text{full}))}{SS_E(\text{full})/df(\text{full})}$$
   \item Can use to test for differences across the group means
         $$ H_0: \mu_1 = \ldots = \mu_g = \mu$$
         $$H_A: \mu_i \neq \mu_j  \text{ for some } i \neq j$$
   \item Reduced model: $y_{ij} = \mu + \epsilon_{ij}$
   \item Full model: $y_{ij} = \mu_i + \epsilon_{ij}$
   \item $SS_E = \sum_{j}(y_j - \hat{y}_j)^2 = $ residual SS
   \item $SS_E(\text{full}) = \sum_{i = 1}^g \sum_{j = 1}^{n_i}(y_{ij} - \bar{y}_{i\cdot})^2$ Where $\bar{y}_{i\cdot}$ is the fitted value for obs in $i$th group
   \item $SS_E(\text{red}) = \sum_{i=1}^g \sum_{j=1}^{n_i}(y_{ij} - \bar{y}_{\cdot \cdot})^2$ where $\bar{y}_{\cdot\cdot} = \sum_{i}\sum_j y_{ij}/N$ mean of all obs
   \item df(full) = $N-g$
   \item df(red) = $N-1$
   \item SSE(red) - SSE(full) = SSTreatment for CRD
   \item Reduced model will have more unexplained variation
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100138
 \end{field}
 \begin{field}
  CRD ANOVA Table
 \end{field}
 \begin{field}
  \begin{tabular}{c|c c c c}
             & DF    & SS      & MS              & F                                \\
   \hline                                                                           \\
   Treatment & $g-1$ & SS(Trt) & SS(Trt)/$(g-1)$ & MS(trt)/MS(E) $\sim F_{g-1,N-g}$ \\
   Error     & $N-g$ & SS(E)   & SS(E)/$(N-g)$   &                                  \\
   \hline                                                                           \\
   Total     & $N-1$ & SS(T)   &                 &
  \end{tabular}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100139
 \end{field}
 \begin{field}
  Distribution of SS(Total)/$\sigma^2$, SS(Treatment)/$\sigma^2$ and SS(E)/$\sigma^2$
 \end{field}
 \begin{field}
  $\chi^2_{N-1}$, $\chi^2_{g-1}$ , $\chi^2_{N-g}$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100140
 \end{field}
 \begin{field}
  $E(MS_{\text{Trt}}) = $
 \end{field}
 \begin{field}
  $$E(MS_{\text{Trt}}) = $$
  \begin{itemize}
   \item If $H_0$ true, then $E(MS_{\text{Trt}}) = \sigma^2$
   \item If $H_A$ true, then $E(MS_{\text{Trt}})> E(MS_E)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100141
 \end{field}
 \begin{field}
  $E(MS_{\text{E}}) = $
 \end{field}
 \begin{field}
  $E(MS_{\text{E}}) = \sigma^2$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100142
 \end{field}
 \begin{field}
  Contrast
 \end{field}
 \begin{field}
  A contrast is a linear combination of treatment means where the coefficients sum to 0
  $C = \sum_{i=1}^g w_i \mu_i$ where $\sum_{i=1}^g w_i = 0$
  Examples:
  \begin{itemize}
   \item $\frac{\mu_1+\mu_2 +\mu_3 }{3} - \mu_4$, $C = 1/3,1/3,1/3,-1$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100143
 \end{field}
 \begin{field}
  Hypothesis test of contrast
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $\hat{C} = \sum_{i=1}^g w_i \bar{y}_{i\cdot}$
   \item $V(\hat{C}) = V\bigg(\sum_{i=1}^g w_i\bar{y}_{i\cdot}\bigg) = \sum_{i=1}^g w_i^2 \frac{\sigma^2}{n_i}$
   \item $\hat{V}(\hat{C}) = \sum_{i=1}^g w_i^2 \frac{MS_E}{n_i} =  \sum_{i=1}^g w_i^2 \frac{\hat{\sigma}^2}{n_i} $
   \item CI: $\hat{C} \pm t_{(1 - \alpha/2, N-g)}SE(\hat{C})$
   \item $t = \frac{\hat{C} - 0}{SE(\hat{C})} \sim t_{N-g}$
   \item Eg if $C = \mu_1 - \mu_4$ a test of $C = 0$ is testing $\mu_1 = \mu_4$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100144
 \end{field}
 \begin{field}
  Contrast sums of squares
 \end{field}
 \begin{field}
  $SS_{\text{Contrast}} = SS_E(\text{reduced}) - SS_E(\text{full})$

  \begin{itemize}
   \item The full model is the separate means model $y_{ij} = \mu_i + \epsilon_{ij}$
   \item The reduced model is the full model with the restriction $H_0: C = 0$ imposed on the $\mu_i$
   \item Eg: $C = \frac{\mu_1 + \mu_2 + \mu_3}{3} = \mu_4$
         Full model parameter vecotr $(\mu_1, \cdots, \mu_4)^t$, reduced model parameter vector: $(\mu_1, \mu_2, \mu_3)$ with $\mu_4 = \frac{\mu_1+ \mu_2 + \mu_3}{3}$
   \item df full = $N-4$, df reduced = $N-3$, df contrast = 1 = $(N-3) - (N-4)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100145
 \end{field}
 \begin{field}
  Orthogonal contrasts
 \end{field}
 \begin{field}
  Contrasts $C_1$ and $C_2$ are orthogonal if $\sum_{i=1}^g \frac{w_i w_i^*}{n_i} = 0$

  We usually only consider orthogonal contrasts when $n_i = n$ (balanced design)

  With $g$ treatments, we can have at most $g-1$ orthogonal contrasts

  If contrasts are orthogonal $SS(trt) = SS(C1) + \ldots + SS(C_{g-1})$
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100146
 \end{field}
 \begin{field}
  Orthogonal polynomial contrasts and polynomial regression
 \end{field}
 \begin{field}
  \begin{itemize}
   \item When data are balanced and treatments are incremental and equally spaced, we can use orthogonal polynomial contrasts
   \item With $g$ treatments, fit a $g-1$ degree polynomial model. Fitted polynomial will fit each treatment mean exactly
   \item The $g-1$ degree polynomial is another parametrization of the separate means model
   \item The cell means model ignores the incremental nature of treatment - polynomial one doesnt
   \item Polynomial models imply something about interpolation
   \item ex: $y_{ij}  = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \beta_3 X_i^3 + \epsilon_{ij}$ $X_i$ is the amount of treatment in the $i$th group.
   \item SS(trt) = SS(linar) + SS(quad) + SS(cubic)
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100147
 \end{field}
 \begin{field}
  Design matrix for orthogonal polynomial contrasts
 \end{field}
 \begin{field}
  $$ X = \begin{pmatrix}
    1      & 0      & 0^2    & 0^3    \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 50     & 50^2   & 50^3   \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 100    & 100^2  & 100^3  \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 150    & 150^2  & 150^3  \\
    \vdots & \vdots & \vdots & \vdots \\
   \end{pmatrix}$$
  $ \beta = (\beta_0, \beta_1, \beta_2, \beta_3)^t$
 \end{field}
\end{note}

% Lecture 8

\begin{note}
 \begin{field}
  \tiny 100148
 \end{field}
 \begin{field}
  Per comparison error rate
 \end{field}
 \begin{field}
  \begin{itemize}
   \item P(reject $H_{0i}$) when $H_{0i}$ is true
   \item Usual $\alpha$
   \item No correction for multiple comparisons
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100149
 \end{field}
 \begin{field}
  Experimentwise error rate
 \end{field}
 \begin{field}
  $\alpha_E = $ P(reject at least one $H_{0i}$) when $H_0$ is true (all $H_{0i}$ true )
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100150
 \end{field}
 \begin{field}
  False Discovery rate (FDR)
 \end{field}
 \begin{field}
  $FDR = \frac{\text{number false rejections}}{total number rejections}$,
  or 0 when no rejections

  Allows more incorrect rejections as the number of true rejections increases
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100151
 \end{field}
 \begin{field}
  Strong familywise error rate
 \end{field}
 \begin{field}
  $\alpha_F = P(\text{at least one false rejection}) = P(FDR > 0)$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100152
 \end{field}
 \begin{field}
  Tradeoff of multiple comparisons
 \end{field}
 \begin{field}
  Stronger error control - less powerful test
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100153
 \end{field}
 \begin{field}
  Bonferroni correction
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $K$ comparisons
   \item Fix $\alpha_F = P(\text{at least one false rejection})$ and set per comparison error rate $\alpha = \alpha_F/K$
   \item Reject $H_{0i}$ if its p value is less than $\alpha_F/K$
   \item Very strict, but easy test
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100154
 \end{field}
 \begin{field}
  Holm multiple comparison
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $K$ comparisons
   \item Sort individual p-values from small to large $p_1, \ldots, p_k$
   \item Reject $H_{0i}$ if $p_i < \frac{\alpha_F}{K-i+1}$
   \item Note $\frac{\alpha_F}{K-i+1} \geq \frac{\alpha_F}{K}$, so Holm is more powerful than Bonferroni , but still conservative
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100155
 \end{field}
 \begin{field}
  Multiple comparison method: FDR
  (edit: define FDR)
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $K$ comparisons
   \item Sort p-values
   \item Reject $H_{0i}$ if $p_i < \frac{i \cdot FDR}{K}$
   \item Controls the false discovery rate
  \end{itemize}
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100156
 \end{field}
 \begin{field}
  Scheffes method
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Only method that controls $\alpha_F$ if we've snooped the data
   \item Tests all possible contrasts (all are 0 )
   \item very conservative
   \item Reject $H_{0i}: C_i = 0$ if $$\frac{SS_{C_i}(g-1)}{MS_E} > F_{\alpha_F, g-1, N-g} $$
   \item Confidence interval: $$\hat{C_i} \pm \sqrt{(g-1)F_{\alpha_F, g-1, N-g}}SE(\hat{C_i}) $$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100157x
 \end{field}
 \begin{field}
  Multiple comparison for pairwise comparisons
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Contrasts of the form $\mu_i - \mu_j$
   \item For $g$ treatment groups there are $\binom{g}{2}$ possible pairwise comparisons
   \item Tukey's Honestly Significant Difference (balanced)
   \item Tukey-Kramer (not balanced )
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100157
 \end{field}
 \begin{field}
  Tukey's Honestly Significant Difference (HSD)
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Pairwise comparisons
   \item Simultaneous tests and CIs of all $C = \mu_i - \mu_j$
   \item Controls $\alpha_F$
   \item CI: $$\bar{y}_{i\cdot} - \bar{y}_{j\cdot} \pm q_{\alpha_F, g, N-g}\sqrt{\frac{MS_E}{n}} $$
   \item Assumes $n$ observations in each group (balanced)
   \item Where $q$ is the studentized range distribution - dividing a statistic by the estimate of its standard error
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100158
 \end{field}
 \begin{field}
  Tukey-Kramer
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Pairwise comparison
   \item If the data are not balanced (but close)
   \item Replace $\sqrt{\frac{MS_E}{n}}$ with $\sqrt{MS_E \frac{n_i + n_j}{2n_in_j}}$ in
   \item  CI: $$\bar{y}_{i\cdot} - \bar{y}_{j\cdot} \pm q_{\alpha_F, g, N-g}\sqrt{\frac{MS_E}{n}} $$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100159
 \end{field}
 \begin{field}
  Ryan-Einot-Gabriel-Welsch Range (REGWR) test
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Controls $\alpha_F$
   \item Stepdown procedure
   \item Order sample means from small to large
   \item Test ranges, starting with largest range $\mu_{(1)} = \mu_{(g)}$
   \item If fail to reject, stop, conclude that no means differ. Otherwise stop down and test next largest ranges. etc
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100160
 \end{field}
 \begin{field}
  Dunnett's Procedure
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Compare all treatments to control
   \item
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100161
 \end{field}
 \begin{field}
  Multiple Comparisons with the best $MCB$
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Identifies either $max(\mu_i)$ or $min(\mu_i)$
   \item Intervals either contain 0 (not different from best) or have 0 as an endpoint, which implies they are different from the best.
   \item Usually done with ANOVA
  \end{itemize}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100162
 \end{field}
 \begin{field}
  Difference between Type I and Type III Sum of Squares
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Type I is a nested model - variables are added
   \item Type III removes one variable
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100163
 \end{field}
 \begin{field}
  Effect of non-normality ("Robustness")
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If tails are too long (compared to normal) estimate of variance will be too large, inference will be conservative (CI to wide, p-values too big, type I error smaller than $\alpha$, lower power)
   \item If tails are too short, reverse is true
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100164
 \end{field}
 \begin{field}
  Equal variance diagnostics
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Levene's test
   \item Plot residuals vs fitted values
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100165
 \end{field}
 \begin{field}
  Effect of non-constant variance + Remedy
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If data are balance and variances are not too unequal, standard procedures work pretty well
   \item If data are unbalanced and large $n_i$ corresponds to larger variances, procedures too conservative
   \item Small $n_i$ correspond to large variances, opposite
   \item Remedy using Welch's ANOVA/weighted least squares, larger balanced sample
  \end{itemize}
 \end{field}
\end{note}

% Lecture 12

\begin{note}
 \begin{field}
  \tiny 100166
 \end{field}
 \begin{field}
  RF Plot
 \end{field}
 \begin{field}
  \begin{itemize}
   \item Residual-Fit Spread plot
   \item Left plot has sorted centered fits $\hat{y}_{ij} - \hat{\bar{y}}_{ij}$
   \item Right plot has sorted residuals $y_{ij} - \hat{y}_{ij}$
   \item Left plot shows variability explained by the model
   \item Right plot shows unexplained variability
   \item Want spread of left plot to be larger than right plot - indicates we have a good model
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100167
 \end{field}
 \begin{field}
  Sample size to perform a 2 sample z test
 \end{field}
 \begin{field}
  For a 2 sample $z$ test $H_0: \mu_1 = \mu_2$ with $\sigma^2$ known
  $$ n \geq 2(z_{\alpha/2} + z_{\beta})^2 \frac{\sigma^2}{\delta^2}$$
  \begin{itemize}
   \item $n$ sample size in each group
   \item $\alpha = $ Type I error rate
   \item $\beta = $ Type II error rate = 1-Power
   \item $z_{\alpha/2}= $ standard normal $1 - \alpha/2$ quantile
   \item $z_{\beta} = $ standard normal $1 - \beta$ quantile
   \item $\sigma^2 = $ common variance
   \item $\delta = \mu_1 - \mu_2 $
  \end{itemize}
 \end{field}
\end{note}

% Lecture 13

\begin{note}
 \begin{field}
  \tiny 100168
 \end{field}
 \begin{field}
  Sample size for one-way ANOVA
 \end{field}
 \begin{field}
  Depends on the distribution when $H_A$ is the case - non central F distribution - to find sample size, simulate repeated sampling under $H_A$ to calculate power for different $N$
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100169
 \end{field}
 \begin{field}
  $2\times 2$ Factorial design difference from ANOVA
 \end{field}
 \begin{field}
  ANOVA fits a model like, for group 1 with treatments C,F and group2 treatments HL
  \begin{tabular}{|c |c |c |c |}
   CH & CL & FH & FL \\
   \hline            \\
      &    &    &    \\
   \hline
  \end{tabular}

  (ignores the structure of treatments )

  Factorial design:
  \begin{tabular}{c|c|c|c|}
          &   & Liquid &   \\
          &   & L      & H \\
   \hline                  \\
   Screen & C &        &   \\
          & F &        &   \\
   \hline
  \end{tabular}

  Uses contrasts
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100170
 \end{field}
 \begin{field}
  Interaction plot
 \end{field}
 \begin{field}
  \begin{itemize}
   \item If the interaction contrast is 0, then the lines will be parallel
   \item If we see non parallel lines, it indicates there is an interaction
   \item Parallel lines associated with large p values of interaction term
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100171
 \end{field}
 \begin{field}
  Model for a $2\times 2$ factorial design
 \end{field}
 \begin{field}
  $y_{ijk}$ is response from the $k$th replicate with $i$th level of factor A, and $j$th level of factor B


  eg:

  \begin{center}
   \begin{tabular}{c c c c }
    \hline                              \\
      &         & B         &           \\
      &         & $j=1$     & $j = 2$   \\
    A & $i = 1$ & $y_{11k}$ & $y_{12k}$ \\
      & $i=2$   & $y_{21k}$ & $y_{22k}$ \\
   \end{tabular}
  \end{center}
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100172
 \end{field}
 \begin{field}
  Cell means parametrization for $2 \times 2$ factorial design
 \end{field}
 \begin{field}
  $$y_{ijk} = \mu_{ij} + \epsilon_{ijk}$$



  $y_{ijk}$ is response from the $k$th replicate with $i$th level of factor A, and $j$th level of factor B

  $\epsilon_{ijk} \sim N(0,\sigma^2)$
 \end{field}
\end{note}


\begin{note}
 \begin{field}
  \tiny 100173
 \end{field}
 \begin{field}
  Factor effects parametrization for $2 \times 2$ design
 \end{field}
 \begin{field}
  $$y_{ijk} = \mu + \alpha_i +\beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}$$
  Where
  \begin{itemize}
   \item $\mu$ is the overall mean
   \item $\alpha_i$ effect of $i$th level of factor A
   \item $\beta_j$ effect of $j$th level of factor B
   \item $(\alpha\beta)_{ij}$ interaction of $i$th level of A and $j$th level of B
   \item Where $\epsilon_{ijk} \sim N(0,\sigma^2)$
   \item $0 = \sum_{i=1}^2 \alpha_i = \sum_{j=1}^2\beta_j = \sum_{i=1}^2 (\alpha\beta)_{ij} = \sum_{i=1}^2(\alpha\beta)_{ij}$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100174
 \end{field}
 \begin{field}
  Equivalence of cell means and factor effects parametrizations
 \end{field}
 \begin{field}
  \begin{center}
   \begin{tabular}{c| c | c | }
          & $j=1$                                                                                                      & $j=2$                                                                                                      \\
    \hline                                                                                                                                                                                                                          \\
    $i=1$ & $\mu_{11} = \mu + \alpha_1 + \beta_1 + (\alpha\beta)_{11}$                                                 & $\mu_{12} = \mu + \alpha_1 + \beta_2 + (\alpha\beta)_{12}
     = \mu + \alpha_1 - \beta_1 - (\alpha\beta)_{11}$                                                                                                                                                                               \\
    $i=2$ & $\mu_{21} = \mu + \alpha_2 + \beta_1 + (\alpha\beta)_{21} = \mu - \alpha_1 + \beta_1 - (\alpha\beta)_{11}$ & $\mu_{22} = \mu + \alpha_2 + \beta_2 + (\alpha\beta)_{22} = \mu - \alpha_1 - \beta_1 + (\alpha\beta)_{11}$
   \end{tabular}
  \end{center}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100175
 \end{field}
 \begin{field}
  Design matrix for $2 \times 2$ factorial design, where each group has 2 options
 \end{field}
 \begin{field}
  $$ \begin{pmatrix}
    y_{111} \\ \vdots \\ y_{11n} \\ y_{121} \\ \vdots \\
    y_{12n} \\ y_{211}\\ \vdots \\ y_{21n} \\ y_{221} \\ \vdots \\
    \\ y_{22n}
   \end{pmatrix} =
   \begin{pmatrix}
    1      & 1      & 1      & 1      \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & 1      & -1     & -1     \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & -1     & 1      & -1     \\
    \vdots & \vdots & \vdots & \vdots \\
    1      & -1     & -1     & 1      \\
    \vdots & \vdots & \vdots & \vdots \\
   \end{pmatrix}
   \begin{pmatrix}
    \mu \\ \alpha_1 \\ \beta_1 \\ (\alpha\beta)_{11}
   \end{pmatrix} + \epsilon $$
 \end{field}
\end{note}



\begin{note}
 \begin{field}
  \tiny 100176
 \end{field}
 \begin{field}
  General model for a 2-factor design
 \end{field}
 \begin{field}
  $y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}$
  \begin{itemize}
   \item A has levels $1 \cdots a$
   \item B has levels $1 \cdots b$
   \item $\epsilon_{ijk} \sim N(0,\sigma^2)$
   \item $0 = \sum_{i=1}^a \alpha_i = \sum_{j=1}^b \beta_j = \sum_{i=1}^a (\alpha\beta)_{ij} = \sum_{j=1}^b (\alpha\beta)_{ij}$
   \item There are $a \times b $ parameters to estimate
   \item $(a-1)\, \alpha_i$s , $(b-1)\, \beta_j$s, $(a-1)(b-1) \, (\alpha\beta)_{ij}$s = $ab$ total parameters
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100177
 \end{field}
 \begin{field}
  Parameter estimates for $2 \times 2$ factorial design
 \end{field}
 \begin{field}
  \begin{itemize}
   \item $\hat{\mu} = \bar{y}_{\cdot\cdot \cdot}$
   \item $\hat{\alpha}_i = \hat{\mu}_{i\cdot} - \hat{\mu} = \bar{y}_{i\cdot \cdot} - \bar{y}_{\cdot \cdot \cdot }$
   \item $\hat{\beta}_j =\hat{\mu}_{\cdot j} - \hat{\mu} = \bar{y}_{\cdot j \cdot} - \bar{y}_{\cdot \cdot \cdot } $
   \item $\hat{(\alpha\beta)}_{ij} = \hat{\mu}_{ij} - \hat{\alpha}_i - \hat{\beta}_j - \hat{\mu}$
   \item Where :


   \item $\mu_{i\cdot }$ population mean for $i$th level of factor A
   \item $\mu_{\cdot j}$ population mean for $j$th level of factor B
   \item $\alpha_i$ deviation from the overall mean associated with $i$th level of factor A
   \item $(\alpha\beta)_{ij}$ deviation of cell mean from the row column and overall mean
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100178
 \end{field}
 \begin{field}
  ANOVA for 2 factor design - Hypothesis test interpretation
 \end{field}
 \begin{field}
  Degrees of freedom:
  \begin{itemize}
   \item $A: a - 1$
   \item $B: b-1$
   \item $AB: (a-1)(b-1)$
   \item Error $N - ab$
   \item Total $N-1$
  \end{itemize}
  Each row in anova sum of squares table gives the F value for if that row was zero, ie test all $\alpha_i = 0$ indicates that that factor has no effect
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100179
 \end{field}
 \begin{field}
  General factorial design (eg $8 \times 2 \times 2$)
 \end{field}
 \begin{field}
  $$ y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + (\alpha\beta)_{ij} + (\alpha\gamma)_{ik} + (\beta\gamma)_{jk} + (\alpha\beta\gamma)_{ijk} + \epsilon_{ijkl} $$


  Where
  \begin{itemize}
   \item $y_{ijkl}$ response for $l$th replicate at $i$th level of A, $j$th level of $B$ and $k$th level of C
   \item $\mu$ overall mean
   \item $\alpha_i$ effect of $i$th level of A
   \item $(\alpha\beta)_{ij}$ interaction of A and B
   \item $(\alpha\beta\gamma)$ interaction of A and B and C
   \item $\epsilon_{ijkl} iid \sim N(0,\sigma^2)$
  \end{itemize}
 \end{field}
\end{note}

\begin{note}
 \begin{field}
  \tiny 100180
 \end{field}
 \begin{field}
  Type I Type II Type III Sum of squares (unbalanced )
 \end{field}
 \begin{field}
  \begin{itemize}
   \item When data are unbalanced Type I and Type III SS are different
   \item I is sequential
   \item II is partial
   \item III is hierarchical
  \end{itemize}

  \begin{center}
   \begin{tabular}{|c|c|c|c|}
    Type & SS & Effects in full model & Effects in reduced model \\
    \hline                                                       \\
    I    & A  & A                     & intercept only           \\
    III  & A  & A,B,C,AB,AC,BC,ABC    & B,C,AB,AC,BC,ABC         \\
    II   & A  & A,B,C,BC              & B,C,BC                   \\
    III  & AB & A,B,C,AB,AC,BC,ABC    & A,B,C,AC,BC,ABC          \\
    II   & AB & A,B,C,AB,AC,BC        & A,B,C,AC,BC
   \end{tabular}
  \end{center}
 \end{field}
\end{note}

% Lecture 17

\begin{note}
    \begin{field}
        \tiny 100181
    \end{field}
    \begin{field}
        Issues with Unbalanced Data for overall mean estimate and sum of squares
    \end{field}
    \begin{field}
        \begin{itemize}
          \item The fitted value for $y_{ijk}$ is still the observed cell mean with unbalanced data
          \item Estimate of overall mean is not the average of all y values $\hat{\mu} \neq \bar{y}_{\cdot \cdot \cdot }$
          \item Issues with row and cell means
          \item $\bar{y}_{1\cdot \cdot }\neq \frac{\bar{y}_{11}+ \bar{y}_{12}}{}$
          \item Type I still sums to Model Sum of Squares, but Type II and III does not (but it does for balanced data)
          \item Type II useful for model building
          \item Type III SS useful for hypothesis testing
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100182
    \end{field}
    \begin{field}
        Full and Reduced Type II Sum of squares, with factors $A$, $B$, $C$
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Reduced model for facor A is largest model not containing $A$ in any terms (ie remove interactions with a), full model adds A, but not interactions of A
          \item Sum of Squares for A
          \item Full: A,B,C,BC
          \item Reduced: B,C,BC
          \item Sum of Squares for AB
          \item Full: A,B,C,AB,AC,BC
          \item Reduced: A,B,C,AC,BC
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100183
    \end{field}
    \begin{field}
        Type I sum of squares, full and reduced model, with factors A,B,C
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Note we could get different values depending on the order of the factors in the specified model (even p-values)
          \item Sequential
          \item Full model AB: A,B,C,AB
          \item Reduced model: A,B,C
          \item Full model: A:, A
          \item Reduced model: intercept only
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100184
    \end{field}
    \begin{field}
        Predicted values for different type of SS for factor A
    \end{field}
    \begin{field}
        \begin{tabular}{c c c }
          & Reduced model & Full model \\
          Type I & $\hat{y}_{ijk} = \hat{\mu}$ & $\hat{y}_{ijk} = \hat{\mu} +\hat{\alpha}_i$\\
          Type II & $\hat{y}_{ijk} = \hat{\mu} + \hat{\beta}_j$ & $\hat{y}_{ijk} = \hat{mu} + \hat{\alpha}_i + \hat{\beta}_j$\\
          Type II & $\hat{y}_{ijk} = \hat{\mu} + \hat{\beta}_j + (\hat{\alpha\beta})_{ij}$ &  $\hat{y}_{ijk} = \hat{\mu} + \hat{\alpha}_i \hat{\beta}_j + (\hat{\alpha\beta})_{ij}$
        \end{tabular}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100185
    \end{field}
    \begin{field}
        Estimates for 2-factor means and interactions
    \end{field}
    \begin{field}
        \begin{align*}
          \hat{\mu} &= \bar{y}_{\cdot\cdot\cdot}\\
          \hat{\alpha}_i &= \bar{y}_{i\cdot\cdot} - \bar{y}_{\cdot\cdot\cdot} \quad \text{group mean - overall mean}\\
          \hat{\beta}_j &= \hat{y}_{\cdot j \cdot } - \bar{y}_{\cdot \cdot \cdot }\\
          (\hat{\alpha\beta})_{ij} &= \bar{y}_{ij\cdot} - \bar{y}_{i\cdot\cdot} - \bar{y}_{\cdot j \cdot } + \bar{y}_{\cdot\cdot\cdot } \quad \text{cell mean - row and col means + overall mean }
        \end{align*}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 100186
    \end{field}
    \begin{field}
        Contrasts, balanced data and orthogonal
    \end{field}
    \begin{field}
        \begin{itemize}
          \item When data are balanced, a contrast for one main effect or interaction is orthogonal to a contrast for any other main effect
          \item Because of orthogonality, we can estimate effects and compute SS one term at a time, and the results for that term dont depend on what other terms are in the model.
          \item With unbalanced data, we dont have orthogonality
        \end{itemize}
    \end{field}
\end{note}



% Lecture 18
\begin{note}
    \begin{field}
        \tiny 100187
    \end{field}
    \begin{field}
        Example of converting a categorical variable into numeric
    \end{field}
    \begin{field}
        \begin{itemize}
          \item We should know at design stage if we want to treat variable as categorical or numeric
          \item Linear model will be less complicated than categorical - lower model degrees of freedom
        \end{itemize}
    \end{field}
\end{note}

% Lecture 19

\begin{note}
    \begin{field}
        \tiny 100188
    \end{field}
    \begin{field}
        Missing cell
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Factorial structure is missing
          \item Can analyze using cell means model and look at contrasts
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100189
    \end{field}
    \begin{field}
        When to use random effects
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Levels of a factor are sampled from a larger population
          \item Repeating experiment would use different factor levels (ie if the choice of levels are drawn using a random sample from larger popluation)
          \item Need to model dependence among observations from the same level of a factor
          \item key word - batch
          \item Not all observations independent (ie boxes from same machine are similar )
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100190
    \end{field}
    \begin{field}
        Random effects Dependence among observations in a single group- covariance
    \end{field}
    \begin{field}
        \begin{align*}
          Cov(y_{11},y_{12}) &= Cov(\mu + \alpha_1 + \epsilon_{11}, \mu + \alpha_1 + \epsilon_{12})\\
          &= Cov(\alpha_1,\alpha_1) + Cov(\alpha_1,\epsilon_{12}) + Cov(\epsilon_{11},\epsilon_{12})\\
          &= \sigma_\alpha^2
        \end{align*}
        \begin{itemize}
          \item Last three terms are 0 because independence assumptions within and between
          \item $\alpha_1$ and $\epsilon_{ij}$ are random, $\mu$ is fixed
          \item Note this model assumes positive covariance
          \item Note that covariance between observations in different groups is 0
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100191
    \end{field}
    \begin{field}
        Random Effects Model and Assumptions (1 factors)
    \end{field}
    \begin{field}
        $$y_{ij} + \alpha_i + \epsilon_{ij} $$
        Where
        \begin{itemize}
          \item $y_{ij} = $ strength of $j$th box made by $i$th machine
          \item $\mu = $ overall mean
          \item $\alpha_i$ = effect of $i$th machine (allows boxes made by two different machines to systematically differ)
          \item $\epsilon_{ij} = $ random error
        \end{itemize}

        Assumptions:
        \begin{itemize}
          \item $\epsilon_{ij} \sim iid N(0,\sigma^2)$
          \item $\alpha_i \sim iid N(0,\sigma_\alpha^2)$
          \item $\epsilon $ independent from $\alpha $
          \item Different condition from fixed effects $\sum_i \alpha = 0$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100192
    \end{field}
    \begin{field}
        Estimates for $\mu$ for random effects model (1 factor ) to make inference
    \end{field}
    \begin{field}
        \begin{align*}
          \hat{mu} &= \bar{y}_{\cdot\cdot\cdot} \\
          E(\hat{\mu}) &= \mu \\
          Var(\hat{\mu}) &= \frac{n \sigma_\alpha^2 + \sigma^2}{N}\\
          \hat{\mu} &\sim N(\mu, \frac{n \sigma_\alpha^2 + \sigma^2}{N})
        \end{align*}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100193
    \end{field}
    \begin{field}
        Random effects model - how to test differences among levels of the factor
    \end{field}
    \begin{field}
        \begin{itemize}
          \item If $\alpha_i$ were fixed, test $H_0: \alpha_i = 0$
          \item If random effect, cant use this $H_0$ since the hypotheses must be about the parameters, and $\alpha_i$ are random variables
          \item Instead test $H_0: \sigma_\alpha^2 = 0$ - no machine effect
          \item Estimated $\sigma^2, \sigma_\alpha^2$ are called variance components
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100194
    \end{field}
    \begin{field}
        Anova for one random factor design (A)
    \end{field}
    \begin{field}
        \begin{tabular}{c c c c }
          Source & df & SS & EMS\\
          A & $\alpha-1$ & $\sum_i \sum_j(\bar{y}_{i\cdot} - \bar{y}_{\cdot\cdot})^2$ & $\sigma^2 + n\sigma_\alpha^2$\\
          Error & $N - \alpha$ & $\sum_i\sum_j (\bar{y_{ij}} - \bar{y}_{i\cdot})^2$ & $\sigma^2$\\
          Total & $N-1$ & $\sum_i\sum_j (\bar{y}_{ij} - \bar{y}_{\cdot\cdot})^2$ &
        \end{tabular}
        $$ \frac{SS_A/(\alpha-1)}{SS_E/(N-\alpha)} \sim F_{\alpha-1,N-\alpha}$$
        Where $\alpha$ is the number of factors in A

    \end{field}
\end{note}

%Lecture 20

\begin{note}
    \begin{field}
        \tiny 100195
    \end{field}
    \begin{field}
        Expected Mean Squares in one random factor ANOVA
    \end{field}
    \begin{field}
        \begin{itemize}
          \item $H_0 = \alpha_i$ is true, then $E(MS_{trt}) = \sigma^2 = E(MS_E)$
          \item $H_0 = \alpha_i$ is false, then $E(MS_{trt}) > E(MS_E)$
          \item F statistic is $\frac{MS_{trt}}{MS_E}$, and we reject $H_0$ if the F statistic is large
          \item $E(MS_A) = \sigma^2 + n\sigma_\alpha^2$. If $H_0: \sigma_\alpha^2 = 0$ true, $E(MS_A) = \sigma^2$
          \item Expected mean squares tell us how to form the F statistic,
          \item The denominator is the MS whose expectation is equal to the numerator $E(MS)$ under $H_0$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100196
    \end{field}
    \begin{field}
        $MS_{trt}$ in factor design
    \end{field}
    \begin{field}
        $MS_{trt}$ is $MS_A$ for the $A$ treatment. or $MS_B$ if testing $B$ treatment so $F = \frac{MS_{trt}}{MS_E}$
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100197
    \end{field}
    \begin{field}
        Two random factors Model and assumptions
    \end{field}
    \begin{field}
        $$y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk} $$
        Where
        \begin{itemize}
          \item $y_{ijk} = $ strength of $k$th box from the $i$th machine made by $j$th operator
          \item $\mu = $ overall mean (fixed)
          \item $\alpha_i = $ effect of $i$th machine
          \item $\beta_j  =$ effect of $j$th operator
          \item $(\alpha\beta)_{ij} = $ interaction between $i$th machine and $j$th operator
          \item $\epsilon_{ijk} = $ random error
        \end{itemize}
        Assumptions:
        \begin{itemize}
          \item $\alpha_i \sim iid N(0,\sigma_\alpha^2)$
          \item $\beta_j \sim iid N(0,\sigma_\beta^2)$
          \item $(\alpha\beta) \sim iid N(0,\sigma_{\alpha\beta}^2)$
          \item $\epsilon_{ijk} \sim iid N(0,\sigma^2)$
          \item $\alpha,\beta,(\alpha\beta),\epsilon$ all independent
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100198
    \end{field}
    \begin{field}
        ANOVA for two random factor model
    \end{field}
    \begin{field}
        \begin{tabular}{c c c}
          Source & DF & EMS\\
          A & $a-1$ & $\sigma^2 + n\sigma_{\alpha\beta}^2 + nb \sigma_{\alpha}^2$ \\
          B & $b-1$ & $\sigma^2 + n\sigma_{\alpha\beta}^2 + na \sigma_\beta^2$\\
          AB & $(a-1)(b-1)$ & $\sigma^2 + n\sigma_{\alpha\beta}^2$\\
          Error & $N-ab = ab(n-1)$ & $\sigma^2$
        \end{tabular}
        \begin{itemize}
          \item Balanced design $N = abn$
          \item To construct test for treatment X, find $MS_X$, and find EMS under $H_0$ for denominator in f test
          \item Interpretation: If we have significatn pvalue, there is evidence that response varies due to random effect A (if testing A)
        \end{itemize}
    \end{field}
\end{note}

%Lecture 21

\begin{note}
    \begin{field}
        \tiny 100199
    \end{field}
    \begin{field}
        Estimate variance components
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Can either use MoM or REML (restricted maximum likelihood)
          \item For MoM, set MS sample quantities equal to their expectation (EMS) from ANOVA table
          \item Solve system of equations
          \item Note MoM estimates may not be in parameter estimates (ie variances negative, can just set to 0 if case), although this may indicate that model is inadequate
          \item If data are (approximately) balanced, and model is good, MoM and REML estimates should be close
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100200
    \end{field}
    \begin{field}
        Model and assumptions for 3 random factors design
    \end{field}
    \begin{field}
        $$y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + (\alpha\beta)_{ij} + (\alpha\gamma)_{ik} + (\beta\gamma)_{ik} + (\alpha\beta\gamma)_{ijk} + \epsilon_{ijkl}$$
        Where
        \begin{itemize}
          \item $y_{ijkl}$: strength of $l$th box from $i$th machine, $j$th operator and $k$th batch of glue
          \item $\mu$ = overall mean (fixed)
          \item $\alpha_i$: effect of $i$th machine
          \item $\beta_j$: effect of $j$th operator
          \item $\gamma_k$: effect of $k$th batch of glue
          \item $(\alpha\beta)_{ij}$: machine $\times$ operator interaction
          \item $(\alpha\gamma)_{ik}$: machine $\times$ glue interaction
          \item $(\beta\gamma)_{jk}$: operator $\times$ glue interaction
          \item $(\alpha\beta\gamma)_{ijk}$: three way interaction
          \item $\epsilon_{ijkl}$: random error
        \end{itemize}
        Assumptions
        \begin{itemize}
          \item Each random quantity $X$ is independent from others and distributed iid $N(0,\sigma_X^2)$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100201
    \end{field}
    \begin{field}
        Anova for 3 factor random effects
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Needs approximate F test
          \item there is no MS with expectation of $MS_X$ under $H_0$, so we must find a linear combination of the MS that has the right expectation: $\sum_s g_s MS_s$
          \item Since the denominator of $F$ statistic is a linear combination of the MSs, the F test is approximate, so we have to approximate the degrees of freedom too
          \item Denominator df: $$ v^* = \frac{(\sum_sg_sMS_s)^2}{\sum_sg_s^2 MS_s^2/v_s}$$
          where $v_s = $df for $MS_s$ (same as Satterthwaite approximation for Welch t-test)
          \item Generally dont estimate variance components (ie for a confidence interval), since these tests are asymptotic (unlike F -test )
        \end{itemize}
    \end{field}
\end{note}

% Lecture 22
\begin{note}
    \begin{field}
        \tiny 100202
    \end{field}
    \begin{field}
        Crossed factors: model and assumptions $A$ is fixed $B $ is random
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Mixed effects model
          \item All combos of factors are tested
          \item Each machine is used by all operators
          \item Each operator produces boxes using both machines
        \end{itemize}
        $$y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk} $$
        \begin{itemize}
          \item $y_{ijk}$: strength of $k$th box, made with $i$th machine and $j$th operator
          \item $\mu$: overal mean (fixed)
          \item $\alpha_i$: effect of $i$th machine (fixed)
          \item $\beta_j$: effect of $j$th operator
          \item $(\alpha\beta)_{ij}$: machine $\times$ operator interaction
          \item $\epsilon_{ijk}$: random error
        \end{itemize}
        Assumptions
        \begin{itemize}
          \item $\sum_{i=1}^2 \alpha_i = 0$
          \item $\beta_j iid N(0,\sigma_\beta^2)$
          \item $(\alpha\beta)_{ij} \sim iid N(0,\sigma_{\alpha\beta}^2)$
          \item $\epsilon_{ijk} \sim N(0,\sigma^2)$
          \item $\beta,(\alpha,\beta),\epsilon$ all independent
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100203
    \end{field}
    \begin{field}
      Nested factors : Model and Assumptions
    \end{field}
    \begin{field}
      \begin{tabular}{c c c c c c}
        & & & B & & \\
        & &  1 & 2 & 3 & 4 \\
        A & 1 & $A_1B_{1(1)}$ & $A_1 B_{2(1)}$ & $A_1B_{3(1)}$ & $A_1B_{4(1)}$\\
        & 2 & $A_2B_{1(2)}$ & $A_2B_{2(2)}$ & $A_2B_{3(2)}$ & $A_2B_{4(2)}$
      \end{tabular}
       \begin{itemize}
         \item A is fixed, B is random
         \item Each operator uses only one machine
         \item Neither machine is used by all operators
         \item Can't compare machine effect among operations( because we dont know how boxes would vary if the operator had used the other machine), so we cant model the interaction
         \item Model does not include $ A \times B$
       \end{itemize}
       $$ y_{ijk} = \mu + \alpha_i + \beta_{j(i)} + \epsilon_{ijk} $$
      \begin{itemize}
        \item $y_{ijk}$: strength of $k$th box, made with $i$th machine, and $j$th operator
        \item $\mu$: overall mean
        \item $\alpha_i$; effect of $i$th machine
        \item $\beta_{j(i)}$: effect of $j$th operator for the $i$th machine
        \item $\sum_{i=1}^a \alpha_i = 0$, $\beta_{j(i)} \sim iid N(0,\sigma_\beta^2), \epsilon_{ijk} \sim iid N(0,\sigma^2)$, both independent
      \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100204
    \end{field}
    \begin{field}
        Comparison between crossed factors and nested factors
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Crossed factors: every level of A saw every level of B, and vice versa,
          \item Nested factors: if B is nested in A, levels of B only see one level of A
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100205
    \end{field}
    \begin{field}
        Reason for nesting
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Feasibility, if machines are in different locations, we wouldnt want to transport operators around
          \item Subsampling: Multiple observations on the same experimental unit
          \begin{itemize}
            \item Observation is nested in experimental unit
            \item Experimental units are always nested in treatment (ie fish in temp fish tank), treatment tank, measurement fish
            \item Usually nested effects are random, but not necessary
          \end{itemize}
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100206
    \end{field}
    \begin{field}
        Multiple levels of nesting Model and assumption
    \end{field}
    \begin{field}
        A, factory (random), B, machine (random), C = operator (random )
        \begin{itemize}
          \item Operator is nested in machine version, nested in factory
          \item No crossed effects means no interaction term
          \item $y_{ijkl} = \mu +\alpha_i + \beta_{j(i)} + \gamma_{k(ij)} + \epsilon_{ijkl}$
          \item Assume each term iid Normal with associated variance, all independent.
          \item $Cov(y_{ijkl}, y_{ijkl'}) = \sigma_\alpha^2 + \sigma_\beta^2 + \sigma_\gamma^2$, covariance within same levels of each factor
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100207
    \end{field}
    \begin{field}
        Crossed and nested factors
    \end{field}
    \begin{field}
        \begin{itemize}
          \item All random
          \item Operator nested in machine (operators make boxes using only one machine
          \item Operator crossed with glue (operators make boxes using both batches of glue )
          \item So glue sees all operators and all machines
          \item $y_{ijkl} = \mu + \alpha_i + \beta_{j(i)} + \gamma_k + (\alpha\gamma)_{ik} + (\beta\gamma)_{j(i)k} + \epsilon_{ijkl}$
          \item All normality and independence assumptions
          \item No three way interaction or machine times operator interaction
        \end{itemize}
    \end{field}
\end{note}

%lecture 23

\begin{note}
    \begin{field}
        \tiny 100208
    \end{field}
    \begin{field}
        Estimating means and contrasts for a mixed model
    \end{field}
    \begin{field}
        \begin{itemize}
          \item A (machine) fixed, B (operator) random
          \item $V(\bar{y}_{1\cdot\cdot}) = V\bigg( \frac{\sum_{j=1}^b\sum_{k=1}^n y_{1jk}}{bn}\bigg)\frac{1}{bn}(n\sigma_\beta^2 + n\sigma_{\alpha\beta}^2 + \sigma^2)$
          \item Calculated using independence assumptions, $\mu,\alpha_1$ fixed
          \item $\hat{C} = \sum_{i=1}^g w_i \bar{y}_{i\cdot}$
          \item To compare machine 1 and 2, let $w_1 =1$, $w_2 = -1$
          \item Compute point estimate $\hat{C} - \bar{y}_{1\cdot\cdot} - \bar{y}_{2\cdot\cdot}$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100209
    \end{field}
    \begin{field}
        Complete Block Designs (RCB,RCBD)
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Grouping observations into groups that are homogeneous
          \item Generalized pair - observations in a group not independent,
          \item Example: litter of animals, locations
          \item Experimental units stratified into blocks
          \item Within each block, randomly assign experimental units to treatments
          \item At least one replicate of each combination in each block
          \item In a balanced design, each block will have the same number of replicates for each treatment combination
          \item Draw out experimental design to identify blocked designs
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100210
    \end{field}
    \begin{field}
        Why use blocking
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Account for non-independence
          \item Explain some of the variability in the response (blocking as a nuisence parameter )
          \item If experimental units can be grouped into homogeneous blocks, then blocks explain some of the variability
          \item variance reduction design
        \end{itemize}
    \end{field}
\end{note}


% Lecture 24

\begin{note}
    \begin{field}
        \tiny 100211
    \end{field}
    \begin{field}
        Model and assumptions for RCBD (with $n=1$ observations per cell)
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Resembles a factorial design
          \item $y_i = \mu + \alpha_i + \beta_j + \epsilon_{ij}$
          \item $y_{ij}$: response for the $i$th level of the treatment in the $j$th block
          \item $\mu$: overall mean
          \item $\alpha_i$: treatment effect
          \item $\beta_j$: block effect
          \item $\epsilon_{ij}$; random error
          \item Assume: $\sum_i \alpha_i = \sum_j\beta_j = 0, \epsilon_{ij} \sim iid N(0,\sigma^2)$
          \item Note no interaction term - we dont want a blocking factor that interacts with treatments
        \end{itemize}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 200212
    \end{field}
    \begin{field}
        ANOVA for RCBD ($n=1$ replicate )
    \end{field}
    \begin{field}
      With a treatment with $g$ gropus, and $r$ blocks
        \begin{tabular}{c c c }
          Source & DF & SS \\
          Treatment & $g-1$ & \\
          Block & $r-1$ &  \\
          Error & $(g-1)(r-1)$ & \\
          Total & $gr-1 = N-1$ &
        \end{tabular}

        Note usually dont test block effect (but cant infer causation since not randomly assigned to blocks )
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 200213
    \end{field}
    \begin{field}
        Relative efficiecy
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Want to compare the amount of information captured from the data by two designs.
          \item Note a more complicated model (eg RCBD) would have a smaller $SS_E$ but also a smaller $df_{error}$
          \item For a single observation from a normal distribution $I = \frac{1}{\sigma^2}$
          \item Information increases as variance decreases
          \item $RE = \frac{I_1}{I_2} = \frac{\sigma_2^2}{\sigma_1^2}$
          \item By convention: $I_2$ is the simpler design.
          \item Where $\sigma_i^2$ is the error variance in design $i$ (which is assumed Normal)
          \item Since variances are not known, they must be estimated
          \item The variance of the design not performed will have to be calculated differently
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 200214
    \end{field}
    \begin{field}
        Calculating and interpreting relative efficiency
    \end{field}
    \begin{field}
      EG for comparing CRD and RCBD
        \begin{itemize}
          \item $\widehat{RE} = \frac{\hat{\sigma}_{CRD}^2}{\hat{\sigma}^2_{RCBD}} \cdot \frac{(v_{RCBD}+1)(v_{CRD} + 3)}{(v_{RCBD}+3)(v_{CRD} + 1)}$
          \item Where $v_{design}$ is the degrees of freedom for that design ($v_{CRD} = N - g$)
          \item $\hat{\sigma}_{CRD} = \frac{(r-1)MS_{block} + ((g-1)+(r-1)(g-1)MS_E)}{(r-1) + (g-1) + (r-1)(g-1)}$ Weighted average of $MS_{block}$ and $MS_E$
          \item IF RE = 2, then the RCBD is twice as efficient as CRD, so we should only need half as many replicates in the blocked design.
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100215
    \end{field}
    \begin{field}
        Latin squares design
    \end{field}
    \begin{field}
        \begin{itemize}
          \item If we have multiple blockign factors, this requires many eus.
          \item For a RCBD need at least one experiment unit in each cell for each treatment
          \item LS design is incomplete block design
          \item A Latin square design has $g$ levels of the treatment, and 2 blocking factors, each with $g$ levels. Each treatment level occurs exactlky once for each level of the blockign factor. (like sudoku )
          \item To randomize a LS experiment pick one LS at random from all possible LS designs of appropriate size. For $g=3$, there are 12
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100216
    \end{field}
    \begin{field}
        Model and assumptions for Latin squares design
    \end{field}
    \begin{field}
        $$y_{ijk} = \mu +\alpha_i + \beta_j + \gamma_k + \epsilon_{ijk} $$
        \begin{itemize}
          \item $y_{ijk}$ response for $i$th level of treatment, $j$th row(level of blocking factor 1), $k$th column (level of blocking factor 2)
          \item $\mu = $ overall mean
          \item $\alpha_i$ effect of level $i$ of treatment
          \item $\beta_j$ effect of level $j$ of blocking factor (row effect)
          \item $\gamma_k$ effect of level $k$ of blocking factor 2 (column effect)
          \item Assumptions: Sum to zero constraints or normality/independence constraints if fixed/random. $\epsilon_{ijk} \sim N(0,\sigma^2)$
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100217
    \end{field}
    \begin{field}
        ANOVA for Latin Squares design
    \end{field}
    \begin{field}
        \begin{tabular}{c c c c }
          Source & DF & MS & F \\
          Treatment & (g-1) & MS(treat) & MS(treat)/MS(E)\\
          Row & (g-1) & MS(row) & \\
          Column & (g-1) & MS(col) & \\
          Error & (g-1)(g-2) & MS(E) & \\
          Total & $g^2-1= N-1$ & &
        \end{tabular}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100218
    \end{field}
    \begin{field}
        Relative efficiency of LS design compared to RCBD
    \end{field}
    \begin{field}
        $$ \widehat{RE}_{LS,RCBD} = \frac{\hat{\sigma}_{RCBD}^2}{\hat{\sigma}_{LS}^2} \frac{(v_{LS} + 1)(v_{RCBD} + 3)}{(v_{LS} + 3)(v_{RCBD} + 1)}$$
        \begin{itemize}
          \item Where $\hat{\sigma}_{RCBD} = \frac{(g-1)MS(row) + ((g-1) + (g-1)(g-2)MS(E))}{2(g-1) + (g-1)(g-2)}$
        \end{itemize}
    \end{field}
\end{note}

% Lecture 26

\begin{note}
    \begin{field}
        \tiny 100219
    \end{field}
    \begin{field}
        Split-plot design description
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Choose a split-plot design when some factors are more difficult or expensive to vary than others
          \item Example A - irrigation (large plot) and B - variety (small subplot of each large plot )
          \item Randomize assignment of levels of A
          \item In each level of A, randomize the subplots and assign levels of factor B
          \item Whole plots are experimental units wrt A, subplots are eus wrt B
          \item $n$ - number of replicates for each level of A. If we want to replicate, we need an entire other set of levels of A.
          \item Assume whole plots are independent of each other
          \item Observations within a single whole plot are not assumed independent
          \item Whole plots nested in A, A and B are crossed
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100220
    \end{field}
    \begin{field}
        Model and assumptions for split plot design
    \end{field}
    \begin{field}
      example when fixed factors
        $$y_{ijk} = \mu + \alpha_i \eta_{k(i)} + \beta_j + (\alpha\beta)_{ij} + \epsilon_{k(i)j} $$
        \begin{itemize}
          \item $\alpha_i$ effect of $i$th level of A
          \item $\eta_{k(i)}$ whole plot error , random effect for whole plot nested in A
          \item $\beta_j$ effect of $j$th level of B
          \item $(\alpha\beta)_{ij}$ interaction of irrigation and variety
          \item $\epsilon_{k(i)j}$ random subplot error
          \item $\alpha,\beta$, interaction assumptions based on fixed/random
          \item $\epsilon_{k(i)j} \sim iid N(0,\sigma^2)$ independent from $\eta_{k(i)} \sim N(0,\sigma_{\eta}^2)$
          \item Note that the first terms are a CRD model, which reflects we did a CRD at whole-plot level, if we did a more complicated design at whole-plot level, our model would reflect that.
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100221
    \end{field}
    \begin{field}
        Difference between split-plot design and complete factorial, blocking
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Between complete factorial
          \begin{itemize}
            \item Split plot has two stages of randomization
            \item Complete factorial has one stage where combinations of factors are assigned to units
          \end{itemize}
          \item Blocking
          \begin{itemize}
            \item Randomization occurred at two stages, RDBD units are not randomly assigned to blocks
            \item Dont care about inference for blocks, do care about inference for whole plot factor
          \end{itemize}
          \item Nesting
          \begin{itemize}
            \item Subplots are nested in whole plot, each level of A sees all level of B and each level of B sees all levels of A
            \item Includes an interaction for A and B
          \end{itemize}
        \end{itemize}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 100222
    \end{field}
    \begin{field}
        ANOVA for split-plot design
    \end{field}
    \begin{field}
        \begin{tabular}{c c c }
          Source & DF & EMS \\
          A & a-1 & $\sigma^2 + b \sigma_\eta^2 + nb \frac{\sum_i\alpha_i^2}{a-1}$\\
          Whole plot error & $a(n-1)$ & $\sigma^2 + b \sigma_\eta^2$\\
          B & $b-1$& $\sigma^2 + na \frac{\sum_j \beta_j^2}{b-1}$\\
          AB & $(a-1)(b-1)$ & $\sigma^2 + n \frac{\sum_i\sum_j (\alpha\beta)_{ij}^2}{(a-1)(b-1)}$\\
          Subplot error & $a(n-1)(b-1)$ & $\sigma^2$
        \end{tabular}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100223
    \end{field}
    \begin{field}
        Hypothesis tests for a split-plot design
    \end{field}
    \begin{field}
        \begin{itemize}
          \item For fixed effects,
          \item A effect: $H_0: \alpha_i = 0$: $F = \frac{MS_A}{MS_{\text{whole plot}}}$
          \item B effect: $H_0: \beta_j = 0$: $F = \frac{MS_B}{MS_{\text{sub plot}}}$
          \item AB effect $H_0: (\alpha\beta)_{ij} = 0, F = \frac{MS_{AB}}{MS_{\text{subplot}}}$
          \item Under the null hypothesis, use the EMS for treatment factor to find the associated denominator EMS
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100224
    \end{field}
    \begin{field}
        Dependence between subplots of same whole plot - Cov and Corr
    \end{field}
    \begin{field}
        \begin{align*}
          Cov(y_{ijk},y_{ij'k}) &= Cov(\eta_{k(i)}+ \epsilon_{k(i)j}, \eta_{k(i)} + \epsilon_{k(i)j'})\\
          &= \sigma_\eta^2\\
          Corr(y_{ijk},y_{ij'k}) &= \frac{Cov(y_{ijk},y_{ij'k}) }{\sqrt{Var(y_{ijk})Var(y_{ij'k})}}\\
          &= \frac{\sigma_\eta^2}{\sigma_\eta^2 + \sigma^2}
        \end{align*}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 100225
    \end{field}
    \begin{field}
        Split plot design, estimate difference and CI (example)
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Eg, CI for difference in 2 factors of B : $\beta_1 - \beta_2$: $\bar{y}_{\cdot 1 \cdot} - \bar{y}_{\cdot 2 \cdot }$
          \item Calculate variance by using model equation to find average, and use independence, and fixed variance rules.
          \item Replace $MS_X$ as the estimate for $\sigma_X^2$
          \item Use $t$ as reference distribution with DF for MS that gives us the variance estimate.
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100226
    \end{field}
    \begin{field}
        Repeated measures
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Multiple observations on the SAME experimental unit
          \item Often repeated measurements in time.
          \item Questions to ask to use repeated measure
          \begin{itemize}
            \item Ignoring repeated measurements, what is experimental design?
            \item Are observations on the same EU independent?
            \item What are research objective? (ie does it want an interaction between time and treatment )
          \end{itemize}
          \item Use split-plot design, but time is not randomized
        \end{itemize}
    \end{field}
\end{note}

\begin{note}
    \begin{field}
        \tiny 100227
    \end{field}
    \begin{field}
        Repeated measures model and assumptions
    \end{field}
    \begin{field}
      Example model (fixed effects):
      $$y_{ijk} = \mu + \alpha_i + \epsilon_{k(i)} + \beta_j + (\alpha\beta)_{ij} + (\epsilon\beta)_{k(i)j} $$
      \begin{itemize}
        \item $\mu$: overall mean
        \item $\alpha_i$: formula effect (fixed)
        \item $\epsilon_{k(i)}$: random baby effect
        \item $\beta_j$: time effect
        \item $(\alpha\beta)_{ij}$: formula times time interaction
        \item $(\epsilon\beta)_{k(i)j}$; baby $\times$ time interaction
        \item All assumptions for fixed/random factors (Here: $0 = \sum_i \alpha_i = \sum_j \beta_j = \sum_i (\alpha\beta)_{ij} = \sum_j(\alpha\beta)_{ij}$)
        \item $\epsilon_{k(i)} \sim iid N(0,\sigma^2)$ independent from $(\epsilon\beta)_{k(i)j} \sim iid N(0,\sigma_{\alpha\beta}^2)$
        \item Note assumes constant correlation over time, Correlation matrix R of one EU is 1 on diagonal and $\rho$ everywhere else Correlation matrix overall is blocks of R on diagonal, 0 everywhere else.
      \end{itemize}
    \end{field}
\end{note}


\begin{note}
    \begin{field}
        \tiny 100228
    \end{field}
    \begin{field}
        ANCOVA
    \end{field}
    \begin{field}
        \begin{itemize}
          \item Same as regression parametrization for interaction terms - seperate intercepts equal slopes
          \item $y_{ij} = \mu + \alpha_i + \beta x_{ij} + \epsilon_{ij}$
          \item Assumes relationships between $x_{ij}$ and $y_{ij}$ is the same for all three groups (equal slopes ) Or we could have had $\beta_i$ to have different slopes
          \item Cant compare $\bar{y}_{1 \cdot}, \bar{y}_{2 \cdot},\bar{y}_{3 \cdot}$ since $\bar{x}_1 \neq \bar{x}_2 \neq \bar{x}_3$ (ie if we draw over mean y for mean $x_i$) these arent comparable.
          \item Compare mean ys by having common x value (often $\bar{x}$)
          \item Covariate adusted means: $y_{ij} = \tilde{mu} + \alpha_i + \beta(x_{ij} - \bar{x}_{\cdot\cdot}) + \epsilon_{ij}$
          \item $\tilde{\mu} - \beta\bar{x}_{\cdot\cdot} = \mu$
        \end{itemize}
    \end{field}
\end{note}

%%end_tag

\end{document}
